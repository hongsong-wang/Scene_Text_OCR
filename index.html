<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2507.16330.pdf' target='_blank'>https://arxiv.org/pdf/2507.16330.pdf</a></span>   <span><a href='https://github.com/josepDe/Project_Aria_STR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph De Mathia, Carlos Francisco Moreno-GarcÃ­a
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16330">Scene Text Detection and Recognition "in light of" Challenging Environmental Conditions using Aria Glasses Egocentric Vision Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In an era where wearable technology is reshaping applications, Scene Text Detection and Recognition (STDR) becomes a straightforward choice through the lens of egocentric vision. Leveraging Meta's Project Aria smart glasses, this paper investigates how environmental variables, such as lighting, distance, and resolution, affect the performance of state-of-the-art STDR algorithms in real-world scenarios. We introduce a novel, custom-built dataset captured under controlled conditions and evaluate two OCR pipelines: EAST with CRNN, and EAST with PyTesseract. Our findings reveal that resolution and distance significantly influence recognition accuracy, while lighting plays a less predictable role. Notably, image upscaling emerged as a key pre-processing technique, reducing Character Error Rate (CER) from 0.65 to 0.48. We further demonstrate the potential of integrating eye-gaze tracking to optimise processing efficiency by focusing on user attention zones. This work not only benchmarks STDR performance under realistic conditions but also lays the groundwork for adaptive, user-aware AR systems. Our contributions aim to inspire future research in robust, context-sensitive text recognition for assistive and research-oriented applications, such as asset inspection and nutrition analysis. The code is available at https://github.com/josepDe/Project_Aria_STR.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2507.02200.pdf' target='_blank'>https://arxiv.org/pdf/2507.02200.pdf</a></span>   <span><a href='https://github.com/Event-AHU/ESTR-CoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Jingtao Jiang, Qiang Chen, Lan Chen, Lin Zhu, Yaowei Wang, Yonghong Tian, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02200">ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event stream based scene text recognition is a newly arising research topic in recent years which performs better than the widely used RGB cameras in extremely challenging scenarios, especially the low illumination, fast motion. Existing works either adopt end-to-end encoder-decoder framework or large language models for enhanced recognition, however, they are still limited by the challenges of insufficient interpretability and weak contextual logical reasoning. In this work, we propose a novel chain-of-thought reasoning based event stream scene text recognition framework, termed ESTR-CoT. Specifically, we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input event stream into tokens and utilize a Llama tokenizer to encode the given generation prompt. A Q-former is used to align the vision token to the pre-trained large language model Vicuna-7B and output both the answer and chain-of-thought (CoT) reasoning process simultaneously. Our framework can be optimized using supervised fine-tuning in an end-to-end manner. In addition, we also propose a large-scale CoT dataset to train our framework via a three stage processing (i.e., generation, polish, and expert verification). This dataset provides a solid data foundation for the development of subsequent reasoning-based large models. Extensive experiments on three event stream STR benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the effectiveness and interpretability of our proposed framework. The source code and pre-trained models will be released on https://github.com/Event-AHU/ESTR-CoT.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2506.10609.pdf' target='_blank'>https://arxiv.org/pdf/2506.10609.pdf</a></span>   <span><a href='https://github.com/yingift/MSTAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Yin, Xudong Xie, Zhang Li, Xiang Bai, Yuliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10609">MSTAR: Box-free Multi-query Scene Text Retrieval with Attention Recycling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text retrieval has made significant progress with the assistance of accurate text localization. However, existing approaches typically require costly bounding box annotations for training. Besides, they mostly adopt a customized retrieval strategy but struggle to unify various types of queries to meet diverse retrieval needs. To address these issues, we introduce Muti-query Scene Text retrieval with Attention Recycling (MSTAR), a box-free approach for scene text retrieval. It incorporates progressive vision embedding to dynamically capture the multi-grained representation of texts and harmonizes free-style text queries with style-aware instructions. Additionally, a multi-instance matching module is integrated to enhance vision-language alignment. Furthermore, we build the Multi-Query Text Retrieval (MQTR) dataset, the first benchmark designed to evaluate the multi-query scene text retrieval capability of models, comprising four query types and 16k images. Extensive experiments demonstrate the superiority of our method across seven public datasets and the MQTR dataset. Notably, MSTAR marginally surpasses the previous state-of-the-art model by 6.4% in MAP on Total-Text while eliminating box annotation costs. Moreover, on the MQTR benchmark, MSTAR significantly outperforms the previous models by an average of 8.5%. The code and datasets are available at https://github.com/yingift/MSTAR.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2505.15649.pdf' target='_blank'>https://arxiv.org/pdf/2505.15649.pdf</a></span>   <span><a href='https://github.com/pd162/LTB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianjiao Cao, Jiahao Lyu, Weichao Zeng, Weimin Mu, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15649">The Devil is in Fine-tuning and Long-tailed Problems:A New Benchmark for Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text detection has seen the emergence of high-performing methods that excel on academic benchmarks. However, these detectors often fail to replicate such success in real-world scenarios. We uncover two key factors contributing to this discrepancy through extensive experiments. First, a \textit{Fine-tuning Gap}, where models leverage \textit{Dataset-Specific Optimization} (DSO) paradigm for one domain at the cost of reduced effectiveness in others, leads to inflated performances on academic benchmarks. Second, the suboptimal performance in practical settings is primarily attributed to the long-tailed distribution of texts, where detectors struggle with rare and complex categories as artistic or overlapped text. Given that the DSO paradigm might undermine the generalization ability of models, we advocate for a \textit{Joint-Dataset Learning} (JDL) protocol to alleviate the Fine-tuning Gap. Additionally, an error analysis is conducted to identify three major categories and 13 subcategories of challenges in long-tailed scene text, upon which we propose a Long-Tailed Benchmark (LTB). LTB facilitates a comprehensive evaluation of ability to handle a diverse range of long-tailed challenges. We further introduce MAEDet, a self-supervised learning-based method, as a strong baseline for LTB. The code is available at https://github.com/pd162/LTB.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2505.03329.pdf' target='_blank'>https://arxiv.org/pdf/2505.03329.pdf</a></span>   <span><a href='https://github.com/AMAP-ML/FluxText' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Lan, Yancheng Bai, Xu Duan, Mingxing Li, Dongyang Jin, Ryan Xu, Lei Sun, Xiangxiang Chu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03329">FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text editing aims to modify or add texts on images while ensuring text fidelity and overall visual quality consistent with the background. Recent methods are primarily built on UNet-based diffusion models, which have improved scene text editing results, but still struggle with complex glyph structures, especially for non-Latin ones (\eg, Chinese, Korean, Japanese). To address these issues, we present \textbf{FLUX-Text}, a simple and advanced multilingual scene text editing DiT method. Specifically, our FLUX-Text enhances glyph understanding and generation through lightweight Visual and Text Embedding Modules, while preserving the original generative capability of FLUX. We further propose a Regional Text Perceptual Loss tailored for text regions, along with a matching two-stage training strategy to better balance text editing and overall image quality. Benefiting from the DiT-based architecture and lightweight feature injection modules, FLUX-Text can be trained with only $0.1$M training examples, a \textbf{97\%} reduction compared to $2.9$M required by popular methods. Extensive experiments on multiple public datasets, including English and Chinese benchmarks, demonstrate that our method surpasses other methods in visual quality and text fidelity. All the code is available at https://github.com/AMAP-ML/FluxText.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2504.09966.pdf' target='_blank'>https://arxiv.org/pdf/2504.09966.pdf</a></span>   <span><a href='https://github.com/DrLuo/SemiETS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongliang Luo, Hanshen Zhu, Ziyang Zhang, Dingkang Liang, Xudong Xie, Yuliang Liu, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09966">SemiETS: Integrating Spatial and Content Consistencies for Semi-Supervised End-to-end Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most previous scene text spotting methods rely on high-quality manual annotations to achieve promising performance. To reduce their expensive costs, we study semi-supervised text spotting (SSTS) to exploit useful information from unlabeled images. However, directly applying existing semi-supervised methods of general scenes to SSTS will face new challenges: 1) inconsistent pseudo labels between detection and recognition tasks, and 2) sub-optimal supervisions caused by inconsistency between teacher/student. Thus, we propose a new Semi-supervised framework for End-to-end Text Spotting, namely SemiETS that leverages the complementarity of text detection and recognition. Specifically, it gradually generates reliable hierarchical pseudo labels for each task, thereby reducing noisy labels. Meanwhile, it extracts important information in locations and transcriptions from bidirectional flows to improve consistency. Extensive experiments on three datasets under various settings demonstrate the effectiveness of SemiETS on arbitrary-shaped text. For example, it outperforms previous state-of-the-art SSL methods by a large margin on end-to-end spotting (+8.7%, +5.6%, and +2.6% H-mean under 0.5%, 1%, and 2% labeled data settings on Total-Text, respectively). More importantly, it still improves upon a strongly supervised text spotter trained with plenty of labeled data by 2.0%. Compelling domain adaptation ability shows practical potential. Moreover, our method demonstrates consistent improvement on different text spotters.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2503.08387.pdf' target='_blank'>https://arxiv.org/pdf/2503.08387.pdf</a></span>   <span><a href='https://github.com/ZhengyaoFang/RS-STE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyao Fang, Pengyuan Lyu, Jingjing Wu, Chengquan Zhang, Jun Yu, Guangming Lu, Wenjie Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08387">Recognition-Synergistic Scene Text Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text editing aims to modify text content within scene images while maintaining style consistency. Traditional methods achieve this by explicitly disentangling style and content from the source image and then fusing the style with the target content, while ensuring content consistency using a pre-trained recognition model. Despite notable progress, these methods suffer from complex pipelines, leading to suboptimal performance in complex scenarios. In this work, we introduce Recognition-Synergistic Scene Text Editing (RS-STE), a novel approach that fully exploits the intrinsic synergy of text recognition for editing. Our model seamlessly integrates text recognition with text editing within a unified framework, and leverages the recognition model's ability to implicitly disentangle style and content while ensuring content consistency. Specifically, our approach employs a multi-modal parallel decoder based on transformer architecture, which predicts both text content and stylized images in parallel. Additionally, our cyclic self-supervised fine-tuning strategy enables effective training on unpaired real-world data without ground truth, enhancing style and content consistency through a twice-cyclic generation process. Built on a relatively simple architecture, RS-STE achieves state-of-the-art performance on both synthetic and real-world benchmarks, and further demonstrates the effectiveness of leveraging the generated hard cases to boost the performance of downstream recognition tasks. Code is available at https://github.com/ZhengyaoFang/RS-STE.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2503.06501.pdf' target='_blank'>https://arxiv.org/pdf/2503.06501.pdf</a></span>   <span><a href='https://github.com/HqiTao/TextInPlace' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaqi Tao, Bingxi Liu, Calvin Chen, Tingjun Huang, He Li, Jinqiang Cui, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06501">TextInPlace: Indoor Visual Place Recognition in Repetitive Structures with Scene Text Spotting and Verification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Place Recognition (VPR) is a crucial capability for long-term autonomous robots, enabling them to identify previously visited locations using visual information. However, existing methods remain limited in indoor settings due to the highly repetitive structures inherent in such environments. We observe that scene texts frequently appear in indoor spaces and can help distinguish visually similar but different places. This inspires us to propose TextInPlace, a simple yet effective VPR framework that integrates Scene Text Spotting (STS) to mitigate visual perceptual ambiguity in repetitive indoor environments. Specifically, TextInPlace adopts a dual-branch architecture within a local parameter sharing network. The VPR branch employs attention-based aggregation to extract global descriptors for coarse-grained retrieval, while the STS branch utilizes a bridging text spotter to detect and recognize scene texts. Finally, the discriminative texts are filtered to compute text similarity and re-rank the top-K retrieved images. To bridge the gap between current text-based repetitive indoor scene datasets and the typical scenarios encountered in robot navigation, we establish an indoor VPR benchmark dataset, called Maze-with-Text. Extensive experiments on both custom and public datasets demonstrate that TextInPlace achieves superior performance over existing methods that rely solely on appearance information. The dataset, code, and trained models are publicly available at https://github.com/HqiTao/TextInPlace.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2502.09020.pdf' target='_blank'>https://arxiv.org/pdf/2502.09020.pdf</a></span>   <span><a href='https://github.com/Event-AHU/EventSTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Jingtao Jiang, Dong Li, Futian Wang, Lin Zhu, Yaowei Wang, Yongyong Tian, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09020">EventSTR: A Benchmark Dataset and Baselines for Event Stream based Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mainstream Scene Text Recognition (STR) algorithms are developed based on RGB cameras which are sensitive to challenging factors such as low illumination, motion blur, and cluttered backgrounds. In this paper, we propose to recognize the scene text using bio-inspired event cameras by collecting and annotating a large-scale benchmark dataset, termed EventSTR. It contains 9,928 high-definition (1280 * 720) event samples and involves both Chinese and English characters. We also benchmark multiple STR algorithms as the baselines for future works to compare. In addition, we propose a new event-based scene text recognition framework, termed SimC-ESTR. It first extracts the event features using a visual encoder and projects them into tokens using a Q-former module. More importantly, we propose to augment the vision tokens based on a memory mechanism before feeding into the large language models. A similarity-based error correction mechanism is embedded within the large language model to correct potential minor errors fundamentally based on contextual information. Extensive experiments on the newly proposed EventSTR dataset and two simulation STR datasets fully demonstrate the effectiveness of our proposed model. We believe that the dataset and algorithmic model can innovatively propose an event-based STR task and are expected to accelerate the application of event cameras in various industries. The source code and pre-trained models will be released on https://github.com/Event-AHU/EventSTR
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2502.07411.pdf' target='_blank'>https://arxiv.org/pdf/2502.07411.pdf</a></span>   <span><a href='https://github.com/zhousheng97/EgoTextVQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Zhou, Junbin Xiao, Qingyun Li, Yicong Li, Xun Yang, Dan Guo, Meng Wang, Tat-Seng Chua, Angela Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07411">EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce EgoTextVQA, a novel and rigorously constructed benchmark for egocentric QA assistance involving scene text. EgoTextVQA contains 1.5K ego-view videos and 7K scene-text aware questions that reflect real user needs in outdoor driving and indoor house-keeping activities. The questions are designed to elicit identification and reasoning on scene text in an egocentric and dynamic environment. With EgoTextVQA, we comprehensively evaluate 10 prominent multimodal large language models. Currently, all models struggle, and the best results (Gemini 1.5 Pro) are around 33\% accuracy, highlighting the severe deficiency of these techniques in egocentric QA assistance. Our further investigations suggest that precise temporal grounding and multi-frame reasoning, along with high resolution and auxiliary scene-text inputs, are key for better performance. With thorough analyses and heuristic suggestions, we hope EgoTextVQA can serve as a solid testbed for research in egocentric scene-text QA assistance. Our dataset is released at: https://github.com/zhousheng97/EgoTextVQA.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2412.17007.pdf' target='_blank'>https://arxiv.org/pdf/2412.17007.pdf</a></span>   <span><a href='https://yejy53.github.io/CVG-Text/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyan Ye, Honglin Lin, Leyan Ou, Dairong Chen, Zihao Wang, Qi Zhu, Conghui He, Weijia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17007">Where am I? Cross-View Geo-localization with Natural Language Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-view geo-localization identifies the locations of street-view images by matching them with geo-tagged satellite images or OSM. However, most existing studies focus on image-to-image retrieval, with fewer addressing text-guided retrieval, a task vital for applications like pedestrian navigation and emergency response. In this work, we introduce a novel task for cross-view geo-localization with natural language descriptions, which aims to retrieve corresponding satellite images or OSM database based on scene text descriptions. To support this task, we construct the CVG-Text dataset by collecting cross-view data from multiple cities and employing a scene text generation approach that leverages the annotation capabilities of Large Multimodal Models to produce high-quality scene text descriptions with localization details. Additionally, we propose a novel text-based retrieval localization method, CrossText2Loc, which improves recall by 10% and demonstrates excellent long-text retrieval capabilities. In terms of explainability, it not only provides similarity scores but also offers retrieval reasons. More information can be found at https://yejy53.github.io/CVG-Text/ .
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2412.01137.pdf' target='_blank'>https://arxiv.org/pdf/2412.01137.pdf</a></span>   <span><a href='https://github.com/YesianRohn/TextSSR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingsong Ye, Yongkun Du, Yunbo Tao, Zhineng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01137">TextSSR: Diffusion-based Data Synthesis for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition (STR) suffers from challenges of either less realistic synthetic training data or the difficulty of collecting sufficient high-quality real-world data, limiting the effectiveness of trained models. Meanwhile, despite producing holistically appealing text images, diffusion-based visual text generation methods struggle to synthesize accurate and realistic instance-level text at scale. To tackle this, we introduce TextSSR: a novel pipeline for Synthesizing Scene Text Recognition training data. TextSSR targets three key synthesizing characteristics: accuracy, realism, and scalability. It achieves accuracy through a proposed region-centric text generation with position-glyph enhancement, ensuring proper character placement. It maintains realism by guiding style and appearance generation using contextual hints from surrounding text or background. This character-aware diffusion architecture enjoys precise character-level control and semantic coherence preservation, without relying on natural language prompts. Therefore, TextSSR supports large-scale generation through combinatorial text permutations. Based on these, we present TextSSR-F, a dataset of 3.55 million quality-screened text instances. Extensive experiments show that STR models trained on TextSSR-F outperform those trained on existing synthetic datasets by clear margins on common benchmarks, and further improvements are observed when mixed with real-world training data. Code is available at https://github.com/YesianRohn/TextSSR.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2411.15858.pdf' target='_blank'>https://arxiv.org/pdf/2411.15858.pdf</a></span>   <span><a href='https://github.com/Topdu/OpenOCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongkun Du, Zhineng Chen, Hongtao Xie, Caiyan Jia, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15858">SVTRv2: CTC Beats Encoder-Decoder Models in Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connectionist temporal classification (CTC)-based scene text recognition (STR) methods, e.g., SVTR, are widely employed in OCR applications, mainly due to their simple architecture, which only contains a visual model and a CTC-aligned linear classifier, and therefore fast inference. However, they generally exhibit worse accuracy than encoder-decoder-based methods (EDTRs) due to struggling with text irregularity and linguistic missing. To address these challenges, we propose SVTRv2, a CTC model endowed with the ability to handle text irregularities and model linguistic context. First, a multi-size resizing strategy is proposed to resize text instances to appropriate predefined sizes, effectively avoiding severe text distortion. Meanwhile, we introduce a feature rearrangement module to ensure that visual features accommodate the requirement of CTC, thus alleviating the alignment puzzle. Second, we propose a semantic guidance module. It integrates linguistic context into the visual features, allowing CTC model to leverage language information for accuracy improvement. This module can be omitted at the inference stage and would not increase the time cost. We extensively evaluate SVTRv2 in both standard and recent challenging benchmarks, where SVTRv2 is fairly compared to popular STR models across multiple scenarios, including different types of text irregularity, languages, long text, and whether employing pretraining. SVTRv2 surpasses most EDTRs across the scenarios in terms of accuracy and inference speed. Code: https://github.com/Topdu/OpenOCR.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2411.15585.pdf' target='_blank'>https://arxiv.org/pdf/2411.15585.pdf</a></span>   <span><a href='https://github.com/qqqyd/ViSu' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yadong Qu, Yuxin Wang, Bangbang Zhou, Zixiao Wang, Hongtao Xie, Yongdong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15585">Boosting Semi-Supervised Scene Text Recognition via Viewing and Summarizing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing scene text recognition (STR) methods struggle to recognize challenging texts, especially for artistic and severely distorted characters. The limitation lies in the insufficient exploration of character morphologies, including the monotonousness of widely used synthetic training data and the sensitivity of the model to character morphologies. To address these issues, inspired by the human learning process of viewing and summarizing, we facilitate the contrastive learning-based STR framework in a self-motivated manner by leveraging synthetic and real unlabeled data without any human cost. In the viewing process, to compensate for the simplicity of synthetic data and enrich character morphology diversity, we propose an Online Generation Strategy to generate background-free samples with diverse character styles. By excluding background noise distractions, the model is encouraged to focus on character morphology and generalize the ability to recognize complex samples when trained with only simple synthetic data. To boost the summarizing process, we theoretically demonstrate the derivation error in the previous character contrastive loss, which mistakenly causes the sparsity in the intra-class distribution and exacerbates ambiguity on challenging samples. Therefore, a new Character Unidirectional Alignment Loss is proposed to correct this error and unify the representation of the same characters in all samples by aligning the character features in the student model with the reference features in the teacher model. Extensive experiment results show that our method achieves SOTA performance (94.7\% and 70.9\% average accuracy on common benchmarks and Union14M-Benchmark). Code will be available at https://github.com/qqqyd/ViSu.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2411.11219.pdf' target='_blank'>https://arxiv.org/pdf/2411.11219.pdf</a></span>   <span><a href='https://github.com/ThunderVVV/RCMSTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiancheng Lin, Jinglei Zhang, Yi Xu, Kai Chen, Rui Zhang, Chang-Wen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11219">Relational Contrastive Learning and Masked Image Modeling for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Context-aware methods have achieved remarkable advancements in supervised scene text recognition by leveraging semantic priors from words. Considering the heterogeneity of text and background in STR, we propose that such contextual priors can be reinterpreted as the relations between textual elements, serving as effective self-supervised labels for representation learning. However, textual relations are restricted to the finite size of the dataset due to lexical dependencies, which causes over-fitting problem, thus compromising the representation quality. To address this, our work introduces a unified framework of Relational Contrastive Learning and Masked Image Modeling for STR (RCMSTR), which explicitly models the enriched textual relations. For the RCL branch, we first introduce the relational rearrangement module to cultivate new relations on the fly. Based on this, we further conduct relational contrastive learning to model the intra- and inter-hierarchical relations for frames, sub-words and words. On the other hand, MIM can naturally boost the context information via masking, where we find that the block masking strategy is more effective for STR. For the effective integration of RCL and MIM, we also introduce a novel decoupling design aimed at mitigating the impact of masked images on contrastive learning. Additionally, to enhance the compatibility of MIM with CNNs, we propose the adoption of sparse convolutions and directly sharing the weights with dense convolutions in training. The proposed RCMSTR demonstrates superior performance in various evaluation protocols for different STR-related downstream tasks, outperforming the existing state-of-the-art self-supervised STR techniques. Ablation studies and qualitative experimental results further validate the effectiveness of our method. The code and pre-trained models will be available at https://github.com/ThunderVVV/RCMSTR .
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2411.10261.pdf' target='_blank'>https://arxiv.org/pdf/2411.10261.pdf</a></span>   <span><a href='https://github.com/lanfeng4659/PSTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wang, Minghui Liao, Zhouyi Xie, Wenyu Liu, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10261">Partial Scene Text Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of partial scene text retrieval involves localizing and searching for text instances that are the same or similar to a given query text from an image gallery. However, existing methods can only handle text-line instances, leaving the problem of searching for partial patches within these text-line instances unsolved due to a lack of patch annotations in the training data. To address this issue, we propose a network that can simultaneously retrieve both text-line instances and their partial patches. Our method embeds the two types of data (query text and scene text instances) into a shared feature space and measures their cross-modal similarities. To handle partial patches, our proposed approach adopts a Multiple Instance Learning (MIL) approach to learn their similarities with query text, without requiring extra annotations. However, constructing bags, which is a standard step of conventional MIL approaches, can introduce numerous noisy samples for training, and lower inference speed. To address this issue, we propose a Ranking MIL (RankMIL) approach to adaptively filter those noisy samples. Additionally, we present a Dynamic Partial Match Algorithm (DPMA) that can directly search for the target partial patch from a text-line instance during the inference stage, without requiring bags. This greatly improves the search efficiency and the performance of retrieving partial patches. The source code and dataset are available at https://github.com/lanfeng4659/PSTR.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2411.02794.pdf' target='_blank'>https://arxiv.org/pdf/2411.02794.pdf</a></span>   <span><a href='https://github.com/fengmulin/SMNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Han, Junyu Gao, Chuang Yang, Yuan Yuan, Qi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02794">Real-Time Text Detection with Similar Mask in Traffic, Industrial, and Natural Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Texts on the intelligent transportation scene include mass information. Fully harnessing this information is one of the critical drivers for advancing intelligent transportation. Unlike the general scene, detecting text in transportation has extra demand, such as a fast inference speed, except for high accuracy. Most existing real-time text detection methods are based on the shrink mask, which loses some geometry semantic information and needs complex post-processing. In addition, the previous method usually focuses on correct output, which ignores feature correction and lacks guidance during the intermediate process. To this end, we propose an efficient multi-scene text detector that contains an effective text representation similar mask (SM) and a feature correction module (FCM). Unlike previous methods, the former aims to preserve the geometric information of the instances as much as possible. Its post-progressing saves 50$\%$ of the time, accurately and efficiently reconstructing text contours. The latter encourages false positive features to move away from the positive feature center, optimizing the predictions from the feature level. Some ablation studies demonstrate the efficiency of the SM and the effectiveness of the FCM. Moreover, the deficiency of existing traffic datasets (such as the low-quality annotation or closed source data unavailability) motivated us to collect and annotate a traffic text dataset, which introduces motion blur. In addition, to validate the scene robustness of the SM-Net, we conduct experiments on traffic, industrial, and natural scene datasets. Extensive experiments verify it achieves (SOTA) performance on several benchmarks. The code and dataset are available at: \url{https://github.com/fengmulin/SMNet}.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2410.16163.pdf' target='_blank'>https://arxiv.org/pdf/2410.16163.pdf</a></span>   <span><a href='https://github.com/jefferyZhan/Griffon' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhan, Hongyin Zhao, Yousong Zhu, Fan Yang, Ming Tang, Jinqiao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16163">Griffon-G: Bridging Vision-Language and Vision-Centric Tasks via Large Multimodal Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Multimodal Models (LMMs) have achieved significant breakthroughs in various vision-language and vision-centric tasks based on auto-regressive modeling. However, these models typically focus on either vision-centric tasks, such as visual grounding and region description, or vision-language tasks, like image caption and multi-scenario VQAs. None of the LMMs have yet comprehensively unified both types of tasks within a single model, as seen in Large Language Models in the natural language processing field. Furthermore, even with abundant multi-task instruction-following data, directly stacking these data for universal capabilities extension remains challenging. To address these issues, we introduce a novel multi-dimension curated and consolidated multimodal dataset, named CCMD-8M, which overcomes the data barriers of unifying vision-centric and vision-language tasks through multi-level data curation and multi-task consolidation. More importantly, we present Griffon-G, a general large multimodal model that addresses both vision-centric and vision-language tasks within a single end-to-end paradigm. Griffon-G resolves the training collapse issue encountered during the joint optimization of these tasks, achieving better training efficiency. Evaluations across multimodal benchmarks, general Visual Question Answering (VQA) tasks, scene text-centric VQA tasks, document-related VQA tasks, Referring Expression Comprehension, and object detection demonstrate that Griffon-G surpasses the advanced LMMs and achieves expert-level performance in complicated vision-centric tasks.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2410.15869.pdf' target='_blank'>https://arxiv.org/pdf/2410.15869.pdf</a></span>   <span><a href='https://github.com/TongxingJin/TXTLCD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongxing Jin, Thien-Minh Nguyen, Xinhang Xu, Yizhuo Yang, Shenghai Yuan, Jianping Li, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15869">Robust Loop Closure by Textual Cues in Challenging Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Loop closure is an important task in robot navigation. However, existing methods mostly rely on some implicit or heuristic features of the environment, which can still fail to work in common environments such as corridors, tunnels, and warehouses. Indeed, navigating in such featureless, degenerative, and repetitive (FDR) environments would also pose a significant challenge even for humans, but explicit text cues in the surroundings often provide the best assistance. This inspires us to propose a multi-modal loop closure method based on explicit human-readable textual cues in FDR environments. Specifically, our approach first extracts scene text entities based on Optical Character Recognition (OCR), then creates a local map of text cues based on accurate LiDAR odometry and finally identifies loop closure events by a graph-theoretic scheme. Experiment results demonstrate that this approach has superior performance over existing methods that rely solely on visual and LiDAR sensors. To benefit the community, we release the source code and datasets at \url{https://github.com/TongxingJin/TXTLCD}.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2410.11538.pdf' target='_blank'>https://arxiv.org/pdf/2410.11538.pdf</a></span>   <span><a href='https://github.com/xfey/MCTBench?tab=readme-ov-file' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Shan, Xiang Fei, Wei Shi, An-Lan Wang, Guozhi Tang, Lei Liao, Jingqun Tang, Xiang Bai, Can Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11538">MCTBench: Multimodal Cognition towards Text-Rich Visual Scenes Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The comprehension of text-rich visual scenes has become a focal point for evaluating Multi-modal Large Language Models (MLLMs) due to their widespread applications. Current benchmarks tailored to the scenario emphasize perceptual capabilities, while overlooking the assessment of cognitive abilities. To address this limitation, we introduce a Multimodal benchmark towards Text-rich visual scenes, to evaluate the Cognitive capabilities of MLLMs through visual reasoning and content-creation tasks (MCTBench). To mitigate potential evaluation bias from the varying distributions of datasets, MCTBench incorporates several perception tasks (e.g., scene text recognition) to ensure a consistent comparison of both the cognitive and perceptual capabilities of MLLMs. To improve the efficiency and fairness of content-creation evaluation, we conduct an automatic evaluation pipeline. Evaluations of various MLLMs on MCTBench reveal that, despite their impressive perceptual capabilities, their cognition abilities require enhancement. We hope MCTBench will offer the community an efficient resource to explore and enhance cognitive capabilities towards text-rich visual scenes.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2410.10168.pdf' target='_blank'>https://arxiv.org/pdf/2410.10168.pdf</a></span>   <span><a href='https://github.com/Zhenhang-Li/GlyphOnly' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhang Li, Yan Shu, Weichao Zeng, Dongbao Yang, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10168">First Creating Backgrounds Then Rendering Texts: A New Paradigm for Visual Text Blending</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models, known for their impressive image generation abilities, have played a pivotal role in the rise of visual text generation. Nevertheless, existing visual text generation methods often focus on generating entire images with text prompts, leading to imprecise control and limited practicality. A more promising direction is visual text blending, which focuses on seamlessly merging texts onto text-free backgrounds. However, existing visual text blending methods often struggle to generate high-fidelity and diverse images due to a shortage of backgrounds for synthesis and limited generalization capabilities. To overcome these challenges, we propose a new visual text blending paradigm including both creating backgrounds and rendering texts. Specifically, a background generator is developed to produce high-fidelity and text-free natural images. Moreover, a text renderer named GlyphOnly is designed for achieving visually plausible text-background integration. GlyphOnly, built on a Stable Diffusion framework, utilizes glyphs and backgrounds as conditions for accurate rendering and consistency control, as well as equipped with an adaptive text block exploration strategy for small-scale text rendering. We also explore several downstream applications based on our method, including scene text dataset synthesis for boosting scene text detectors, as well as text image customization and editing. Code and model will be available at \url{https://github.com/Zhenhang-Li/GlyphOnly}.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2410.09913.pdf' target='_blank'>https://arxiv.org/pdf/2410.09913.pdf</a></span>   <span><a href='https://github.com/KhaLee2307/StrDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kha Nhat Le, Hoang-Tuan Nguyen, Hung Tien Tran, Thanh Duc Ngo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09913">Stratified Domain Adaptation: A Progressive Self-Training Approach for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised domain adaptation (UDA) has become increasingly prevalent in scene text recognition (STR), especially where training and testing data reside in different domains. The efficacy of existing UDA approaches tends to degrade when there is a large gap between the source and target domains. To deal with this problem, gradually shifting or progressively learning to shift from domain to domain is the key issue. In this paper, we introduce the Stratified Domain Adaptation (StrDA) approach, which examines the gradual escalation of the domain gap for the learning process. The objective is to partition the training data into subsets so that the progressively self-trained model can adapt to gradual changes. We stratify the training data by evaluating the proximity of each data sample to both the source and target domains. We propose a novel method for employing domain discriminators to estimate the out-of-distribution and domain discriminative levels of data samples. Extensive experiments on benchmark scene-text datasets show that our approach significantly improves the performance of baseline (source-trained) STR models.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2409.14319.pdf' target='_blank'>https://arxiv.org/pdf/2409.14319.pdf</a></span>   <span><a href='https://github.com/zhousheng97/ViTXT-GQA.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Zhou, Junbin Xiao, Xun Yang, Peipei Song, Dan Guo, Angela Yao, Meng Wang, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14319">Scene-Text Grounding for Text-Based Video Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing efforts in text-based video question answering (TextVideoQA) are criticized for their opaque decisionmaking and heavy reliance on scene-text recognition. In this paper, we propose to study Grounded TextVideoQA by forcing models to answer questions and spatio-temporally localize the relevant scene-text regions, thus decoupling QA from scenetext recognition and promoting research towards interpretable QA. The task has three-fold significance. First, it encourages scene-text evidence versus other short-cuts for answer predictions. Second, it directly accepts scene-text regions as visual answers, thus circumventing the problem of ineffective answer evaluation by stringent string matching. Third, it isolates the challenges inherited in VideoQA and scene-text recognition. This enables the diagnosis of the root causes for failure predictions, e.g., wrong QA or wrong scene-text recognition? To achieve Grounded TextVideoQA, we propose the T2S-QA model that highlights a disentangled temporal-to-spatial contrastive learning strategy for weakly-supervised scene-text grounding and grounded TextVideoQA. To facilitate evaluation, we construct a new dataset ViTXT-GQA which features 52K scene-text bounding boxes within 2.2K temporal segments related to 2K questions and 729 videos. With ViTXT-GQA, we perform extensive experiments and demonstrate the severe limitations of existing techniques in Grounded TextVideoQA. While T2S-QA achieves superior results, the large performance gap with human leaves ample space for improvement. Our further analysis of oracle scene-text inputs posits that the major challenge is scene-text recognition. To advance the research of Grounded TextVideoQA, our dataset and code are at https://github.com/zhousheng97/ViTXT-GQA.git
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2409.13431.pdf' target='_blank'>https://arxiv.org/pdf/2409.13431.pdf</a></span>   <span><a href='https://github.com/wzx99/TMIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixiao Wang, Hongtao Xie, YuXin Wang, Yadong Qu, Fengjun Guo, Pengwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13431">Leveraging Text Localization for Scene Text Removal via Text-aware Masked Image Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing scene text removal (STR) task suffers from insufficient training data due to the expensive pixel-level labeling. In this paper, we aim to address this issue by introducing a Text-aware Masked Image Modeling algorithm (TMIM), which can pretrain STR models with low-cost text detection labels (e.g., text bounding box). Different from previous pretraining methods that use indirect auxiliary tasks only to enhance the implicit feature extraction ability, our TMIM first enables the STR task to be directly trained in a weakly supervised manner, which explores the STR knowledge explicitly and efficiently. In TMIM, first, a Background Modeling stream is built to learn background generation rules by recovering the masked non-text region. Meanwhile, it provides pseudo STR labels on the masked text region. Second, a Text Erasing stream is proposed to learn from the pseudo labels and equip the model with end-to-end STR ability. Benefiting from the two collaborative streams, our STR model can achieve impressive performance only with the public text detection datasets, which greatly alleviates the limitation of the high-cost STR labels. Experiments demonstrate that our method outperforms other pretrain methods and achieves state-of-the-art performance (37.35 PSNR on SCUT-EnsText). Code will be available at https://github.com/wzx99/TMIM.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2408.14805.pdf' target='_blank'>https://arxiv.org/pdf/2408.14805.pdf</a></span>   <span><a href='https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/Platypus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Wang, Zhaohai Li, Jun Tang, Humen Zhong, Fei Huang, Zhibo Yang, Cong Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14805">Platypus: A Generalized Specialist Model for Reading Text in Various Forms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reading text from images (either natural scenes or documents) has been a long-standing research topic for decades, due to the high technical challenge and wide application range. Previously, individual specialist models are developed to tackle the sub-tasks of text reading (e.g., scene text recognition, handwritten text recognition and mathematical expression recognition). However, such specialist models usually cannot effectively generalize across different sub-tasks. Recently, generalist models (such as GPT-4V), trained on tremendous data in a unified way, have shown enormous potential in reading text in various scenarios, but with the drawbacks of limited accuracy and low efficiency. In this work, we propose Platypus, a generalized specialist model for text reading. Specifically, Platypus combines the best of both worlds: being able to recognize text of various forms with a single unified architecture, while achieving excellent accuracy and high efficiency. To better exploit the advantage of Platypus, we also construct a text reading dataset (called Worms), the images of which are curated from previous datasets and partially re-labeled. Experiments on standard benchmarks demonstrate the effectiveness and superiority of the proposed Platypus model. Model and data will be made publicly available at https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/Platypus.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2408.05706.pdf' target='_blank'>https://arxiv.org/pdf/2408.05706.pdf</a></span>   <span><a href='https://github.com/Topdu/OpenOCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Zhao, Yongkun Du, Zhineng Chen, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05706">Decoder Pre-Training with only Text for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition (STR) pre-training methods have achieved remarkable progress, primarily relying on synthetic datasets. However, the domain gap between synthetic and real images poses a challenge in acquiring feature representations that align well with images on real scenes, thereby limiting the performance of these methods. We note that vision-language models like CLIP, pre-trained on extensive real image-text pairs, effectively align images and text in a unified embedding space, suggesting the potential to derive the representations of real images from text alone. Building upon this premise, we introduce a novel method named Decoder Pre-training with only text for STR (DPTR). DPTR treats text embeddings produced by the CLIP text encoder as pseudo visual embeddings and uses them to pre-train the decoder. An Offline Randomized Perturbation (ORP) strategy is introduced. It enriches the diversity of text embeddings by incorporating natural image embeddings extracted from the CLIP image encoder, effectively directing the decoder to acquire the potential representations of real images. In addition, we introduce a Feature Merge Unit (FMU) that guides the extracted visual embeddings focusing on the character foreground within the text image, thereby enabling the pre-trained decoder to work more efficiently and accurately. Extensive experiments across various STR decoders and language recognition tasks underscore the broad applicability and remarkable performance of DPTR, providing a novel insight for STR pre-training. Code is available at https://github.com/Topdu/OpenOCR
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2408.00441.pdf' target='_blank'>https://arxiv.org/pdf/2408.00441.pdf</a></span>   <span><a href='https://github.com/Gyann-z/FDP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gangyan Zeng, Yuan Zhang, Jin Wei, Dongbao Yang, Peng Zhang, Yiwen Gao, Xugong Qin, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00441">Focus, Distinguish, and Prompt: Unleashing CLIP for Efficient and Flexible Scene Text Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text retrieval aims to find all images containing the query text from an image gallery. Current efforts tend to adopt an Optical Character Recognition (OCR) pipeline, which requires complicated text detection and/or recognition processes, resulting in inefficient and inflexible retrieval. Different from them, in this work we propose to explore the intrinsic potential of Contrastive Language-Image Pre-training (CLIP) for OCR-free scene text retrieval. Through empirical analysis, we observe that the main challenges of CLIP as a text retriever are: 1) limited text perceptual scale, and 2) entangled visual-semantic concepts. To this end, a novel model termed FDP (Focus, Distinguish, and Prompt) is developed. FDP first focuses on scene text via shifting the attention to the text area and probing the hidden text knowledge, and then divides the query text into content word and function word for processing, in which a semantic-aware prompting scheme and a distracted queries assistance module are utilized. Extensive experiments show that FDP significantly enhances the inference speed while achieving better or competitive retrieval accuracy compared to existing methods. Notably, on the IIIT-STR benchmark, FDP surpasses the state-of-the-art model by 4.37% with a 4 times faster speed. Furthermore, additional experiments under phrase-level and attribute-aware scene text retrieval settings validate FDP's particular advantages in handling diverse forms of query text. The source code will be publicly available at https://github.com/Gyann-z/FDP.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2407.21422.pdf' target='_blank'>https://arxiv.org/pdf/2407.21422.pdf</a></span>   <span><a href='https://github.com/qcf-568/OSTF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenfan Qu, Yiwu Zhong, Fengjun Guo, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21422">Revisiting Tampered Scene Text Detection in the Era of Generative AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancements of generative AI have fueled the potential of generative text image editing, meanwhile escalating the threat of misinformation spreading. However, existing forensics methods struggle to detect unseen forgery types that they have not been trained on, underscoring the need for a model capable of generalized detection of tampered scene text. To tackle this, we propose a novel task: open-set tampered scene text detection, which evaluates forensics models on their ability to identify both seen and previously unseen forgery types. We have curated a comprehensive, high-quality dataset, featuring the texts tampered by eight text editing models, to thoroughly assess the open-set generalization capabilities. Further, we introduce a novel and effective training paradigm that subtly alters the texture of selected texts within an image and trains the model to identify these regions. This approach not only mitigates the scarcity of high-quality training data but also enhances models' fine-grained perception and open-set generalization abilities. Additionally, we present DAF, a novel framework that improves open-set generalization by distinguishing between the features of authentic and tampered text, rather than focusing solely on the tampered text's features. Our extensive experiments validate the remarkable efficacy of our methods. For example, our zero-shot performance can even beat the previous state-of-the-art full-shot model by a large margin. Our dataset and code are available at https://github.com/qcf-568/OSTF.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2407.18616.pdf' target='_blank'>https://arxiv.org/pdf/2407.18616.pdf</a></span>   <span><a href='https://github.com/lancercat/Moose/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Liu, Simon CorbillÃ©, Elisa H Barney Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18616">MOoSE: Multi-Orientation Sharing Experts for Open-set Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-set text recognition, which aims to address both novel characters and previously seen ones, is one of the rising subtopics in the text recognition field. However, the current open-set text recognition solutions only focuses on horizontal text, which fail to model the real-life challenges posed by the variety of writing directions in real-world scene text. Multi-orientation text recognition, in general, faces challenges from the diverse image aspect ratios, significant imbalance in data amount, and domain gaps between orientations. In this work, we first propose a Multi-Oriented Open-Set Text Recognition task (MOOSTR) to model the challenges of both novel characters and writing direction variety. We then propose a Multi-Orientation Sharing Experts (MOoSE) framework as a strong baseline solution. MOoSE uses a mixture-of-experts scheme to alleviate the domain gaps between orientations, while exploiting common structural knowledge among experts to alleviate the data scarcity that some experts face. The proposed MOoSE framework is validated by ablative experiments, and also tested for feasibility on the existing open-set benchmark. Code, models, and documents are available at: https://github.com/lancercat/Moose/
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2407.12317.pdf' target='_blank'>https://arxiv.org/pdf/2407.12317.pdf</a></span>   <span><a href='https://github.com/Topdu/OpenOCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongkun Du, Zhineng Chen, Caiyan Jia, Xieping Gao, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12317">Out of Length Text Recognition with Sub-String Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Text Recognition (STR) methods have demonstrated robust performance in word-level text recognition. However, in real applications the text image is sometimes long due to detected with multiple horizontal words. It triggers the requirement to build long text recognition models from readily available short (i.e., word-level) text datasets, which has been less studied previously. In this paper, we term this task Out of Length (OOL) text recognition. We establish the first Long Text Benchmark (LTB) to facilitate the assessment of different methods in long text recognition. Meanwhile, we propose a novel method called OOL Text Recognition with sub-String Matching (SMTR). SMTR comprises two cross-attention-based modules: one encodes a sub-string containing multiple characters into next and previous queries, and the other employs the queries to attend to the image features, matching the sub-string and simultaneously recognizing its next and previous character. SMTR can recognize text of arbitrary length by iterating the process above. To avoid being trapped in recognizing highly similar sub-strings, we introduce a regularization training to compel SMTR to effectively discover subtle differences between similar sub-strings for precise matching. In addition, we propose an inference augmentation strategy to alleviate confusion caused by identical sub-strings in the same text and improve the overall recognition efficiency. Extensive experimental results reveal that SMTR, even when trained exclusively on short text, outperforms existing methods in public short text benchmarks and exhibits a clear advantage on LTB. Code: https://github.com/Topdu/OpenOCR.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2407.05562.pdf' target='_blank'>https://arxiv.org/pdf/2407.05562.pdf</a></span>   <span><a href='https://github.com/bang123-box/CFE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bangbang Zhou, Yadong Qu, Zixiao Wang, Zicheng Li, Boqiang Zhang, Hongtao Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05562">Focus on the Whole Character: Discriminative Character Modeling for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, scene text recognition (STR) models have shown significant performance improvements. However, existing models still encounter difficulties in recognizing challenging texts that involve factors such as severely distorted and perspective characters. These challenging texts mainly cause two problems: (1) Large Intra-Class Variance. (2) Small Inter-Class Variance. An extremely distorted character may prominently differ visually from other characters within the same category, while the variance between characters from different classes is relatively small. To address the above issues, we propose a novel method that enriches the character features to enhance the discriminability of characters. Firstly, we propose the Character-Aware Constraint Encoder (CACE) with multiple blocks stacked. CACE introduces a decay matrix in each block to explicitly guide the attention region for each token. By continuously employing the decay matrix, CACE enables tokens to perceive morphological information at the character level. Secondly, an Intra-Inter Consistency Loss (I^2CL) is introduced to consider intra-class compactness and inter-class separability at feature space. I^2CL improves the discriminative capability of features by learning a long-term memory unit for each character category. Trained with synthetic data, our model achieves state-of-the-art performance on common benchmarks (94.1% accuracy) and Union14M-Benchmark (61.6% accuracy). Code is available at https://github.com/bang123-box/CFE.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2405.14701.pdf' target='_blank'>https://arxiv.org/pdf/2405.14701.pdf</a></span>   <span><a href='https://codegoat24.github.io/DreamText/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/CodeGoat24/DreamText,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yibin Wang, Weizhong Zhang, Honghui Xu, Cheng Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14701">DreamText: High Fidelity Scene Text Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text synthesis involves rendering specified texts onto arbitrary images. Current methods typically formulate this task in an end-to-end manner but lack effective character-level guidance during training. Besides, their text encoders, pre-trained on a single font type, struggle to adapt to the diverse font styles encountered in practical applications. Consequently, these methods suffer from character distortion, repetition, and absence, particularly in polystylistic scenarios. To this end, this paper proposes DreamText for high-fidelity scene text synthesis. Our key idea is to reconstruct the diffusion training process, introducing more refined guidance tailored to this task, to expose and rectify the model's attention at the character level and strengthen its learning of text regions. This transformation poses a hybrid optimization challenge, involving both discrete and continuous variables. To effectively tackle this challenge, we employ a heuristic alternate optimization strategy. Meanwhile, we jointly train the text encoder and generator to comprehensively learn and utilize the diverse font present in the training dataset. This joint training is seamlessly integrated into the alternate optimization process, fostering a synergistic relationship between learning character embedding and re-estimating character attention. Specifically, in each step, we first encode potential character-generated position information from cross-attention maps into latent character masks. These masks are then utilized to update the representation of specific characters in the current step, which, in turn, enables the generator to correct the character's attention in the subsequent steps. Both qualitative and quantitative results demonstrate the superiority of our method to the state of the art.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2405.13896.pdf' target='_blank'>https://arxiv.org/pdf/2405.13896.pdf</a></span>   <span><a href='https://github.com/mkoshkina/jersey-number-pipeline' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Koshkina, James H. Elder
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13896">A General Framework for Jersey Number Recognition in Sports Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Jersey number recognition is an important task in sports video analysis, partly due to its importance for long-term player tracking. It can be viewed as a variant of scene text recognition. However, there is a lack of published attempts to apply scene text recognition models on jersey number data. Here we introduce a novel public jersey number recognition dataset for hockey and study how scene text recognition methods can be adapted to this problem. We address issues of occlusions and assess the degree to which training on one sport (hockey) can be generalized to another (soccer). For the latter, we also consider how jersey number recognition at the single-image level can be aggregated across frames to yield tracklet-level jersey number labels. We demonstrate high performance on image- and tracklet-level tasks, achieving 91.4% accuracy for hockey images and 87.4% for soccer tracklets. Code, models, and data are available at https://github.com/mkoshkina/jersey-number-pipeline.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2405.12533.pdf' target='_blank'>https://arxiv.org/pdf/2405.12533.pdf</a></span>   <span><a href='https://github.com/Hiba-MeiRuan/Urdu-VQA-Dataset-/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hiba Maryam, Ling Fu, Jiajun Song, Tajrian ABM Shafayet, Qidi Luo, Xiang Bai, Yuliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12533">Dataset and Benchmark for Urdu Natural Scenes Text Detection, Recognition and Visual Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of Urdu scene text detection, recognition, and Visual Question Answering (VQA) technologies is crucial for advancing accessibility, information retrieval, and linguistic diversity in digital content, facilitating better understanding and interaction with Urdu-language visual data. This initiative seeks to bridge the gap between textual and visual comprehension. We propose a new multi-task Urdu scene text dataset comprising over 1000 natural scene images, which can be used for text detection, recognition, and VQA tasks. We provide fine-grained annotations for text instances, addressing the limitations of previous datasets for facing arbitrary-shaped texts. By incorporating additional annotation points, this dataset facilitates the development and assessment of methods that can handle diverse text layouts, intricate shapes, and non-standard orientations commonly encountered in real-world scenarios. Besides, the VQA annotations make it the first benchmark for the Urdu Text VQA method, which can prompt the development of Urdu scene text understanding. The proposed dataset is available at: https://github.com/Hiba-MeiRuan/Urdu-VQA-Dataset-/tree/main
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2405.11437.pdf' target='_blank'>https://arxiv.org/pdf/2405.11437.pdf</a></span>   <span><a href='https://github.com/FadilaW/Swahili-STR-Dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fadila Wendigoundi Douamba, Jianjun Song, Ling Fu, Yuliang Liu, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11437">The First Swahili Language Scene Text Detection and Recognition Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition is essential in many applications, including automated translation, information retrieval, driving assistance, and enhancing accessibility for individuals with visual impairments. Much research has been done to improve the accuracy and performance of scene text detection and recognition models. However, most of this research has been conducted in the most common languages, English and Chinese. There is a significant gap in low-resource languages, especially the Swahili Language. Swahili is widely spoken in East African countries but is still an under-explored language in scene text recognition. No studies have been focused explicitly on Swahili natural scene text detection and recognition, and no dataset for Swahili language scene text detection and recognition is publicly available. We propose a comprehensive dataset of Swahili scene text images and evaluate the dataset on different scene text detection and recognition models. The dataset contains 976 images collected in different places and under various circumstances. Each image has its annotation at the word level. The proposed dataset can also serve as a benchmark dataset specific to the Swahili language for evaluating and comparing different approaches and fostering future research endeavors. The dataset is available on GitHub via this link: https://github.com/FadilaW/Swahili-STR-Dataset
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2405.10370.pdf' target='_blank'>https://arxiv.org/pdf/2405.10370.pdf</a></span>   <span><a href='https://groundedscenellm.github.io/grounded_3d-llm.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilun Chen, Shuai Yang, Haifeng Huang, Tai Wang, Runsen Xu, Ruiyuan Lyu, Dahua Lin, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10370">Grounded 3D-LLM with Referent Tokens</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prior studies on 3D scene understanding have primarily developed specialized models for specific tasks or required task-specific fine-tuning. In this study, we propose Grounded 3D-LLM, which explores the potential of 3D large multi-modal models (3D LMMs) to consolidate various 3D vision tasks within a unified generative framework. The model uses scene referent tokens as special noun phrases to reference 3D scenes, enabling it to handle sequences that interleave 3D and textual data. Per-task instruction-following templates are employed to ensure natural and diversity in translating 3D vision tasks into language formats. To facilitate the use of referent tokens in subsequent language modeling, we provide a large-scale, automatically curated grounded scene-text dataset with over 1 million phrase-to-region correspondences and introduce Contrastive Language-Scene Pre-training (CLASP) to perform phrase-level scene-text alignment using this data. Our comprehensive evaluation covers open-ended tasks like dense captioning and 3D question answering, alongside close-ended tasks such as object detection and language grounding. Experiments across multiple 3D benchmarks reveal the leading performance and the broad applicability of Grounded 3D-LLM. Code and datasets are available at the https://groundedscenellm.github.io/grounded_3d-llm.github.io.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2405.05841.pdf' target='_blank'>https://arxiv.org/pdf/2405.05841.pdf</a></span>   <span><a href='https://github.com/FaltingsA/SSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuan Gao, Yuxin Wang, Yadong Qu, Boqiang Zhang, Zixiao Wang, Jianjun Xu, Hongtao Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05841">Self-Supervised Pre-training with Symmetric Superimposition Modeling for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In text recognition, self-supervised pre-training emerges as a good solution to reduce dependence on expansive annotated real data. Previous studies primarily focus on local visual representation by leveraging mask image modeling or sequence contrastive learning. However, they omit modeling the linguistic information in text images, which is crucial for recognizing text. To simultaneously capture local character features and linguistic information in visual space, we propose Symmetric Superimposition Modeling (SSM). The objective of SSM is to reconstruct the direction-specific pixel and feature signals from the symmetrically superimposed input. Specifically, we add the original image with its inverted views to create the symmetrically superimposed inputs. At the pixel level, we reconstruct the original and inverted images to capture character shapes and texture-level linguistic context. At the feature level, we reconstruct the feature of the same original image and inverted image with different augmentations to model the semantic-level linguistic context and the local character discrimination. In our design, we disrupt the character shape and linguistic rules. Consequently, the dual-level reconstruction facilitates understanding character shapes and linguistic information from the perspective of visual texture and feature semantics. Experiments on various text recognition benchmarks demonstrate the effectiveness and generality of SSM, with 4.1% average performance gains and 86.6% new state-of-the-art average word accuracy on Union14M benchmarks. The code is available at https://github.com/FaltingsA/SSM.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2404.14135.pdf' target='_blank'>https://arxiv.org/pdf/2404.14135.pdf</a></span>   <span><a href='https://github.com/chunchet-ng/Text-in-the-Dark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Che-Tsung Lin, Chun Chet Ng, Zhi Qin Tan, Wan Jun Nah, Xinyu Wang, Jie Long Kew, Pohao Hsu, Shang Hong Lai, Chee Seng Chan, Christopher Zach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14135">Text in the Dark: Extremely Low-Light Text Image Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extremely low-light text images are common in natural scenes, making scene text detection and recognition challenging. One solution is to enhance these images using low-light image enhancement methods before text extraction. However, previous methods often do not try to particularly address the significance of low-level features, which are crucial for optimal performance on downstream scene text tasks. Further research is also hindered by the lack of extremely low-light text datasets. To address these limitations, we propose a novel encoder-decoder framework with an edge-aware attention module to focus on scene text regions during enhancement. Our proposed method uses novel text detection and edge reconstruction losses to emphasize low-level scene text features, leading to successful text extraction. Additionally, we present a Supervised Deep Curve Estimation (Supervised-DCE) model to synthesize extremely low-light images based on publicly available scene text datasets such as ICDAR15 (IC15). We also labeled texts in the extremely low-light See In the Dark (SID) and ordinary LOw-Light (LOL) datasets to allow for objective assessment of extremely low-light image enhancement through scene text tasks. Extensive experiments show that our model outperforms state-of-the-art methods in terms of both image quality and scene text metrics on the widely-used LOL, SID, and synthetic IC15 datasets. Code and dataset will be released publicly at https://github.com/chunchet-ng/Text-in-the-Dark.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2404.10652.pdf' target='_blank'>https://arxiv.org/pdf/2404.10652.pdf</a></span>   <span><a href='https://github.com/minhquan6203/ViTextVQA-Dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Van Nguyen, Dan Quang Tran, Huy Quang Pham, Thang Kien-Bao Nguyen, Nghia Hieu Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10652">ViTextVQA: A Large-Scale Visual Question Answering Dataset for Evaluating Vietnamese Text Comprehension in Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Question Answerinng (VQA) is a complicated task that requires the capability of simultaneously processing natural language and images. This task was initially researched with a focus on developing methods to help machines understand objects and scene contexts in images. However, some scene text that carries explicit information about the full content of the image is not mentioned. Along with the continuous development of the AI era, there have been many studies on the reading comprehension ability of VQA models in the world. Therefore, we introduce the first large-scale dataset in Vietnamese specializing in the ability to understand scene text, we call it ViTextVQA (\textbf{Vi}etnamese \textbf{Text}-based \textbf{V}isual \textbf{Q}uestion \textbf{A}nswering dataset) which contains \textbf{over 16,000} images and \textbf{over 50,000} questions with answers. To tackle this task efficiently, we propose ViTextBLIP-2, an novel multimodal feature fusion Method, which optimizes Vietnamese OCR-based VQA by integrating a frozen Vision Transformer, SwinTextSpotter OCR, and ViT5 LLM with a trainable Q-Former for multimodal feature fusion. Through experiments with various state-of-the-art models, we uncover the significance of the order in which tokens in OCR text are processed and selected to formulate answers. This finding helped us significantly improve the performance of the baseline models on the ViTextVQA dataset. Our dataset is available (https://github.com/minhquan6203/ViTextVQA-Dataset) for research purposes.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2403.13330.pdf' target='_blank'>https://arxiv.org/pdf/2403.13330.pdf</a></span>   <span><a href='https://github.com/SijieLiu518/SGENet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>LeoWu TomyEnrique, Xiangcheng Du, Kangliang Liu, Han Yuan, Zhao Zhou, Cheng Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13330">Efficient scene text image super-resolution with semantic guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text image super-resolution has significantly improved the accuracy of scene text recognition. However, many existing methods emphasize performance over efficiency and ignore the practical need for lightweight solutions in deployment scenarios. Faced with the issues, our work proposes an efficient framework called SGENet to facilitate deployment on resource-limited platforms. SGENet contains two branches: super-resolution branch and semantic guidance branch. We apply a lightweight pre-trained recognizer as a semantic extractor to enhance the understanding of text information. Meanwhile, we design the visual-semantic alignment module to achieve bidirectional alignment between image features and semantics, resulting in the generation of highquality prior guidance. We conduct extensive experiments on benchmark dataset, and the proposed SGENet achieves excellent performance with fewer computational costs. Code is available at https://github.com/SijieLiu518/SGENet
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2403.04473.pdf' target='_blank'>https://arxiv.org/pdf/2403.04473.pdf</a></span>   <span><a href='https://github.com/Yuliang-Liu/Monkey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04473">TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention with zero-initialization, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model's performance. Moreover, by expanding our model's capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. It also learns to perform screenshot tasks through finetuning. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9\% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding. Code will be released at https://github.com/Yuliang-Liu/Monkey.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2403.00303.pdf' target='_blank'>https://arxiv.org/pdf/2403.00303.pdf</a></span>   <span><a href='https://github.com/PriNing/ODM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Duan, Pei Fu, Shan Guo, Qianyi Jiang, Xiaoming Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00303">ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, text-image joint pre-training techniques have shown promising results in various tasks. However, in Optical Character Recognition (OCR) tasks, aligning text instances with their corresponding text regions in images poses a challenge, as it requires effective alignment between text and OCR-Text (referring to the text in images as OCR-Text to distinguish from the text in natural language) rather than a holistic understanding of the overall image content. In this paper, we propose a new pre-training method called OCR-Text Destylization Modeling (ODM) that transfers diverse styles of text found in images to a uniform style based on the text prompt. With ODM, we achieve better alignment between text and OCR-Text and enable pre-trained models to adapt to the complex and diverse styles of scene text detection and spotting tasks. Additionally, we have designed a new labeling generation method specifically for ODM and combined it with our proposed Text-Controller module to address the challenge of annotation costs in OCR tasks, allowing a larger amount of unlabeled data to participate in pre-training. Extensive experiments on multiple public datasets demonstrate that our method significantly improves performance and outperforms current pre-training methods in scene text detection and spotting tasks. Code is available at https://github.com/PriNing/ODM.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2402.13643.pdf' target='_blank'>https://arxiv.org/pdf/2402.13643.pdf</a></span>   <span><a href='https://github.com/MelosY/CAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingkun Yang, Biao Yang, Minghui Liao, Yingying Zhu, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13643">Class-Aware Mask-Guided Feature Refinement for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition is a rapidly developing field that faces numerous challenges due to the complexity and diversity of scene text, including complex backgrounds, diverse fonts, flexible arrangements, and accidental occlusions. In this paper, we propose a novel approach called Class-Aware Mask-guided feature refinement (CAM) to address these challenges. Our approach introduces canonical class-aware glyph masks generated from a standard font to effectively suppress background and text style noise, thereby enhancing feature discrimination. Additionally, we design a feature alignment and fusion module to incorporate the canonical mask guidance for further feature refinement for text recognition. By enhancing the alignment between the canonical mask feature and the text feature, the module ensures more effective fusion, ultimately leading to improved recognition performance. We first evaluate CAM on six standard text recognition benchmarks to demonstrate its effectiveness. Furthermore, CAM exhibits superiority over the state-of-the-art method by an average performance gain of 4.1% across six more challenging datasets, despite utilizing a smaller model size. Our study highlights the importance of incorporating canonical mask guidance and aligned feature refinement techniques for robust scene text recognition. The code is available at https://github.com/MelosY/CAM.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2401.17851.pdf' target='_blank'>https://arxiv.org/pdf/2401.17851.pdf</a></span>   <span><a href='https://github.com/Topdu/OpenOCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongkun Du, Zhineng Chen, Yuchen Su, Caiyan Jia, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.17851">Instruction-Guided Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal models have shown appealing performance in visual recognition tasks, as free-form text-guided training evokes the ability to understand fine-grained visual content. However, current models cannot be trivially applied to scene text recognition (STR) due to the compositional difference between natural and text images. We propose a novel instruction-guided scene text recognition (IGTR) paradigm that formulates STR as an instruction learning problem and understands text images by predicting character attributes, e.g., character frequency, position, etc. IGTR first devises $\left \langle condition,question,answer\right \rangle$ instruction triplets, providing rich and diverse descriptions of character attributes. To effectively learn these attributes through question-answering, IGTR develops a lightweight instruction encoder, a cross-modal feature fusion module and a multi-task answer head, which guides nuanced text image understanding. Furthermore, IGTR realizes different recognition pipelines simply by using different instructions, enabling a character-understanding-based text reasoning paradigm that differs from current methods considerably. Experiments on English and Chinese benchmarks show that IGTR outperforms existing models by significant margins, while maintaining a small model size and fast inference speed. Moreover, by adjusting the sampling of instructions, IGTR offers an elegant way to tackle the recognition of rarely appearing and morphologically similar characters, which were previous challenges. Code: https://github.com/Topdu/OpenOCR.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2401.14832.pdf' target='_blank'>https://arxiv.org/pdf/2401.14832.pdf</a></span>   <span><a href='https://github.com/blackprotoss/GSDM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shipeng Zhu, Pengfei Fang, Chenjie Zhu, Zuoyan Zhao, Qiang Xu, Hui Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14832">Text Image Inpainting via Global Structure-Guided Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world text can be damaged by corrosion issues caused by environmental or human factors, which hinder the preservation of the complete styles of texts, e.g., texture and structure. These corrosion issues, such as graffiti signs and incomplete signatures, bring difficulties in understanding the texts, thereby posing significant challenges to downstream applications, e.g., scene text recognition and signature identification. Notably, current inpainting techniques often fail to adequately address this problem and have difficulties restoring accurate text images along with reasonable and consistent styles. Formulating this as an open problem of text image inpainting, this paper aims to build a benchmark to facilitate its study. In doing so, we establish two specific text inpainting datasets which contain scene text images and handwritten text images, respectively. Each of them includes images revamped by real-life and synthetic datasets, featuring pairs of original images, corrupted images, and other assistant information. On top of the datasets, we further develop a novel neural framework, Global Structure-guided Diffusion Model (GSDM), as a potential solution. Leveraging the global structure of the text as a prior, the proposed GSDM develops an efficient diffusion model to recover clean texts. The efficacy of our approach is demonstrated by thorough empirical study, including a substantial boost in both recognition accuracy and image quality. These findings not only highlight the effectiveness of our method but also underscore its potential to enhance the broader field of text image understanding and processing. Code and datasets are available at: https://github.com/blackprotoss/GSDM.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2401.10110.pdf' target='_blank'>https://arxiv.org/pdf/2401.10110.pdf</a></span>   <span><a href='https://github.com/cxfyxl/VIPTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianfu Cheng, Weixiao Zhou, Xiang Li, Jian Yang, Hang Zhang, Tao Sun, Wei Zhang, Yuying Mai, Tongliang Li, Xiaoming Chen, Zhoujun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10110">SVIPTR: Fast and Efficient Scene Text Recognition with Vision Permutable Extractor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Text Recognition (STR) is an important and challenging upstream task for building structured information databases, that involves recognizing text within images of natural scenes. Although current state-of-the-art (SOTA) models for STR exhibit high performance, they typically suffer from low inference efficiency due to their reliance on hybrid architectures comprised of visual encoders and sequence decoders. In this work, we propose a VIsion Permutable extractor for fast and efficient Scene Text Recognition (SVIPTR), which achieves an impressive balance between high performance and rapid inference speeds in the domain of STR. Specifically, SVIPTR leverages a visual-semantic extractor with a pyramid structure, characterized by the Permutation and combination of local and global self-attention layers. This design results in a lightweight and efficient model and its inference is insensitive to input length. Extensive experimental results on various standard datasets for both Chinese and English scene text recognition validate the superiority of SVIPTR. Notably, the SVIPTR-T (Tiny) variant delivers highly competitive accuracy on par with other lightweight models and achieves SOTA inference speeds. Meanwhile, the SVIPTR-L (Large) attains SOTA accuracy in single-encoder-type models, while maintaining a low parameter count and favorable inference speed. Our proposed method provides a compelling solution for the STR challenge, which greatly benefits real-world applications requiring fast and efficient STR. The code is publicly available at https://github.com/cxfyxl/VIPTR.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2401.07641.pdf' target='_blank'>https://arxiv.org/pdf/2401.07641.pdf</a></span>   <span><a href='https://github.com/mxin262/SwinTextSpotterv2' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/mxin262/SwinTextSpotterv2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxin Huang, Dezhi Peng, Hongliang Li, Zhenghao Peng, Chongyu Liu, Dahua Lin, Yuliang Liu, Xiang Bai, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.07641">SwinTextSpotter v2: Towards Better Synergy for Scene Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end scene text spotting, which aims to read the text in natural images, has garnered significant attention in recent years. However, recent state-of-the-art methods usually incorporate detection and recognition simply by sharing the backbone, which does not directly take advantage of the feature interaction between the two tasks. In this paper, we propose a new end-to-end scene text spotting framework termed SwinTextSpotter v2, which seeks to find a better synergy between text detection and recognition. Specifically, we enhance the relationship between two tasks using novel Recognition Conversion and Recognition Alignment modules. Recognition Conversion explicitly guides text localization through recognition loss, while Recognition Alignment dynamically extracts text features for recognition through the detection predictions. This simple yet effective design results in a concise framework that requires neither an additional rectification module nor character-level annotations for the arbitrarily-shaped text. Furthermore, the parameters of the detector are greatly reduced without performance degradation by introducing a Box Selection Schedule. Qualitative and quantitative experiments demonstrate that SwinTextSpotter v2 achieved state-of-the-art performance on various multilingual (English, Chinese, and Vietnamese) benchmarks. The code will be available at \href{https://github.com/mxin262/SwinTextSpotterv2}{SwinTextSpotter v2}.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2401.00028.pdf' target='_blank'>https://arxiv.org/pdf/2401.00028.pdf</a></span>   <span><a href='https://github.com/large-ocr-model/large-ocr-model.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Miao Rang, Zhenni Bi, Chuanjian Liu, Yunhe Wang, Kai Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00028">An Empirical Study of Scaling Law for OCR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The laws of model size, data volume, computation and model performance have been extensively studied in the field of Natural Language Processing (NLP). However, the scaling laws in Optical Character Recognition (OCR) have not yet been investigated. To address this, we conducted comprehensive studies that involved examining the correlation between performance and the scale of models, data volume and computation in the field of text recognition.Conclusively, the study demonstrates smooth power laws between performance and model size, as well as training data volume, when other influencing factors are held constant. Additionally, we have constructed a large-scale dataset called REBU-Syn, which comprises 6 million real samples and 18 million synthetic samples. Based on our scaling law and new dataset, we have successfully trained a scene text recognition model, achieving a new state-ofthe-art on 6 common test benchmarks with a top-1 average accuracy of 97.42%. The models and dataset are publicly available at https://github.com/large-ocr-model/large-ocr-model.github.io.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2312.12232.pdf' target='_blank'>https://arxiv.org/pdf/2312.12232.pdf</a></span>   <span><a href='https://github.com/ecnuljzhang/brush-your-text' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingjun Zhang, Xinyuan Chen, Yaohui Wang, Yue Lu, Yu Qiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12232">Brush Your Text: Synthesize Any Scene Text on Images via Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, diffusion-based image generation methods are credited for their remarkable text-to-image generation capabilities, while still facing challenges in accurately generating multilingual scene text images. To tackle this problem, we propose Diff-Text, which is a training-free scene text generation framework for any language. Our model outputs a photo-realistic image given a text of any language along with a textual description of a scene. The model leverages rendered sketch images as priors, thus arousing the potential multilingual-generation ability of the pre-trained Stable Diffusion. Based on the observation from the influence of the cross-attention map on object placement in generated images, we propose a localized attention constraint into the cross-attention layer to address the unreasonable positioning problem of scene text. Additionally, we introduce contrastive image-level prompts to further refine the position of the textual region and achieve more accurate scene text generation. Experiments demonstrate that our method outperforms the existing method in both the accuracy of text recognition and the naturalness of foreground-background blending.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2312.11153.pdf' target='_blank'>https://arxiv.org/pdf/2312.11153.pdf</a></span>   <span><a href='https://github.com/wangmelon/CEMLT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11153">Research on Multilingual Natural Scene Text Detection Algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural scene text detection is a significant challenge in computer vision, with tremendous potential applications in multilingual, diverse, and complex text scenarios. We propose a multilingual text detection model to address the issues of low accuracy and high difficulty in detecting multilingual text in natural scenes. In response to the challenges posed by multilingual text images with multiple character sets and various font styles, we introduce the SFM Swin Transformer feature extraction network to enhance the model's robustness in detecting characters and fonts across different languages. Dealing with the considerable variation in text scales and complex arrangements in natural scene text images, we present the AS-HRFPN feature fusion network by incorporating an Adaptive Spatial Feature Fusion module and a Spatial Pyramid Pooling module. The feature fusion network improvements enhance the model's ability to detect text sizes and orientations. Addressing diverse backgrounds and font variations in multilingual scene text images is a challenge for existing methods. Limited local receptive fields hinder detection performance. To overcome this, we propose a Global Semantic Segmentation Branch, extracting and preserving global features for more effective text detection, aligning with the need for comprehensive information. In this study, we collected and built a real-world multilingual natural scene text image dataset and conducted comprehensive experiments and analyses. The experimental results demonstrate that the proposed algorithm achieves an F-measure of 85.02\%, which is 4.71\% higher than the baseline model. We also conducted extensive cross-dataset validation on MSRA-TD500, ICDAR2017MLT, and ICDAR2015 datasets to verify the generality of our approach. The code and dataset can be found at https://github.com/wangmelon/CEMLT.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2312.10806.pdf' target='_blank'>https://arxiv.org/pdf/2312.10806.pdf</a></span>   <span><a href='https://github.com/ku21fan/CLL-STR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeonghun Baek, Yusuke Matsui, Kiyoharu Aizawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10806">Cross-Lingual Learning in Multilingual Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate cross-lingual learning (CLL) for multilingual scene text recognition (STR). CLL transfers knowledge from one language to another. We aim to find the condition that exploits knowledge from high-resource languages for improving performance in low-resource languages. To do so, we first examine if two general insights about CLL discussed in previous works are applied to multilingual STR: (1) Joint learning with high- and low-resource languages may reduce performance on low-resource languages, and (2) CLL works best between typologically similar languages. Through extensive experiments, we show that two general insights may not be applied to multilingual STR. After that, we show that the crucial condition for CLL is the dataset size of high-resource languages regardless of the kind of high-resource languages. Our code, data, and models are available at https://github.com/ku21fan/CLL-STR.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2312.05286.pdf' target='_blank'>https://arxiv.org/pdf/2312.05286.pdf</a></span>   <span><a href='https://github.com/SJTU-DeepVisionLab/FreeReal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongkun Guan, Wei Shen, Xue Yang, Xuehui Wang, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05286">Bridging Synthetic and Real Worlds for Pre-training Scene Text Detectors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing scene text detection methods typically rely on extensive real data for training. Due to the lack of annotated real images, recent works have attempted to exploit large-scale labeled synthetic data (LSD) for pre-training text detectors. However, a synth-to-real domain gap emerges, further limiting the performance of text detectors. Differently, in this work, we propose FreeReal, a real-domain-aligned pre-training paradigm that enables the complementary strengths of both LSD and unlabeled real data (URD). Specifically, to bridge real and synthetic worlds for pre-training, a glyph-based mixing mechanism (GlyphMix) is tailored for text images.GlyphMix delineates the character structures of synthetic images and embeds them as graffiti-like units onto real images. Without introducing real domain drift, GlyphMix freely yields real-world images with annotations derived from synthetic labels. Furthermore, when given free fine-grained synthetic labels, GlyphMix can effectively bridge the linguistic domain gap stemming from English-dominated LSD to URD in various languages. Without bells and whistles, FreeReal achieves average gains of 1.97%, 3.90%, 3.85%, and 4.56% in improving the performance of FCENet, PSENet, PANet, and DBNet methods, respectively, consistently outperforming previous pre-training methods by a substantial margin across four public datasets. Code is available at https://github.com/SJTU-DeepVisionLab/FreeReal.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2312.04884.pdf' target='_blank'>https://arxiv.org/pdf/2312.04884.pdf</a></span>   <span><a href='https://github.com/ZYM-PKU/UDiffText' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Zhao, Zhouhui Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04884">UDiffText: A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-Image (T2I) generation methods based on diffusion model have garnered significant attention in the last few years. Although these image synthesis methods produce visually appealing results, they frequently exhibit spelling errors when rendering text within the generated images. Such errors manifest as missing, incorrect or extraneous characters, thereby severely constraining the performance of text image generation based on diffusion models. To address the aforementioned issue, this paper proposes a novel approach for text image generation, utilizing a pre-trained diffusion model (i.e., Stable Diffusion [27]). Our approach involves the design and training of a light-weight character-level text encoder, which replaces the original CLIP encoder and provides more robust text embeddings as conditional guidance. Then, we fine-tune the diffusion model using a large-scale dataset, incorporating local attention control under the supervision of character-level segmentation maps. Finally, by employing an inference stage refinement process, we achieve a notably high sequence accuracy when synthesizing text in arbitrarily given images. Both qualitative and quantitative results demonstrate the superiority of our method to the state of the art. Furthermore, we showcase several potential applications of the proposed UDiffText, including text-centric image synthesis, scene text editing, etc. Code and model will be available at https://github.com/ZYM-PKU/UDiffText .
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2311.17955.pdf' target='_blank'>https://arxiv.org/pdf/2311.17955.pdf</a></span>   <span><a href='https://github.com/jdfxzzy/PEAN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuoyan Zhao, Hui Xue, Pengfei Fang, Shipeng Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17955">PEAN: A Diffusion-Based Prior-Enhanced Attention Network for Scene Text Image Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text image super-resolution (STISR) aims at simultaneously increasing the resolution and readability of low-resolution scene text images, thus boosting the performance of the downstream recognition task. Two factors in scene text images, visual structure and semantic information, affect the recognition performance significantly. To mitigate the effects from these factors, this paper proposes a Prior-Enhanced Attention Network (PEAN). Specifically, an attention-based modulation module is leveraged to understand scene text images by neatly perceiving the local and global dependence of images, despite the shape of the text. Meanwhile, a diffusion-based module is developed to enhance the text prior, hence offering better guidance for the SR network to generate SR images with higher semantic accuracy. Additionally, a multi-task learning paradigm is employed to optimize the network, enabling the model to generate legible SR images. As a result, PEAN establishes new SOTA results on the TextZoom benchmark. Experiments are also conducted to analyze the importance of the enhanced text prior as a means of improving the performance of the SR network. Code is available at https://github.com/jdfxzzy/PEAN.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2311.17629.pdf' target='_blank'>https://arxiv.org/pdf/2311.17629.pdf</a></span>   <span><a href='https://github.com/wokaikaixinxin/RQFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Zhao, Zeyu Ding, Yong Zhou, Hancheng Zhu, Wenliang Du, Rui Yao, Abdulmotaleb El Saddik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17629">RQFormer: Rotated Query Transformer for End-to-End Oriented Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Oriented object detection presents a challenging task due to the presence of object instances with multiple orientations, varying scales, and dense distributions. Recently, end-to-end detectors have made significant strides by employing attention mechanisms and refining a fixed number of queries through consecutive decoder layers. However, existing end-to-end oriented object detectors still face two primary challenges: 1) misalignment between positional queries and keys, leading to inconsistency between classification and localization; and 2) the presence of a large number of similar queries, which complicates one-to-one label assignments and optimization. To address these limitations, we propose an end-to-end oriented detector called the Rotated Query Transformer, which integrates two key technologies: Rotated RoI Attention (RRoI Attention) and Selective Distinct Queries (SDQ). First, RRoI Attention aligns positional queries and keys from oriented regions of interest through cross-attention. Second, SDQ collects queries from intermediate decoder layers and filters out similar ones to generate distinct queries, thereby facilitating the optimization of one-to-one label assignments. Finally, extensive experiments conducted on four remote sensing datasets and one scene text dataset demonstrate the effectiveness of our method. To further validate its generalization capability, we also extend our approach to horizontal object detection The code is available at \url{https://github.com/wokaikaixinxin/RQFormer}.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2311.13120.pdf' target='_blank'>https://arxiv.org/pdf/2311.13120.pdf</a></span>   <span><a href='https://github.com/bytedance/E2STR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Zhao, Jingqun Tang, Chunhui Lin, Binghong Wu, Can Huang, Hao Liu, Xin Tan, Zhizhong Zhang, Yuan Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13120">Multi-modal In-Context Learning Makes an Ego-evolving Scene Text Recognizer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition (STR) in the wild frequently encounters challenges when coping with domain variations, font diversity, shape deformations, etc. A straightforward solution is performing model fine-tuning tailored to a specific scenario, but it is computationally intensive and requires multiple model copies for various scenarios. Recent studies indicate that large language models (LLMs) can learn from a few demonstration examples in a training-free manner, termed "In-Context Learning" (ICL). Nevertheless, applying LLMs as a text recognizer is unacceptably resource-consuming. Moreover, our pilot experiments on LLMs show that ICL fails in STR, mainly attributed to the insufficient incorporation of contextual information from diverse samples in the training stage. To this end, we introduce E$^2$STR, a STR model trained with context-rich scene text sequences, where the sequences are generated via our proposed in-context training strategy. E$^2$STR demonstrates that a regular-sized model is sufficient to achieve effective ICL capabilities in STR. Extensive experiments show that E$^2$STR exhibits remarkable training-free adaptation in various scenarios and outperforms even the fine-tuned state-of-the-art approaches on public benchmarks. The code is released at https://github.com/bytedance/E2STR .
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2310.16809.pdf' target='_blank'>https://arxiv.org/pdf/2310.16809.pdf</a></span>   <span><a href='https://github.com/SCUT-DLVCLab/GPT-4V_OCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongxin Shi, Dezhi Peng, Wenhui Liao, Zening Lin, Xinhong Chen, Chongyu Liu, Yuyi Zhang, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16809">Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a comprehensive evaluation of the Optical Character Recognition (OCR) capabilities of the recently released GPT-4V(ision), a Large Multimodal Model (LMM). We assess the model's performance across a range of OCR tasks, including scene text recognition, handwritten text recognition, handwritten mathematical expression recognition, table structure recognition, and information extraction from visually-rich document. The evaluation reveals that GPT-4V performs well in recognizing and understanding Latin contents, but struggles with multilingual scenarios and complex tasks. Specifically, it showed limitations when dealing with non-Latin languages and complex tasks such as handwriting mathematical expression recognition, table structure recognition, and end-to-end semantic entity recognition and pair extraction from document image. Based on these observations, we affirm the necessity and continued research value of specialized OCR models. In general, despite its versatility in handling diverse OCR tasks, GPT-4V does not outperform existing state-of-the-art OCR models. How to fully utilize pre-trained general-purpose LMMs such as GPT-4V for OCR downstream tasks remains an open problem. The study offers a critical reference for future research in OCR with LMMs. Evaluation pipeline and results are available at https://github.com/SCUT-DLVCLab/GPT-4V_OCR.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2310.04999.pdf' target='_blank'>https://arxiv.org/pdf/2310.04999.pdf</a></span>   <span><a href='https://github.com/wzx99/CLIPOCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixiao Wang, Hongtao Xie, Yuxin Wang, Jianjun Xu, Boqiang Zhang, Yongdong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04999">Symmetrical Linguistic Feature Distillation with CLIP for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we explore the potential of the Contrastive Language-Image Pretraining (CLIP) model in scene text recognition (STR), and establish a novel Symmetrical Linguistic Feature Distillation framework (named CLIP-OCR) to leverage both visual and linguistic knowledge in CLIP. Different from previous CLIP-based methods mainly considering feature generalization on visual encoding, we propose a symmetrical distillation strategy (SDS) that further captures the linguistic knowledge in the CLIP text encoder. By cascading the CLIP image encoder with the reversed CLIP text encoder, a symmetrical structure is built with an image-to-text feature flow that covers not only visual but also linguistic information for distillation.Benefiting from the natural alignment in CLIP, such guidance flow provides a progressive optimization objective from vision to language, which can supervise the STR feature forwarding process layer-by-layer.Besides, a new Linguistic Consistency Loss (LCL) is proposed to enhance the linguistic capability by considering second-order statistics during the optimization. Overall, CLIP-OCR is the first to design a smooth transition between image and text for the STR task.Extensive experiments demonstrate the effectiveness of CLIP-OCR with 93.8% average accuracy on six popular STR benchmarks.Code will be available at https://github.com/wzx99/CLIPOCR.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2309.12382.pdf' target='_blank'>https://arxiv.org/pdf/2309.12382.pdf</a></span>   <span><a href='https://github.com/naver-ai/scob' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daehee Kim, Yoonsik Kim, DongHyun Kim, Yumin Lim, Geewook Kim, Taeho Kil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12382">SCOB: Universal Text Understanding via Character-wise Supervised Contrastive Learning with Online Text Rendering for Bridging Domain Gap</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the great success of language model (LM)-based pre-training, recent studies in visual document understanding have explored LM-based pre-training methods for modeling text within document images. Among them, pre-training that reads all text from an image has shown promise, but often exhibits instability and even fails when applied to broader domains, such as those involving both visual documents and scene text images. This is a substantial limitation for real-world scenarios, where the processing of text image inputs in diverse domains is essential. In this paper, we investigate effective pre-training tasks in the broader domains and also propose a novel pre-training method called SCOB that leverages character-wise supervised contrastive learning with online text rendering to effectively pre-train document and scene text domains by bridging the domain gap. Moreover, SCOB enables weakly supervised learning, significantly reducing annotation costs. Extensive benchmarks demonstrate that SCOB generally improves vanilla pre-training methods and achieves comparable performance to state-of-the-art methods. Our findings suggest that SCOB can be served generally and effectively for read-type pre-training methods. The code will be available at https://github.com/naver-ai/scob.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2309.08919.pdf' target='_blank'>https://arxiv.org/pdf/2309.08919.pdf</a></span>   <span><a href='https://github.com/wenyu1009/RTSRN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenyu Zhang, Xin Deng, Baojun Jia, Xingtong Yu, Yifan Chen, jin Ma, Qing Ding, Xinming Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.08919">Pixel Adapter: A Graph-Based Post-Processing Approach for Scene Text Image Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current Scene text image super-resolution approaches primarily focus on extracting robust features, acquiring text information, and complex training strategies to generate super-resolution images. However, the upsampling module, which is crucial in the process of converting low-resolution images to high-resolution ones, has received little attention in existing works. To address this issue, we propose the Pixel Adapter Module (PAM) based on graph attention to address pixel distortion caused by upsampling. The PAM effectively captures local structural information by allowing each pixel to interact with its neighbors and update features. Unlike previous graph attention mechanisms, our approach achieves 2-3 orders of magnitude improvement in efficiency and memory utilization by eliminating the dependency on sparse adjacency matrices and introducing a sliding window approach for efficient parallel computation. Additionally, we introduce the MLP-based Sequential Residual Block (MSRB) for robust feature extraction from text images, and a Local Contour Awareness loss ($\mathcal{L}_{lca}$) to enhance the model's perception of details. Comprehensive experiments on TextZoom demonstrate that our proposed method generates high-quality super-resolution images, surpassing existing methods in recognition accuracy. For single-stage and multi-stage strategies, we achieved improvements of 0.7\% and 2.6\%, respectively, increasing the performance from 52.6\% and 53.7\% to 53.3\% and 56.3\%. The code is available at https://github.com/wenyu1009/RTSRN.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2309.08042.pdf' target='_blank'>https://arxiv.org/pdf/2309.08042.pdf</a></span>   <span><a href='https://github.com/ya0-sun/STR-Berlin' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Sun, Anna Kruspe, Liqiu Meng, Yifan Tian, Eike J Hoffmann, Stefan Auer, Xiao Xiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.08042">Towards Large-scale Building Attribute Mapping using Crowdsourced Images: Scene Text Recognition on Flickr and Problems to be Solved</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crowdsourced platforms provide huge amounts of street-view images that contain valuable building information. This work addresses the challenges in applying Scene Text Recognition (STR) in crowdsourced street-view images for building attribute mapping. We use Flickr images, particularly examining texts on building facades. A Berlin Flickr dataset is created, and pre-trained STR models are used for text detection and recognition. Manual checking on a subset of STR-recognized images demonstrates high accuracy. We examined the correlation between STR results and building functions, and analysed instances where texts were recognized on residential buildings but not on commercial ones. Further investigation revealed significant challenges associated with this task, including small text regions in street-view images, the absence of ground truth labels, and mismatches in buildings in Flickr images and building footprints in OpenStreetMap (OSM). To develop city-wide mapping beyond urban hotspot locations, we suggest differentiating the scenarios where STR proves effective while developing appropriate algorithms or bringing in additional data for handling other cases. Furthermore, interdisciplinary collaboration should be undertaken to understand the motivation behind building photography and labeling. The STR-on-Flickr results are publicly available at https://github.com/ya0-sun/STR-Berlin.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2309.01083.pdf' target='_blank'>https://arxiv.org/pdf/2309.01083.pdf</a></span>   <span><a href='https://github.com/FudanVI/FudanOCR/tree/main/image-ids-CTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyang Yu, Xiaocong Wang, Bin Li, Xiangyang Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01083">Chinese Text Recognition with A Pre-Trained CLIP-Like Model Through Image-IDS Aligning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition has been studied for decades due to its broad applications. However, despite Chinese characters possessing different characteristics from Latin characters, such as complex inner structures and large categories, few methods have been proposed for Chinese Text Recognition (CTR). Particularly, the characteristic of large categories poses challenges in dealing with zero-shot and few-shot Chinese characters. In this paper, inspired by the way humans recognize Chinese texts, we propose a two-stage framework for CTR. Firstly, we pre-train a CLIP-like model through aligning printed character images and Ideographic Description Sequences (IDS). This pre-training stage simulates humans recognizing Chinese characters and obtains the canonical representation of each character. Subsequently, the learned representations are employed to supervise the CTR model, such that traditional single-character recognition can be improved to text-line recognition through image-IDS matching. To evaluate the effectiveness of the proposed method, we conduct extensive experiments on both Chinese character recognition (CCR) and CTR. The experimental results demonstrate that the proposed method performs best in CCR and outperforms previous methods in most scenarios of the CTR benchmark. It is worth noting that the proposed method can recognize zero-shot Chinese characters in text images without fine-tuning, whereas previous methods require fine-tuning when new classes appear. The code is available at https://github.com/FudanVI/FudanOCR/tree/main/image-ids-CTR.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2309.00410.pdf' target='_blank'>https://arxiv.org/pdf/2309.00410.pdf</a></span>   <span><a href='https://github.com/mitanihayato/Selective-Scene-Text-Removal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hayato Mitani, Akisato Kimura, Seiichi Uchida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00410">Selective Scene Text Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text removal (STR) is the image transformation task to remove text regions in scene images. The conventional STR methods remove all scene text. This means that the existing methods cannot select text to be removed. In this paper, we propose a novel task setting named selective scene text removal (SSTR) that removes only target words specified by the user. Although SSTR is a more complex task than STR, the proposed multi-module structure enables efficient training for SSTR. Experimental results show that the proposed method can remove target words as expected.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2308.10408.pdf' target='_blank'>https://arxiv.org/pdf/2308.10408.pdf</a></span>   <span><a href='https://github.com/wenwenyu/TCM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenwen Yu, Yuliang Liu, Xingkui Zhu, Haoyu Cao, Xing Sun, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10408">Turning a CLIP Model into a Scene Text Spotter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We exploit the potential of the large-scale Contrastive Language-Image Pretraining (CLIP) model to enhance scene text detection and spotting tasks, transforming it into a robust backbone, FastTCM-CR50. This backbone utilizes visual prompt learning and cross-attention in CLIP to extract image and text-based prior knowledge. Using predefined and learnable prompts, FastTCM-CR50 introduces an instance-language matching process to enhance the synergy between image and text embeddings, thereby refining text regions. Our Bimodal Similarity Matching (BSM) module facilitates dynamic language prompt generation, enabling offline computations and improving performance. FastTCM-CR50 offers several advantages: 1) It can enhance existing text detectors and spotters, improving performance by an average of 1.7% and 1.5%, respectively. 2) It outperforms the previous TCM-CR50 backbone, yielding an average improvement of 0.2% and 0.56% in text detection and spotting tasks, along with a 48.5% increase in inference speed. 3) It showcases robust few-shot training capabilities. Utilizing only 10% of the supervised data, FastTCM-CR50 improves performance by an average of 26.5% and 5.5% for text detection and spotting tasks, respectively. 4) It consistently enhances performance on out-of-distribution text detection and spotting datasets, particularly the NightTime-ArT subset from ICDAR2019-ArT and the DOTA dataset for oriented object detection. The code is available at https://github.com/wenwenyu/TCM.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2308.10147.pdf' target='_blank'>https://arxiv.org/pdf/2308.10147.pdf</a></span>   <span><a href='https://github.com/mxin262/ESTextSpotter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxin Huang, Jiaxin Zhang, Dezhi Peng, Hao Lu, Can Huang, Yuliang Liu, Xiang Bai, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10147">ESTextSpotter: Towards Better Scene Text Spotting with Explicit Synergy in Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, end-to-end scene text spotting approaches are evolving to the Transformer-based framework. While previous studies have shown the crucial importance of the intrinsic synergy between text detection and recognition, recent advances in Transformer-based methods usually adopt an implicit synergy strategy with shared query, which can not fully realize the potential of these two interactive tasks. In this paper, we argue that the explicit synergy considering distinct characteristics of text detection and recognition can significantly improve the performance text spotting. To this end, we introduce a new model named Explicit Synergy-based Text Spotting Transformer framework (ESTextSpotter), which achieves explicit synergy by modeling discriminative and interactive features for text detection and recognition within a single decoder. Specifically, we decompose the conventional shared query into task-aware queries for text polygon and content, respectively. Through the decoder with the proposed vision-language communication module, the queries interact with each other in an explicit manner while preserving discriminative patterns of text detection and recognition, thus improving performance significantly. Additionally, we propose a task-aware query initialization scheme to ensure stable training. Experimental results demonstrate that our model significantly outperforms previous state-of-the-art methods. Code is available at https://github.com/mxin262/ESTextSpotter.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2308.08769.pdf' target='_blank'>https://arxiv.org/pdf/2308.08769.pdf</a></span>   <span><a href='https://chat-3d.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehan Wang, Haifeng Huang, Yang Zhao, Ziang Zhang, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.08769">Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D scene understanding has gained significant attention due to its wide range of applications. However, existing methods for 3D scene understanding are limited to specific downstream tasks, which hinders their practicality in real-world applications. This paper presents Chat-3D, which combines the 3D visual perceptual ability of pre-trained 3D representations and the impressive reasoning and conversation capabilities of advanced LLMs to achieve the first universal dialogue systems for 3D scenes. Specifically, we align 3D representations into the feature space of LLMs, thus enabling LLMs to perceive the 3D world. Given the scarcity of 3D scene-text data, we propose a three-stage training strategy to efficiently utilize the available data for better alignment. To enhance the reasoning ability and develop a user-friendly interaction scheme, we further construct a high-quality object-centric 3D instruction dataset and design an associated object-centric prompt. Our experiments show that Chat-3D achieves an impressive ability to comprehend diverse instructions for 3D scenes, engage in intricate spatial reasoning, and incorporate external knowledge into its responses. Chat-3D achieves a 75.6% relative score compared with GPT-4 on the constructed instruction dataset.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2308.06743.pdf' target='_blank'>https://arxiv.org/pdf/2308.06743.pdf</a></span>   <span><a href='https://github.com/Lenubolim/TextDiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Baolin Liu, Zongyuan Yang, Pengfei Wang, Junjie Zhou, Ziqi Liu, Ziyi Song, Yan Liu, Yongping Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06743">TextDiff: Mask-Guided Residual Diffusion Models for Scene Text Image Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of scene text image super-resolution is to reconstruct high-resolution text-line images from unrecognizable low-resolution inputs. The existing methods relying on the optimization of pixel-level loss tend to yield text edges that exhibit a notable degree of blurring, thereby exerting a substantial impact on both the readability and recognizability of the text. To address these issues, we propose TextDiff, the first diffusion-based framework tailored for scene text image super-resolution. It contains two modules: the Text Enhancement Module (TEM) and the Mask-Guided Residual Diffusion Module (MRD). The TEM generates an initial deblurred text image and a mask that encodes the spatial location of the text. The MRD is responsible for effectively sharpening the text edge by modeling the residuals between the ground-truth images and the initial deblurred images. Extensive experiments demonstrate that our TextDiff achieves state-of-the-art (SOTA) performance on public benchmark datasets and can improve the readability of scene text images. Moreover, our proposed MRD module is plug-and-play that effectively sharpens the text edges produced by SOTA methods. This enhancement not only improves the readability and recognizability of the results generated by SOTA methods but also does not require any additional joint training. Available Codes:https://github.com/Lenubolim/TextDiff.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2308.03262.pdf' target='_blank'>https://arxiv.org/pdf/2308.03262.pdf</a></span>   <span><a href='https://github.com/mjq11302010044/Real-CE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianqi Ma, Zhetong Liang, Wangmeng Xiang, Xi Yang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03262">A Benchmark for Chinese-English Scene Text Image Super-resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Text Image Super-resolution (STISR) aims to recover high-resolution (HR) scene text images with visually pleasant and readable text content from the given low-resolution (LR) input. Most existing works focus on recovering English texts, which have relatively simple character structures, while little work has been done on the more challenging Chinese texts with diverse and complex character structures. In this paper, we propose a real-world Chinese-English benchmark dataset, namely Real-CE, for the task of STISR with the emphasis on restoring structurally complex Chinese characters. The benchmark provides 1,935/783 real-world LR-HR text image pairs~(contains 33,789 text lines in total) for training/testing in 2$\times$ and 4$\times$ zooming modes, complemented by detailed annotations, including detection boxes and text transcripts. Moreover, we design an edge-aware learning method, which provides structural supervision in image and feature domains, to effectively reconstruct the dense structures of Chinese characters. We conduct experiments on the proposed Real-CE benchmark and evaluate the existing STISR models with and without our edge-aware loss. The benchmark, including data and source code, is available at https://github.com/mjq11302010044/Real-CE.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2308.03024.pdf' target='_blank'>https://arxiv.org/pdf/2308.03024.pdf</a></span>   <span><a href='https://vl2g.github.io/projects/visTrans/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shreyas Vaidya, Arvind Kumar Sharma, Prajwal Gatti, Anand Mishra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03024">Show Me the World in My Language: Establishing the First Baseline for Scene-Text to Scene-Text Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we study the task of ``visually'' translating scene text from a source language (e.g., Hindi) to a target language (e.g., English). Visual translation involves not just the recognition and translation of scene text but also the generation of the translated image that preserves visual features of the source scene text, such as font, size, and background. There are several challenges associated with this task, such as translation with limited context, deciding between translation and transliteration, accommodating varying text lengths within fixed spatial boundaries, and preserving the font and background styles of the source scene text in the target language. To address this problem, we make the following contributions: (i) We study visual translation as a standalone problem for the first time in the literature. (ii) We present a cascaded framework for visual translation that combines state-of-the-art modules for scene text recognition, machine translation, and scene text synthesis as a baseline for the task. (iii) We propose a set of task-specific design enhancements to design a variant of the baseline to obtain performance improvements. (iv) Currently, the existing related literature lacks any comprehensive performance evaluation for this novel task. To fill this gap, we introduce several automatic and user-assisted evaluation metrics designed explicitly for evaluating visual translation. Further, we evaluate presented baselines for translating scene text between Hindi and English. Our experiments demonstrate that although we can effectively perform visual translation over a large collection of scene text images, the presented baseline only partially addresses challenges posed by visual translation tasks. We firmly believe that this new task and the limitations of existing models, as reported in this paper, should encourage further research in visual translation.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2308.00508.pdf' target='_blank'>https://arxiv.org/pdf/2308.00508.pdf</a></span>   <span><a href='https://github.com/ThunderVVV/RCLSTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinglei Zhang, Tiancheng Lin, Yi Xu, Kai Chen, Rui Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00508">Relational Contrastive Learning for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Context-aware methods achieved great success in supervised scene text recognition via incorporating semantic priors from words. We argue that such prior contextual information can be interpreted as the relations of textual primitives due to the heterogeneous text and background, which can provide effective self-supervised labels for representation learning. However, textual relations are restricted to the finite size of dataset due to lexical dependencies, which causes the problem of over-fitting and compromises representation robustness. To this end, we propose to enrich the textual relations via rearrangement, hierarchy and interaction, and design a unified framework called RCLSTR: Relational Contrastive Learning for Scene Text Recognition. Based on causality, we theoretically explain that three modules suppress the bias caused by the contextual prior and thus guarantee representation robustness. Experiments on representation quality show that our method outperforms state-of-the-art self-supervised STR methods. Code is available at https://github.com/ThunderVVV/RCLSTR.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2307.13244.pdf' target='_blank'>https://arxiv.org/pdf/2307.13244.pdf</a></span>   <span><a href='https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Da, Peng Wang, Cong Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13244">Multi-Granularity Prediction with Learnable Fusion for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the enormous technical challenges and wide range of applications, scene text recognition (STR) has been an active research topic in computer vision for years. To tackle this tough problem, numerous innovative methods have been successively proposed, and incorporating linguistic knowledge into STR models has recently become a prominent trend. In this work, we first draw inspiration from the recent progress in Vision Transformer (ViT) to construct a conceptually simple yet functionally powerful vision STR model, which is built upon ViT and a tailored Adaptive Addressing and Aggregation (A$^3$) module. It already outperforms most previous state-of-the-art models for scene text recognition, including both pure vision models and language-augmented methods. To integrate linguistic knowledge, we further propose a Multi-Granularity Prediction strategy to inject information from the language modality into the model in an implicit way, \ie, subword representations (BPE and WordPiece) widely used in NLP are introduced into the output space, in addition to the conventional character level representation, while no independent language model (LM) is adopted. To produce the final recognition results, two strategies for effectively fusing the multi-granularity predictions are devised. The resultant algorithm (termed MGP-STR) is able to push the performance envelope of STR to an even higher level. Specifically, MGP-STR achieves an average recognition accuracy of $94\%$ on standard benchmarks for scene text recognition. Moreover, it also achieves state-of-the-art results on widely-used handwritten benchmarks as well as more challenging scene text datasets, demonstrating the generality of the proposed MGP-STR algorithm. The source code and models will be available at: \url{https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR}.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2307.12270.pdf' target='_blank'>https://arxiv.org/pdf/2307.12270.pdf</a></span>   <span><a href='https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_en/algorithm_rec_cppd_en.md' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_en/algorithm_rec_cppd_en.md' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongkun Du, Zhineng Chen, Caiyan Jia, Xiaoting Yin, Chenxia Li, Yuning Du, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12270">Context Perception Parallel Decoder for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition (STR) methods have struggled to attain high accuracy and fast inference speed. Autoregressive (AR)-based models implement the recognition in a character-by-character manner, showing superiority in accuracy but with slow inference speed. Alternatively, parallel decoding (PD)-based models infer all characters in a single decoding pass, offering faster inference speed but generally worse accuracy. We first present an empirical study of AR decoding in STR, and discover that the AR decoder not only models linguistic context, but also provides guidance on visual context perception. Consequently, we propose Context Perception Parallel Decoder (CPPD) to predict the character sequence in a PD pass. CPPD devises a character counting module to infer the occurrence count of each character, and a character ordering module to deduce the content-free reading order and placeholders. Meanwhile, the character prediction task associates the placeholders with characters. They together build a comprehensive recognition context. We construct a series of CPPD models and also plug the proposed modules into existing STR decoders. Experiments on both English and Chinese benchmarks demonstrate that the CPPD models achieve highly competitive accuracy while running approximately 8x faster than their AR-based counterparts. Moreover, the plugged models achieve significant accuracy improvements. Code is at \href{https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_en/algorithm_rec_cppd_en.md}{this https URL}.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2307.09749.pdf' target='_blank'>https://arxiv.org/pdf/2307.09749.pdf</a></span>   <span><a href='https://github.com/csguoh/LEMMA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Guo, Tao Dai, Guanghao Meng, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09749">Towards Robust Scene Text Image Super-resolution via Explicit Location Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text image super-resolution (STISR), aiming to improve image quality while boosting downstream scene text recognition accuracy, has recently achieved great success. However, most existing methods treat the foreground (character regions) and background (non-character regions) equally in the forward process, and neglect the disturbance from the complex background, thus limiting the performance. To address these issues, in this paper, we propose a novel method LEMMA that explicitly models character regions to produce high-level text-specific guidance for super-resolution. To model the location of characters effectively, we propose the location enhancement module to extract character region features based on the attention map sequence. Besides, we propose the multi-modal alignment module to perform bidirectional visual-semantic alignment to generate high-quality prior guidance, which is then incorporated into the super-resolution branch in an adaptive manner using the proposed adaptive fusion module. Experiments on TextZoom and four scene text recognition benchmarks demonstrate the superiority of our method over other state-of-the-art methods. Code is available at https://github.com/csguoh/LEMMA.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2306.15142.pdf' target='_blank'>https://arxiv.org/pdf/2306.15142.pdf</a></span>   <span><a href='https://github.com/ychensu/LRANet.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Su, Zhineng Chen, Zhiwen Shao, Yuning Du, Zhilong Ji, Jinfeng Bai, Yong Zhou, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15142">LRANet: Towards Accurate and Efficient Scene Text Detection with Low-Rank Approximation Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, regression-based methods, which predict parameterized text shapes for text localization, have gained popularity in scene text detection. However, the existing parameterized text shape methods still have limitations in modeling arbitrary-shaped texts due to ignoring the utilization of text-specific shape information. Moreover, the time consumption of the entire pipeline has been largely overlooked, leading to a suboptimal overall inference speed. To address these issues, we first propose a novel parameterized text shape method based on low-rank approximation. Unlike other shape representation methods that employ data-irrelevant parameterization, our approach utilizes singular value decomposition and reconstructs the text shape using a few eigenvectors learned from labeled text contours. By exploring the shape correlation among different text contours, our method achieves consistency, compactness, simplicity, and robustness in shape representation. Next, we propose a dual assignment scheme for speed acceleration. It adopts a sparse assignment branch to accelerate the inference speed, and meanwhile, provides ample supervised signals for training through a dense assignment branch. Building upon these designs, we implement an accurate and efficient arbitrary-shaped text detector named LRANet. Extensive experiments are conducted on several challenging benchmarks, demonstrating the superior accuracy and efficiency of LRANet compared to state-of-the-art methods. Code is available at: \url{https://github.com/ychensu/LRANet.git}
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2306.12106.pdf' target='_blank'>https://arxiv.org/pdf/2306.12106.pdf</a></span>   <span><a href='https://github.com/shannanyinxiang/ViTEraser' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dezhi Peng, Chongyu Liu, Yuliang Liu, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.12106">ViTEraser: Harnessing the Power of Vision Transformers for Scene Text Removal with SegMIM Pretraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text removal (STR) aims at replacing text strokes in natural scenes with visually coherent backgrounds. Recent STR approaches rely on iterative refinements or explicit text masks, resulting in high complexity and sensitivity to the accuracy of text localization. Moreover, most existing STR methods adopt convolutional architectures while the potential of vision Transformers (ViTs) remains largely unexplored. In this paper, we propose a simple-yet-effective ViT-based text eraser, dubbed ViTEraser. Following a concise encoder-decoder framework, ViTEraser can easily incorporate various ViTs to enhance long-range modeling. Specifically, the encoder hierarchically maps the input image into the hidden space through ViT blocks and patch embedding layers, while the decoder gradually upsamples the hidden features to the text-erased image with ViT blocks and patch splitting layers. As ViTEraser implicitly integrates text localization and inpainting, we propose a novel end-to-end pretraining method, termed SegMIM, which focuses the encoder and decoder on the text box segmentation and masked image modeling tasks, respectively. Experimental results demonstrate that ViTEraser with SegMIM achieves state-of-the-art performance on STR by a substantial margin and exhibits strong generalization ability when extended to other tasks, \textit{e.g.}, tampered scene text detection. Furthermore, we comprehensively explore the architecture, pretraining, and scalability of the ViT-based encoder-decoder for STR, which provides deep insights into the application of ViT to the STR field. Code is available at https://github.com/shannanyinxiang/ViTEraser.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2306.09593.pdf' target='_blank'>https://arxiv.org/pdf/2306.09593.pdf</a></span>   <span><a href='https://github.com/GuangtaoLyu/FETNet' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/GuangtaoLyu/FETNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangtao Lyu, Kun Liu, Anna Zhu, Seiichi Uchida, Brian Kenji Iwana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09593">FETNet: Feature Erasing and Transferring Network for Scene Text Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The scene text removal (STR) task aims to remove text regions and recover the background smoothly in images for private information protection. Most existing STR methods adopt encoder-decoder-based CNNs, with direct copies of the features in the skip connections. However, the encoded features contain both text texture and structure information. The insufficient utilization of text features hampers the performance of background reconstruction in text removal regions. To tackle these problems, we propose a novel Feature Erasing and Transferring (FET) mechanism to reconfigure the encoded features for STR in this paper. In FET, a Feature Erasing Module (FEM) is designed to erase text features. An attention module is responsible for generating the feature similarity guidance. The Feature Transferring Module (FTM) is introduced to transfer the corresponding features in different layers based on the attention guidance. With this mechanism, a one-stage, end-to-end trainable network called FETNet is constructed for scene text removal. In addition, to facilitate research on both scene text removal and segmentation tasks, we introduce a novel dataset, Flickr-ST, with multi-category annotations. A sufficient number of experiments and ablation studies are conducted on the public datasets and Flickr-ST. Our proposed method achieves state-of-the-art performance using most metrics, with remarkably higher quality scene text removal results. The source code of our work is available at: \href{https://github.com/GuangtaoLyu/FETNet}{https://github.com/GuangtaoLyu/FETNet.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2306.07842.pdf' target='_blank'>https://arxiv.org/pdf/2306.07842.pdf</a></span>   <span><a href='https://github.com/GuangtaoLyu/PSSTRNet' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/GuangtaoLyu/PSSTRNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangtao Lyu, Anna Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07842">PSSTRNet: Progressive Segmentation-guided Scene Text Removal Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text removal (STR) is a challenging task due to the complex text fonts, colors, sizes, and background textures in scene images. However, most previous methods learn both text location and background inpainting implicitly within a single network, which weakens the text localization mechanism and makes a lossy background. To tackle these problems, we propose a simple Progressive Segmentation-guided Scene Text Removal Network(PSSTRNet) to remove the text in the image iteratively. It contains two decoder branches, a text segmentation branch, and a text removal branch, with a shared encoder. The text segmentation branch generates text mask maps as the guidance for the regional removal branch. In each iteration, the original image, previous text removal result, and text mask are input to the network to extract the rest part of the text segments and cleaner text removal result. To get a more accurate text mask map, an update module is developed to merge the mask map in the current and previous stages. The final text removal result is obtained by adaptive fusion of results from all previous stages. A sufficient number of experiments and ablation studies conducted on the real and synthetic public datasets demonstrate our proposed method achieves state-of-the-art performance. The source code of our work is available at: \href{https://github.com/GuangtaoLyu/PSSTRNet}{https://github.com/GuangtaoLyu/PSSTRNet.}
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2306.03482.pdf' target='_blank'>https://arxiv.org/pdf/2306.03482.pdf</a></span>   <span><a href='https://github.com/wenwenyu/AudioOCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenwen Yu, Mingyu Liu, Biao Yang, Enming Zhang, Deqiang Jiang, Xing Sun, Yuliang Liu, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03482">Looking and Listening: Audio Guided Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text recognition in the wild is a long-standing problem in computer vision. Driven by end-to-end deep learning, recent studies suggest vision and language processing are effective for scene text recognition. Yet, solving edit errors such as add, delete, or replace is still the main challenge for existing approaches. In fact, the content of the text and its audio are naturally corresponding to each other, i.e., a single character error may result in a clear different pronunciation. In this paper, we propose the AudioOCR, a simple yet effective probabilistic audio decoder for mel spectrogram sequence prediction to guide the scene text recognition, which only participates in the training phase and brings no extra cost during the inference stage. The underlying principle of AudioOCR can be easily applied to the existing approaches. Experiments using 7 previous scene text recognition methods on 12 existing regular, irregular, and occluded benchmarks demonstrate our proposed method can bring consistent improvement. More importantly, through our experimentation, we show that AudioOCR possesses a generalizability that extends to more challenging scenarios, including recognizing non-English text, out-of-vocabulary words, and text with various accents. Code will be available at https://github.com/wenwenyu/AudioOCR.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2305.19957.pdf' target='_blank'>https://arxiv.org/pdf/2305.19957.pdf</a></span>   <span><a href='https://github.com/ViTAE-Transformer/DeepSolo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maoyuan Ye, Jing Zhang, Shanshan Zhao, Juhua Liu, Tongliang Liu, Bo Du, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19957">DeepSolo++: Let Transformer Decoder with Explicit Points Solo for Multilingual Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end text spotting aims to integrate scene text detection and recognition into a unified framework. Dealing with the relationship between the two sub-tasks plays a pivotal role in designing effective spotters. Although Transformer-based methods eliminate the heuristic post-processing, they still suffer from the synergy issue between the sub-tasks and low training efficiency. Besides, they overlook the exploring on multilingual text spotting which requires an extra script identification task. In this paper, we present DeepSolo++, a simple DETR-like baseline that lets a single decoder with explicit points solo for text detection, recognition, and script identification simultaneously. Technically, for each text instance, we represent the character sequence as ordered points and model them with learnable explicit point queries. After passing a single decoder, the point queries have encoded requisite text semantics and locations, thus can be further decoded to the center line, boundary, script, and confidence of text via very simple prediction heads in parallel. Furthermore, we show the surprisingly good extensibility of our method, in terms of character class, language type, and task. On the one hand, our method not only performs well in English scenes but also masters the transcription with complex font structure and a thousand-level character classes, such as Chinese. On the other hand, our DeepSolo++ achieves better performance on the additionally introduced script identification task with a simpler training pipeline compared with previous methods. In addition, our models are also compatible with line annotations, which require much less annotation cost than polygons. The code is available at \url{https://github.com/ViTAE-Transformer/DeepSolo}.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2305.19498.pdf' target='_blank'>https://arxiv.org/pdf/2305.19498.pdf</a></span>   <span><a href='https://github.com/husterpzh/PSSR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenghua Peng, Yu Luo, Tianshui Chen, Keke Xu, Shuangping Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19498">Perception and Semantic Aware Regularization for Sequential Confidence Calibration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep sequence recognition (DSR) models receive increasing attention due to their superior application to various applications. Most DSR models use merely the target sequences as supervision without considering other related sequences, leading to over-confidence in their predictions. The DSR models trained with label smoothing regularize labels by equally and independently smoothing each token, reallocating a small value to other tokens for mitigating overconfidence. However, they do not consider tokens/sequences correlations that may provide more effective information to regularize training and thus lead to sub-optimal performance. In this work, we find tokens/sequences with high perception and semantic correlations with the target ones contain more correlated and effective information and thus facilitate more effective regularization. To this end, we propose a Perception and Semantic aware Sequence Regularization framework, which explore perceptively and semantically correlated tokens/sequences as regularization. Specifically, we introduce a semantic context-free recognition and a language model to acquire similar sequences with high perceptive similarities and semantic correlation, respectively. Moreover, over-confidence degree varies across samples according to their difficulties. Thus, we further design an adaptive calibration intensity module to compute a difficulty score for each samples to obtain finer-grained regularization. Extensive experiments on canonical sequence recognition tasks, including scene text and speech recognition, demonstrate that our method sets novel state-of-the-art results. Code is available at https://github.com/husterpzh/PSSR.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2305.14014.pdf' target='_blank'>https://arxiv.org/pdf/2305.14014.pdf</a></span>   <span><a href='https://github.com/VamosC/CLIP4STR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Zhao, Ruijie Quan, Linchao Zhu, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14014">CLIP4STR: A Simple Baseline for Scene Text Recognition with Pre-trained Vision-Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained vision-language models~(VLMs) are the de-facto foundation models for various downstream tasks. However, scene text recognition methods still prefer backbones pre-trained on a single modality, namely, the visual modality, despite the potential of VLMs to serve as powerful scene text readers. For example, CLIP can robustly identify regular (horizontal) and irregular (rotated, curved, blurred, or occluded) text in images. With such merits, we transform CLIP into a scene text reader and introduce CLIP4STR, a simple yet effective STR method built upon image and text encoders of CLIP. It has two encoder-decoder branches: a visual branch and a cross-modal branch. The visual branch provides an initial prediction based on the visual feature, and the cross-modal branch refines this prediction by addressing the discrepancy between the visual feature and text semantics. To fully leverage the capabilities of both branches, we design a dual predict-and-refine decoding scheme for inference. We scale CLIP4STR in terms of the model size, pre-training data, and training data, achieving state-of-the-art performance on 13 STR benchmarks. Additionally, a comprehensive empirical study is provided to enhance the understanding of the adaptation of CLIP to STR. Our method establishes a simple yet strong baseline for future STR research with VLMs.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2305.07895.pdf' target='_blank'>https://arxiv.org/pdf/2305.07895.pdf</a></span>   <span><a href='https://github.com/Yuliang-Liu/MultimodalOCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.07895">OCRBench: On the Hidden Mystery of OCR in Large Multimodal Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large models have recently played a dominant role in natural language processing and multimodal vision-language learning. However, their effectiveness in text-related visual tasks remains relatively unexplored. In this paper, we conducted a comprehensive evaluation of Large Multimodal Models, such as GPT4V and Gemini, in various text-related visual tasks including Text Recognition, Scene Text-Centric Visual Question Answering (VQA), Document-Oriented VQA, Key Information Extraction (KIE), and Handwritten Mathematical Expression Recognition (HMER). To facilitate the assessment of Optical Character Recognition (OCR) capabilities in Large Multimodal Models, we propose OCRBench, a comprehensive evaluation benchmark. OCRBench contains 29 datasets, making it the most comprehensive OCR evaluation benchmark available. Furthermore, our study reveals both the strengths and weaknesses of these models, particularly in handling multilingual text, handwritten text, non-semantic text, and mathematical expression recognition. Most importantly, the baseline results presented in this study could provide a foundational framework for the conception and assessment of innovative strategies targeted at enhancing zero-shot multimodal techniques. The evaluation pipeline and benchmark are available at https://github.com/Yuliang-Liu/MultimodalOCR.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2305.05322.pdf' target='_blank'>https://arxiv.org/pdf/2305.05322.pdf</a></span>   <span><a href='https://github.com/simplify23/TPS_PP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianlun Zheng, Zhineng Chen, Jinfeng Bai, Hongtao Xie, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05322">TPS++: Attention-Enhanced Thin-Plate Spline for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text irregularities pose significant challenges to scene text recognizers. Thin-Plate Spline (TPS)-based rectification is widely regarded as an effective means to deal with them. Currently, the calculation of TPS transformation parameters purely depends on the quality of regressed text borders. It ignores the text content and often leads to unsatisfactory rectified results for severely distorted text. In this work, we introduce TPS++, an attention-enhanced TPS transformation that incorporates the attention mechanism to text rectification for the first time. TPS++ formulates the parameter calculation as a joint process of foreground control point regression and content-based attention score estimation, which is computed by a dedicated designed gated-attention block. TPS++ builds a more flexible content-aware rectifier, generating a natural text correction that is easier to read by the subsequent recognizer. Moreover, TPS++ shares the feature backbone with the recognizer in part and implements the rectification at feature-level rather than image-level, incurring only a small overhead in terms of parameters and inference time. Experiments on public benchmarks show that TPS++ consistently improves the recognition and achieves state-of-the-art accuracy. Meanwhile, it generalizes well on different backbones and recognizers. Code is at https://github.com/simplify23/TPS_PP.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2305.05140.pdf' target='_blank'>https://arxiv.org/pdf/2305.05140.pdf</a></span>   <span><a href='https://github.com/CyrilSterling/LPV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boqiang Zhang, Hongtao Xie, Yuxin Wang, Jianjun Xu, Yongdong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05140">Linguistic More: Taking a Further Step toward Efficient and Accurate Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision model have gained increasing attention due to their simplicity and efficiency in Scene Text Recognition (STR) task. However, due to lacking the perception of linguistic knowledge and information, recent vision models suffer from two problems: (1) the pure vision-based query results in attention drift, which usually causes poor recognition and is summarized as linguistic insensitive drift (LID) problem in this paper. (2) the visual feature is suboptimal for the recognition in some vision-missing cases (e.g. occlusion, etc.). To address these issues, we propose a $\textbf{L}$inguistic $\textbf{P}$erception $\textbf{V}$ision model (LPV), which explores the linguistic capability of vision model for accurate text recognition. To alleviate the LID problem, we introduce a Cascade Position Attention (CPA) mechanism that obtains high-quality and accurate attention maps through step-wise optimization and linguistic information mining. Furthermore, a Global Linguistic Reconstruction Module (GLRM) is proposed to improve the representation of visual features by perceiving the linguistic information in the visual space, which gradually converts visual features into semantically rich ones during the cascade process. Different from previous methods, our method obtains SOTA results while keeping low complexity (92.4% accuracy with only 8.11M parameters). Code is available at https://github.com/CyrilSterling/LPV.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2305.03327.pdf' target='_blank'>https://arxiv.org/pdf/2305.03327.pdf</a></span>   <span><a href='https://github.com/callsys/FlowText' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhong Zhao, Weijia Wu, Zhuang Li, Jiahong Li, Weiqiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.03327">FlowText: Synthesizing Realistic Scene Text Video with Optical Flow Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current video text spotting methods can achieve preferable performance, powered with sufficient labeled training data. However, labeling data manually is time-consuming and labor-intensive. To overcome this, using low-cost synthetic data is a promising alternative. This paper introduces a novel video text synthesis technique called FlowText, which utilizes optical flow estimation to synthesize a large amount of text video data at a low cost for training robust video text spotters. Unlike existing methods that focus on image-level synthesis, FlowText concentrates on synthesizing temporal information of text instances across consecutive frames using optical flow. This temporal information is crucial for accurately tracking and spotting text in video sequences, including text movement, distortion, appearance, disappearance, shelter, and blur. Experiments show that combining general detectors like TransDETR with the proposed FlowText produces remarkable results on various datasets, such as ICDAR2015video and ICDAR2013video. Code is available at https://github.com/callsys/FlowText.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2305.01443.pdf' target='_blank'>https://arxiv.org/pdf/2305.01443.pdf</a></span>   <span><a href='https://github.com/ViTAE-Transformer/SAMText' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haibin He, Jing Zhang, Mengyang Xu, Juhua Liu, Bo Du, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01443">Scalable Mask Annotation for Video Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video text spotting refers to localizing, recognizing, and tracking textual elements such as captions, logos, license plates, signs, and other forms of text within consecutive video frames. However, current datasets available for this task rely on quadrilateral ground truth annotations, which may result in including excessive background content and inaccurate text boundaries. Furthermore, methods trained on these datasets often produce prediction results in the form of quadrilateral boxes, which limits their ability to handle complex scenarios such as dense or curved text. To address these issues, we propose a scalable mask annotation pipeline called SAMText for video text spotting. SAMText leverages the SAM model to generate mask annotations for scene text images or video frames at scale. Using SAMText, we have created a large-scale dataset, SAMText-9M, that contains over 2,400 video clips sourced from existing datasets and over 9 million mask annotations. We have also conducted a thorough statistical analysis of the generated masks and their quality, identifying several research topics that could be further explored based on this dataset. The code and dataset will be released at \url{https://github.com/ViTAE-Transformer/SAMText}.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2304.14178.pdf' target='_blank'>https://arxiv.org/pdf/2304.14178.pdf</a></span>   <span><a href='https://github.com/X-PLUG/mPLUG-Owl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14178">mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2304.05568.pdf' target='_blank'>https://arxiv.org/pdf/2304.05568.pdf</a></span>   <span><a href='https://github.com/UCSB-NLP-Chang/DiffSTE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiabao Ji, Guanhua Zhang, Zhaowen Wang, Bairu Hou, Zhifei Zhang, Brian Price, Shiyu Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05568">Improving Diffusion Models for Scene Text Editing with Dual Encoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text editing is a challenging task that involves modifying or inserting specified texts in an image while maintaining its natural and realistic appearance. Most previous approaches to this task rely on style-transfer models that crop out text regions and feed them into image transfer models, such as GANs. However, these methods are limited in their ability to change text style and are unable to insert texts into images. Recent advances in diffusion models have shown promise in overcoming these limitations with text-conditional image editing. However, our empirical analysis reveals that state-of-the-art diffusion models struggle with rendering correct text and controlling text style. To address these problems, we propose DIFFSTE to improve pre-trained diffusion models with a dual encoder design, which includes a character encoder for better text legibility and an instruction encoder for better style control. An instruction tuning framework is introduced to train our model to learn the mapping from the text instruction to the corresponding image with either the specified style or the style of the surrounding texts in the background. Such a training method further brings our method the zero-shot generalization ability to the following three scenarios: generating text with unseen font variation, e.g., italic and bold, mixing different fonts to construct a new font, and using more relaxed forms of natural language as the instructions to guide the generation task. We evaluate our approach on five datasets and demonstrate its superior performance in terms of text correctness, image naturalness, and style controllability. Our code is publicly available. https://github.com/UCSB-NLP-Chang/DiffSTE
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2304.03435.pdf' target='_blank'>https://arxiv.org/pdf/2304.03435.pdf</a></span>   <span><a href='https://github.com/clovaai/units' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taeho Kil, Seonghyeon Kim, Sukmin Seo, Yoonsik Kim, Daehee Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03435">Towards Unified Scene Text Spotting based on Sequence Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sequence generation models have recently made significant progress in unifying various vision tasks. Although some auto-regressive models have demonstrated promising results in end-to-end text spotting, they use specific detection formats while ignoring various text shapes and are limited in the maximum number of text instances that can be detected. To overcome these limitations, we propose a UNIfied scene Text Spotter, called UNITS. Our model unifies various detection formats, including quadrilaterals and polygons, allowing it to detect text in arbitrary shapes. Additionally, we apply starting-point prompting to enable the model to extract texts from an arbitrary starting point, thereby extracting more texts beyond the number of instances it was trained on. Experimental results demonstrate that our method achieves competitive performance compared to state-of-the-art methods. Further analysis shows that UNITS can extract a larger number of texts than it was trained on. We provide the code for our method at https://github.com/clovaai/units.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2303.04291.pdf' target='_blank'>https://arxiv.org/pdf/2303.04291.pdf</a></span>   <span><a href='https://ccnguyen.github.io/diffusion-in-the-dark/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cindy M. Nguyen, Eric R. Chan, Alexander W. Bergman, Gordon Wetzstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.04291">Diffusion in the Dark: A Diffusion Model for Low-Light Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Capturing images is a key part of automation for high-level tasks such as scene text recognition. Low-light conditions pose a challenge for high-level perception stacks, which are often optimized on well-lit, artifact-free images. Reconstruction methods for low-light images can produce well-lit counterparts, but typically at the cost of high-frequency details critical for downstream tasks. We propose Diffusion in the Dark (DiD), a diffusion model for low-light image reconstruction for text recognition. DiD provides qualitatively competitive reconstructions with that of state-of-the-art (SOTA), while preserving high-frequency details even in extremely noisy, dark conditions. We demonstrate that DiD, without any task-specific optimization, can outperform SOTA low-light methods in low-light text recognition on real images, bolstering the potential of diffusion models to solve ill-posed inverse problems.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2302.14338.pdf' target='_blank'>https://arxiv.org/pdf/2302.14338.pdf</a></span>   <span><a href='https://github.com/wenwenyu/TCM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenwen Yu, Yuliang Liu, Wei Hua, Deqiang Jiang, Bo Ren, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14338">Turning a CLIP Model into a Scene Text Detector</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent large-scale Contrastive Language-Image Pretraining (CLIP) model has shown great potential in various downstream tasks via leveraging the pretrained vision and language knowledge. Scene text, which contains rich textual and visual information, has an inherent connection with a model like CLIP. Recently, pretraining approaches based on vision language models have made effective progresses in the field of text detection. In contrast to these works, this paper proposes a new method, termed TCM, focusing on Turning the CLIP Model directly for text detection without pretraining process. We demonstrate the advantages of the proposed TCM as follows: (1) The underlying principle of our framework can be applied to improve existing scene text detector. (2) It facilitates the few-shot training capability of existing methods, e.g., by using 10% of labeled data, we significantly improve the performance of the baseline method with an average of 22% in terms of the F-measure on 4 benchmarks. (3) By turning the CLIP model into existing scene text detection methods, we further achieve promising domain adaptation ability. The code will be publicly released at https://github.com/wenwenyu/TCM.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2302.10414.pdf' target='_blank'>https://arxiv.org/pdf/2302.10414.pdf</a></span>   <span><a href='https://github.com/jdfxzzy/DPMN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shipeng Zhu, Zuoyan Zhao, Pengfei Fang, Hui Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10414">Improving Scene Text Image Super-resolution via Dual Prior Modulation Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text image super-resolution (STISR) aims to simultaneously increase the resolution and legibility of the text images, and the resulting images will significantly affect the performance of downstream tasks. Although numerous progress has been made, existing approaches raise two crucial issues: (1) They neglect the global structure of the text, which bounds the semantic determinism of the scene text. (2) The priors, e.g., text prior or stroke prior, employed in existing works, are extracted from pre-trained text recognizers. That said, such priors suffer from the domain gap including low resolution and blurriness caused by poor imaging conditions, leading to incorrect guidance. Our work addresses these gaps and proposes a plug-and-play module dubbed Dual Prior Modulation Network (DPMN), which leverages dual image-level priors to bring performance gain over existing approaches. Specifically, two types of prior-guided refinement modules, each using the text mask or graphic recognition result of the low-quality SR image from the preceding layer, are designed to improve the structural clarity and semantic accuracy of the text, respectively. The following attention mechanism hence modulates two quality-enhanced images to attain a superior SR result. Extensive experiments validate that our method improves the image quality and boosts the performance of downstream tasks over five typical approaches on the benchmark. Substantial visualizations and ablation studies demonstrate the advantages of the proposed DPMN. Code is available at: https://github.com/jdfxzzy/DPMN.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2302.03873.pdf' target='_blank'>https://arxiv.org/pdf/2302.03873.pdf</a></span>   <span><a href='https://github.com/ACRA-FL/GeoTRNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>P. N. Deelaka, D. R. Jayakodi, D. Y. Silva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03873">Geometric Perception based Efficient Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Every Scene Text Recognition (STR) task consists of text localization \& text recognition as the prominent sub-tasks. However, in real-world applications with fixed camera positions such as equipment monitor reading, image-based data entry, and printed document data extraction, the underlying data tends to be regular scene text. Hence, in these tasks, the use of generic, bulky models comes up with significant disadvantages compared to customized, efficient models in terms of model deployability, data privacy \& model reliability. Therefore, this paper introduces the underlying concepts, theory, implementation, and experiment results to develop models, which are highly specialized for the task itself, to achieve not only the SOTA performance but also to have minimal model weights, shorter inference time, and high model reliability. We introduce a novel deep learning architecture (GeoTRNet), trained to identify digits in a regular scene image, only using the geometrical features present, mimicking human perception over text recognition. The code is publicly available at https://github.com/ACRA-FL/GeoTRNet
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2301.08898.pdf' target='_blank'>https://arxiv.org/pdf/2301.08898.pdf</a></span>   <span><a href='https://github.com/fh2019ustc/PolySnake' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Feng, Keyi Zhou, Wengang Zhou, Yufei Yin, Jiajun Deng, Qi Sun, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.08898">Recurrent Generic Contour-based Instance Segmentation with Progressive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contour-based instance segmentation has been actively studied, thanks to its flexibility and elegance in processing visual objects within complex backgrounds. In this work, we propose a novel deep network architecture, i.e., PolySnake, for generic contour-based instance segmentation. Motivated by the classic Snake algorithm, the proposed PolySnake achieves superior and robust segmentation performance with an iterative and progressive contour refinement strategy. Technically, PolySnake introduces a recurrent update operator to estimate the object contour iteratively. It maintains a single estimate of the contour that is progressively deformed toward the object boundary. At each iteration, PolySnake builds a semantic-rich representation for the current contour and feeds it to the recurrent operator for further contour adjustment. Through the iterative refinements, the contour progressively converges to a stable status that tightly encloses the object instance. Beyond the scope of general instance segmentation, extensive experiments are conducted to validate the effectiveness and generalizability of our PolySnake in two additional specific task scenarios, including scene text detection and lane detection. The results demonstrate that the proposed PolySnake outperforms the existing advanced methods on several multiple prevalent benchmarks across the three tasks. The codes and pre-trained models are available at https://github.com/fh2019ustc/PolySnake
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2301.01635.pdf' target='_blank'>https://arxiv.org/pdf/2301.01635.pdf</a></span>   <span><a href='https://github.com/Yuliang-Liu/SPTSv2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuliang Liu, Jiaxin Zhang, Dezhi Peng, Mingxin Huang, Xinyu Wang, Jingqun Tang, Can Huang, Dahua Lin, Chunhua Shen, Xiang Bai, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.01635">SPTS v2: Single-Point Scene Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end scene text spotting has made significant progress due to its intrinsic synergy between text detection and recognition. Previous methods commonly regard manual annotations such as horizontal rectangles, rotated rectangles, quadrangles, and polygons as a prerequisite, which are much more expensive than using single-point. Our new framework, SPTS v2, allows us to train high-performing text-spotting models using a single-point annotation. SPTS v2 reserves the advantage of the auto-regressive Transformer with an Instance Assignment Decoder (IAD) through sequentially predicting the center points of all text instances inside the same predicting sequence, while with a Parallel Recognition Decoder (PRD) for text recognition in parallel, which significantly reduces the requirement of the length of the sequence. These two decoders share the same parameters and are interactively connected with a simple but effective information transmission process to pass the gradient and information. Comprehensive experiments on various existing benchmark datasets demonstrate the SPTS v2 can outperform previous state-of-the-art single-point text spotters with fewer parameters while achieving 19$\times$ faster inference speed. Within the context of our SPTS v2 framework, our experiments suggest a potential preference for single-point representation in scene text spotting when compared to other representations. Such an attempt provides a significant opportunity for scene text spotting applications beyond the realms of existing paradigms. Code is available at: https://github.com/Yuliang-Liu/SPTSv2.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2212.02340.pdf' target='_blank'>https://arxiv.org/pdf/2212.02340.pdf</a></span>   <span><a href='https://github.com/XiiZhao/cbn.pytorch' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/XiiZhao/cbn.pytorch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Zhao, Wei Feng, Zheng Zhang, Jingjing Lv, Xin Zhu, Zhangang Lin, Jinghe Hu, Jingping Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.02340">CBNet: A Plug-and-Play Network for Segmentation-Based Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, segmentation-based methods are quite popular in scene text detection, which mainly contain two steps: text kernel segmentation and expansion. However, the segmentation process only considers each pixel independently, and the expansion process is difficult to achieve a favorable accuracy-speed trade-off. In this paper, we propose a Context-aware and Boundary-guided Network (CBN) to tackle these problems. In CBN, a basic text detector is firstly used to predict initial segmentation results. Then, we propose a context-aware module to enhance text kernel feature representations, which considers both global and local contexts. Finally, we introduce a boundary-guided module to expand enhanced text kernels adaptively with only the pixels on the contours, which not only obtains accurate text boundaries but also keeps high speed, especially on high-resolution output maps. In particular, with a lightweight backbone, the basic detector equipped with our proposed CBN achieves state-of-the-art results on several popular benchmarks, and our proposed CBN can be plugged into several segmentation-based methods. Code is available at https://github.com/XiiZhao/cbn.pytorch.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2211.10772.pdf' target='_blank'>https://arxiv.org/pdf/2211.10772.pdf</a></span>   <span><a href='https://github.com/ViTAE-Transformer/DeepSolo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maoyuan Ye, Jing Zhang, Shanshan Zhao, Juhua Liu, Tongliang Liu, Bo Du, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.10772">DeepSolo: Let Transformer Decoder with Explicit Points Solo for Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end text spotting aims to integrate scene text detection and recognition into a unified framework. Dealing with the relationship between the two sub-tasks plays a pivotal role in designing effective spotters. Although Transformer-based methods eliminate the heuristic post-processing, they still suffer from the synergy issue between the sub-tasks and low training efficiency. In this paper, we present DeepSolo, a simple DETR-like baseline that lets a single Decoder with Explicit Points Solo for text detection and recognition simultaneously. Technically, for each text instance, we represent the character sequence as ordered points and model them with learnable explicit point queries. After passing a single decoder, the point queries have encoded requisite text semantics and locations, thus can be further decoded to the center line, boundary, script, and confidence of text via very simple prediction heads in parallel. Besides, we also introduce a text-matching criterion to deliver more accurate supervisory signals, thus enabling more efficient training. Quantitative experiments on public benchmarks demonstrate that DeepSolo outperforms previous state-of-the-art methods and achieves better training efficiency. In addition, DeepSolo is also compatible with line annotations, which require much less annotation cost than polygons. The code is available at https://github.com/ViTAE-Transformer/DeepSolo.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2207.00193.pdf' target='_blank'>https://arxiv.org/pdf/2207.00193.pdf</a></span>   <span><a href='https://github.com/ayumiymk/DiG' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ayumiymk/DiG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingkun Yang, Minghui Liao, Pu Lu, Jing Wang, Shenggao Zhu, Hualin Luo, Qi Tian, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.00193">Reading and Writing: Discriminative and Generative Modeling for Self-Supervised Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing text recognition methods usually need large-scale training data. Most of them rely on synthetic training data due to the lack of annotated real images. However, there is a domain gap between the synthetic data and real data, which limits the performance of the text recognition models. Recent self-supervised text recognition methods attempted to utilize unlabeled real images by introducing contrastive learning, which mainly learns the discrimination of the text images. Inspired by the observation that humans learn to recognize the texts through both reading and writing, we propose to learn discrimination and generation by integrating contrastive learning and masked image modeling in our self-supervised method. The contrastive learning branch is adopted to learn the discrimination of text images, which imitates the reading behavior of humans. Meanwhile, masked image modeling is firstly introduced for text recognition to learn the context generation of the text images, which is similar to the writing behavior. The experimental results show that our method outperforms previous self-supervised text recognition methods by 10.2%-20.2% on irregular scene text recognition datasets. Moreover, our proposed text recognizer exceeds previous state-of-the-art text recognition methods by averagely 5.3% on 11 benchmarks, with similar model size. We also demonstrate that our pre-trained model can be easily applied to other text-related tasks with obvious performance gain. The code is available at https://github.com/ayumiymk/DiG.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2111.11011.pdf' target='_blank'>https://arxiv.org/pdf/2111.11011.pdf</a></span>   <span><a href='https://github.com/simplify23/CDistNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianlun Zheng, Zhineng Chen, Shancheng Fang, Hongtao Xie, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.11011">CDistNet: Perceiving Multi-Domain Character Distance for Robust Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Transformer-based encoder-decoder framework is becoming popular in scene text recognition, largely because it naturally integrates recognition clues from both visual and semantic domains. However, recent studies show that the two kinds of clues are not always well registered and therefore, feature and character might be misaligned in difficult text (e.g., with a rare shape). As a result, constraints such as character position are introduced to alleviate this problem. Despite certain success, visual and semantic are still separately modeled and they are merely loosely associated. In this paper, we propose a novel module called Multi-Domain Character Distance Perception (MDCDP) to establish a visually and semantically related position embedding. MDCDP uses the position embedding to query both visual and semantic features following the cross-attention mechanism. The two kinds of clues are fused into the position branch, generating a content-aware embedding that well perceives character spacing and orientation variants, character semantic affinities, and clues tying the two kinds of information. They are summarized as the multi-domain character distance. We develop CDistNet that stacks multiple MDCDPs to guide a gradually precise distance modeling. Thus, the feature-character alignment is well built even various recognition difficulties are presented. We verify CDistNet on ten challenging public datasets and two series of augmented datasets created by ourselves. The experiments demonstrate that CDistNet performs highly competitively. It not only ranks top-tier in standard benchmarks, but also outperforms recent popular methods by obvious margins on real and augmented datasets presenting severe text deformation, poor linguistic support, and rare character layouts. Code is available at https://github.com/simplify23/CDistNet.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2111.02394.pdf' target='_blank'>https://arxiv.org/pdf/2111.02394.pdf</a></span>   <span><a href='https://github.com/czczup/FAST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Chen, Jiahao Wang, Wenhai Wang, Guo Chen, Enze Xie, Ping Luo, Tong Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.02394">FAST: Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an accurate and efficient scene text detection framework, termed FAST (i.e., faster arbitrarily-shaped text detector). Different from recent advanced text detectors that used complicated post-processing and hand-crafted network architectures, resulting in low inference speed, FAST has two new designs. (1) We design a minimalist kernel representation (only has 1-channel output) to model text with arbitrary shape, as well as a GPU-parallel post-processing to efficiently assemble text lines with a negligible time overhead. (2) We search the network architecture tailored for text detection, leading to more powerful features than most networks that are searched for image classification. Benefiting from these two designs, FAST achieves an excellent trade-off between accuracy and efficiency on several challenging datasets, including Total Text, CTW1500, ICDAR 2015, and MSRA-TD500. For example, FAST-T yields 81.6% F-measure at 152 FPS on Total-Text, outperforming the previous fastest method by 1.7 points and 70 FPS in terms of accuracy and speed. With TensorRT optimization, the inference speed can be further accelerated to over 600 FPS. Code and models will be released at https://github.com/czczup/FAST.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2306.04362.pdf' target='_blank'>https://arxiv.org/pdf/2306.04362.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyang Xu, Qinghao Ye, Xuan Wu, Ming Yan, Yuan Miao, Jiabo Ye, Guohai Xu, Anwen Hu, Yaya Shi, Guangwei Xu, Chenliang Li, Qi Qian, Maofei Que, Ji Zhang, Xiao Zeng, Fei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.04362">Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To promote the development of Vision-Language Pre-training (VLP) and multimodal Large Language Model (LLM) in the Chinese community, we firstly release the largest public Chinese high-quality video-language dataset named Youku-mPLUG, which is collected from Youku, a well-known Chinese video-sharing website, with strict criteria of safety, diversity, and quality. Youku-mPLUG contains 10 million Chinese video-text pairs filtered from 400 million raw videos across a wide range of 45 diverse categories for large-scale pre-training. In addition, to facilitate a comprehensive evaluation of video-language models, we carefully build the largest human-annotated Chinese benchmarks covering three popular video-language tasks of cross-modal retrieval, video captioning, and video category classification. Youku-mPLUG can enable researchers to conduct more in-depth multimodal research and develop better applications in the future. Furthermore, we release popular video-language pre-training models, ALPRO and mPLUG-2, and our proposed modularized decoder-only model mPLUG-video pre-trained on Youku-mPLUG. Experiments show that models pre-trained on Youku-mPLUG gain up to 23.1% improvement in video category classification. Besides, mPLUG-video achieves a new state-of-the-art result on these benchmarks with 80.5% top-1 accuracy in video category classification and 68.9 CIDEr score in video captioning, respectively. Finally, we scale up mPLUG-video based on the frozen Bloomz with only 1.7% trainable parameters as Chinese multimodal LLM, and demonstrate impressive instruction and video understanding ability. The zero-shot instruction understanding experiment indicates that pretraining with Youku-mPLUG can enhance the ability to comprehend overall and detailed visual semantics, recognize scene text, and leverage open-domain knowledge.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2503.20198.pdf' target='_blank'>https://arxiv.org/pdf/2503.20198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, Min Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20198">Beyond Words: Advancing Long-Text Image Generation via Multimodal Autoregressive Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in autoregressive and diffusion models have led to strong performance in image generation with short scene text words. However, generating coherent, long-form text in images, such as paragraphs in slides or documents, remains a major challenge for current generative models. We present the first work specifically focused on long text image generation, addressing a critical gap in existing text-to-image systems that typically handle only brief phrases or single sentences. Through comprehensive analysis of state-of-the-art autoregressive generation models, we identify the image tokenizer as a critical bottleneck in text generating quality. To address this, we introduce a novel text-focused, binary tokenizer optimized for capturing detailed scene text features. Leveraging our tokenizer, we develop \ModelName, a multimodal autoregressive model that excels in generating high-quality long-text images with unprecedented fidelity. Our model offers robust controllability, enabling customization of text properties such as font style, size, color, and alignment. Extensive experiments demonstrate that \ModelName~significantly outperforms SD3.5 Large~\cite{sd3} and GPT4o~\cite{gpt4o} with DALL-E 3~\cite{dalle3} in generating long text accurately, consistently, and flexibly. Beyond its technical achievements, \ModelName~opens up exciting opportunities for innovative applications like interleaved document and PowerPoint generation, establishing a new frontier in long-text image generating.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2411.16713.pdf' target='_blank'>https://arxiv.org/pdf/2411.16713.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taewook Kim, Ze Wang, Zhengyuan Yang, Jiang Wang, Lijuan Wang, Zicheng Liu, Qiang Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16713">Conditional Text-to-Image Generation with Reference Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image diffusion models have demonstrated tremendous success in synthesizing visually stunning images given textual instructions. Despite remarkable progress in creating high-fidelity visuals, text-to-image models can still struggle with precisely rendering subjects, such as text spelling. To address this challenge, this paper explores using additional conditions of an image that provides visual guidance of the particular subjects for diffusion models to generate. In addition, this reference condition empowers the model to be conditioned in ways that the vocabularies of the text tokenizer cannot adequately represent, and further extends the model's generalization to novel capabilities such as generating non-English text spellings. We develop several small-scale expert plugins that efficiently endow a Stable Diffusion model with the capability to take different references. Each plugin is trained with auxiliary networks and loss functions customized for applications such as English scene-text generation, multi-lingual scene-text generation, and logo-image generation. Our expert plugins demonstrate superior results than the existing methods on all tasks, each containing only 28.55M trainable parameters.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2312.13503.pdf' target='_blank'>https://arxiv.org/pdf/2312.13503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingbing Wen, Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Bill Howe, Lijuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.13503">InfoVisDial: An Informative Visual Dialogue Dataset by Bridging Large Multimodal and Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we build a visual dialogue dataset, named InfoVisDial, which provides rich informative answers in each round even with external knowledge related to the visual content. Different from existing datasets where the answer is compact and short, InfoVisDial contains long free-form answers with rich information in each round of dialogue. For effective data collection, the key idea is to bridge the large-scale multimodal model (e.g., GIT) and the language models (e.g., GPT-3). GIT can describe the image content even with scene text, while GPT-3 can generate informative dialogue based on the image description and appropriate prompting techniques. With such automatic pipeline, we can readily generate informative visual dialogue data at scale. Then, we ask human annotators to rate the generated dialogues to filter the low-quality conversations.Human analyses show that InfoVisDial covers informative and diverse dialogue topics: $54.4\%$ of the dialogue rounds are related to image scene texts, and $36.7\%$ require external knowledge. Each round's answer is also long and open-ended: $87.3\%$ of answers are unique with an average length of $8.9$, compared with $27.37\%$ and $2.9$ in VisDial. Last, we propose a strong baseline by adapting the GIT model for the visual dialogue task and fine-tune the model on InfoVisDial. Hopefully, our work can motivate more effort on this direction.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2407.13219.pdf' target='_blank'>https://arxiv.org/pdf/2407.13219.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Feng, Xin Wang, Hong Chen, Zeyang Zhang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13219">Multi-sentence Video Grounding for Long Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation has witnessed great success recently, but their application in generating long videos still remains challenging due to the difficulty in maintaining the temporal consistency of generated videos and the high memory cost during generation. To tackle the problems, in this paper, we propose a brave and new idea of Multi-sentence Video Grounding for Long Video Generation, connecting the massive video moment retrieval to the video generation task for the first time, providing a new paradigm for long video generation. The method of our work can be summarized as three steps: (i) We design sequential scene text prompts as the queries for video grounding, utilizing the massive video moment retrieval to search for video moment segments that meet the text requirements in the video database. (ii) Based on the source frames of retrieved video moment segments, we adopt video editing methods to create new video content while preserving the temporal consistency of the retrieved video. Since the editing can be conducted segment by segment, and even frame by frame, it largely reduces the memory cost. (iii) We also attempt video morphing and personalized generation methods to improve the subject consistency of long video generation, providing ablation experimental results for the subtasks of long video generation. Our approach seamlessly extends the development in image/video editing, video morphing and personalized generation, and video grounding to the long video generation, offering effective solutions for generating long videos at low memory cost.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2407.17020.pdf' target='_blank'>https://arxiv.org/pdf/2407.17020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyang Yu, Teng Fu, Bin Li, Xiangyang Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17020">EAFormer: Scene Text Segmentation with Edge-Aware Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text segmentation aims at cropping texts from scene images, which is usually used to help generative models edit or remove texts. The existing text segmentation methods tend to involve various text-related supervisions for better performance. However, most of them ignore the importance of text edges, which are significant for downstream applications. In this paper, we propose Edge-Aware Transformers, termed EAFormer, to segment texts more accurately, especially at the edge of texts. Specifically, we first design a text edge extractor to detect edges and filter out edges of non-text areas. Then, we propose an edge-guided encoder to make the model focus more on text edges. Finally, an MLP-based decoder is employed to predict text masks. We have conducted extensive experiments on commonly-used benchmarks to verify the effectiveness of EAFormer. The experimental results demonstrate that the proposed method can perform better than previous methods, especially on the segmentation of text edges. Considering that the annotations of several benchmarks (e.g., COCO_TS and MLT_S) are not accurate enough to fairly evaluate our methods, we have relabeled these datasets. Through experiments, we observe that our method can achieve a higher performance improvement when more accurate annotations are used for training.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2309.01081.pdf' target='_blank'>https://arxiv.org/pdf/2309.01081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyang Yu, Xiaocong Wang, Bin Li, Xiangyang Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01081">Orientation-Independent Chinese Text Recognition in Scene Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition (STR) has attracted much attention due to its broad applications. The previous works pay more attention to dealing with the recognition of Latin text images with complex backgrounds by introducing language models or other auxiliary networks. Different from Latin texts, many vertical Chinese texts exist in natural scenes, which brings difficulties to current state-of-the-art STR methods. In this paper, we take the first attempt to extract orientation-independent visual features by disentangling content and orientation information of text images, thus recognizing both horizontal and vertical texts robustly in natural scenes. Specifically, we introduce a Character Image Reconstruction Network (CIRN) to recover corresponding printed character images with disentangled content and orientation information. We conduct experiments on a scene dataset for benchmarking Chinese text recognition, and the results demonstrate that the proposed method can indeed improve performance through disentangling content and orientation information. To further validate the effectiveness of our method, we additionally collect a Vertical Chinese Text Recognition (VCTR) dataset. The experimental results show that the proposed method achieves 45.63% improvement on VCTR when introducing CIRN to the baseline model.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2405.19765.pdf' target='_blank'>https://arxiv.org/pdf/2405.19765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Wan, Chengquan Zhang, Pengyuan Lyu, Sen Fan, Zihan Ni, Kun Yao, Errui Ding, Jingdong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19765">Towards Unified Multi-granularity Text Detection with Interactive Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing OCR engines or document image analysis systems typically rely on training separate models for text detection in varying scenarios and granularities, leading to significant computational complexity and resource demands. In this paper, we introduce "Detect Any Text" (DAT), an advanced paradigm that seamlessly unifies scene text detection, layout analysis, and document page detection into a cohesive, end-to-end model. This design enables DAT to efficiently manage text instances at different granularities, including *word*, *line*, *paragraph* and *page*. A pivotal innovation in DAT is the across-granularity interactive attention module, which significantly enhances the representation learning of text instances at varying granularities by correlating structural information across different text queries. As a result, it enables the model to achieve mutually beneficial detection performances across multiple text granularities. Additionally, a prompt-based segmentation module refines detection outcomes for texts of arbitrary curvature and complex layouts, thereby improving DAT's accuracy and expanding its real-world applicability. Experimental results demonstrate that DAT achieves state-of-the-art performances across a variety of text-related benchmarks, including multi-oriented/arbitrarily-shaped scene text detection, document layout analysis and page detection tasks.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2404.19652.pdf' target='_blank'>https://arxiv.org/pdf/2404.19652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuliang Liu, Mingxin Huang, Hao Yan, Linger Deng, Weijia Wu, Hao Lu, Chunhua Shen, Lianwen Jin, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19652">VimTS: A Unified Video and Image Text Spotter for Enhancing the Cross-domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text spotting, a task involving the extraction of textual information from image or video sequences, faces challenges in cross-domain adaption, such as image-to-image and image-to-video generalization. In this paper, we introduce a new method, termed VimTS, which enhances the generalization ability of the model by achieving better synergy among different tasks. Typically, we propose a Prompt Queries Generation Module and a Tasks-aware Adapter to effectively convert the original single-task model into a multi-task model suitable for both image and video scenarios with minimal additional parameters. The Prompt Queries Generation Module facilitates explicit interaction between different tasks, while the Tasks-aware Adapter helps the model dynamically learn suitable features for each task. Additionally, to further enable the model to learn temporal information at a lower cost, we propose a synthetic video text dataset (VTD-368k) by leveraging the Content Deformation Fields (CoDeF) algorithm. Notably, our method outperforms the state-of-the-art method by an average of 2.6% in six cross-domain benchmarks such as TT-to-IC15, CTW1500-to-TT, and TT-to-CTW1500. For video-level cross-domain adaption, our method even surpasses the previous end-to-end video spotting method in ICDAR2015 video and DSText v2 by an average of 5.5% on the MOTA metric, using only image-level data. We further demonstrate that existing Large Multimodal Models exhibit limitations in generating cross-domain scene text spotting, in contrast to our VimTS model which requires significantly fewer parameters and data. The code and datasets will be made available at the https://VimTextSpotter.github.io.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2312.13778.pdf' target='_blank'>https://arxiv.org/pdf/2312.13778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linger Deng, Mingxin Huang, Xudong Xie, Yuliang Liu, Lianwen Jin, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.13778">Progressive Evolution from Single-Point to Polygon for Scene Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of text shape representations towards compactness has enhanced text detection and spotting performance, but at a high annotation cost. Current models use single-point annotations to reduce costs, yet they lack sufficient localization information for downstream applications. To overcome this limitation, we introduce Point2Polygon, which can efficiently transform single-points into compact polygons. Our method uses a coarse-to-fine process, starting with creating and selecting anchor points based on recognition confidence, then vertically and horizontally refining the polygon using recognition information to optimize its shape. We demonstrate the accuracy of the generated polygons through extensive experiments: 1) By creating polygons from ground truth points, we achieved an accuracy of 82.0% on ICDAR 2015; 2) In training detectors with polygons generated by our method, we attained 86% of the accuracy relative to training with ground truth (GT); 3) Additionally, the proposed Point2Polygon can be seamlessly integrated to empower single-point spotters to generate polygons. This integration led to an impressive 82.5% accuracy for the generated polygons. It is worth mentioning that our method relies solely on synthetic recognition information, eliminating the need for any manual annotation beyond single points.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2311.16555.pdf' target='_blank'>https://arxiv.org/pdf/2311.16555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ling Fu, Zijie Wu, Yingying Zhu, Yuliang Liu, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16555">Enhancing Scene Text Detectors with Realistic Text Image Synthesis Using Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text detection techniques have garnered significant attention due to their wide-ranging applications. However, existing methods have a high demand for training data, and obtaining accurate human annotations is labor-intensive and time-consuming. As a solution, researchers have widely adopted synthetic text images as a complementary resource to real text images during pre-training. Yet there is still room for synthetic datasets to enhance the performance of scene text detectors. We contend that one main limitation of existing generation methods is the insufficient integration of foreground text with the background. To alleviate this problem, we present the Diffusion Model based Text Generator (DiffText), a pipeline that utilizes the diffusion model to seamlessly blend foreground text regions with the background's intrinsic features. Additionally, we propose two strategies to generate visually coherent text with fewer spelling errors. With fewer text instances, our produced text images consistently surpass other synthetic data in aiding text detectors. Extensive experiments on detecting horizontal, rotated, curved, and line-level texts demonstrate the effectiveness of DiffText in producing realistic text images.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2403.20271.pdf' target='_blank'>https://arxiv.org/pdf/2403.20271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.20271">Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present the Draw-and-Understand framework, exploring how to integrate visual prompting understanding capabilities into Multimodal Large Language Models (MLLMs). Visual prompts allow users to interact through multi-modal instructions, enhancing the models' interactivity and fine-grained image comprehension. In this framework, we propose a general architecture adaptable to different pre-trained MLLMs, enabling it to recognize various types of visual prompts (such as points, bounding boxes, and free-form shapes) alongside language understanding. Additionally, we introduce MDVP-Instruct-Data, a multi-domain dataset featuring 1.2 million image-visual prompt-text triplets, including natural images, document images, scene text images, mobile/web screenshots, and remote sensing images. Building on this dataset, we introduce MDVP-Bench, a challenging benchmark designed to evaluate a model's ability to understand visual prompting instructions. The experimental results demonstrate that our framework can be easily and effectively applied to various MLLMs, such as SPHINX-X and LLaVA. After training with MDVP-Instruct-Data and image-level instruction datasets, our models exhibit impressive multimodal interaction capabilities and pixel-level understanding, while maintaining their image-level visual perception performance.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2402.08017.pdf' target='_blank'>https://arxiv.org/pdf/2402.08017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashish Shenoy, Yichao Lu, Srihari Jayakumar, Debojeet Chatterjee, Mohsen Moslehpour, Pierce Chuang, Abhay Harpale, Vikas Bhardwaj, Di Xu, Shicong Zhao, Longfang Zhao, Ankit Ramchandani, Xin Luna Dong, Anuj Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08017">Lumos : Empowering Multimodal LLMs with Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the core of Lumos is a Scene Text Recognition (STR) component that extracts text from first person point-of-view images, the output of which is used to augment input to a Multimodal Large Language Model (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2404.00852.pdf' target='_blank'>https://arxiv.org/pdf/2404.00852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hieu Nguyen, Cong-Hoang Ta, Phuong-Thuy Le-Nguyen, Minh-Triet Tran, Trung-Nghia Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00852">Ensemble Learning for Vietnamese Scene Text Spotting in Urban Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a simple yet efficient ensemble learning framework for Vietnamese scene text spotting. Leveraging the power of ensemble learning, which combines multiple models to yield more accurate predictions, our approach aims to significantly enhance the performance of scene text spotting in challenging urban settings. Through experimental evaluations on the VinText dataset, our proposed method achieves a significant improvement in accuracy compared to existing methods with an impressive accuracy of 5%. These results unequivocally demonstrate the efficacy of ensemble learning in the context of Vietnamese scene text spotting in urban environments, highlighting its potential for real world applications, such as text detection and recognition in urban signage, advertisements, and various text-rich urban scenes.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2403.13307.pdf' target='_blank'>https://arxiv.org/pdf/2403.13307.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peishan Cong, Ziyi Wang, Zhiyang Dou, Yiming Ren, Wei Yin, Kai Cheng, Yujing Sun, Xiaoxiao Long, Xinge Zhu, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13307">LaserHuman: Language-guided Scene-aware Human Motion Generation in Free Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-guided scene-aware human motion generation has great significance for entertainment and robotics. In response to the limitations of existing datasets, we introduce LaserHuman, a pioneering dataset engineered to revolutionize Scene-Text-to-Motion research. LaserHuman stands out with its inclusion of genuine human motions within 3D environments, unbounded free-form natural language descriptions, a blend of indoor and outdoor scenarios, and dynamic, ever-changing scenes. Diverse modalities of capture data and rich annotations present great opportunities for the research of conditional motion generation, and can also facilitate the development of real-life applications. Moreover, to generate semantically consistent and physically plausible human motions, we propose a multi-conditional diffusion model, which is simple but effective, achieving state-of-the-art performance on existing datasets.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2503.18746.pdf' target='_blank'>https://arxiv.org/pdf/2503.18746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Zhang, Chang Liu, Jin Wei, Xiaomeng Yang, Yu Zhou, Can Ma, Xiangyang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18746">Linguistics-aware Masked Image Modeling for Self-supervised Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text images are unique in their dual nature, encompassing both visual and linguistic information. The visual component encompasses structural and appearance-based features, while the linguistic dimension incorporates contextual and semantic elements. In scenarios with degraded visual quality, linguistic patterns serve as crucial supplements for comprehension, highlighting the necessity of integrating both aspects for robust scene text recognition (STR). Contemporary STR approaches often use language models or semantic reasoning modules to capture linguistic features, typically requiring large-scale annotated datasets. Self-supervised learning, which lacks annotations, presents challenges in disentangling linguistic features related to the global context. Typically, sequence contrastive learning emphasizes the alignment of local features, while masked image modeling (MIM) tends to exploit local structures to reconstruct visual patterns, resulting in limited linguistic knowledge. In this paper, we propose a Linguistics-aware Masked Image Modeling (LMIM) approach, which channels the linguistic information into the decoding process of MIM through a separate branch. Specifically, we design a linguistics alignment module to extract vision-independent features as linguistic guidance using inputs with different visual appearances. As features extend beyond mere visual structures, LMIM must consider the global context to achieve reconstruction. Extensive experiments on various benchmarks quantitatively demonstrate our state-of-the-art performance, and attention visualizations qualitatively show the simultaneous capture of both visual and linguistic information.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2308.04352.pdf' target='_blank'>https://arxiv.org/pdf/2308.04352.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, Qing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04352">3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D vision-language grounding (3D-VL) is an emerging field that aims to connect the 3D physical world with natural language, which is crucial for achieving embodied intelligence. Current 3D-VL models rely heavily on sophisticated modules, auxiliary losses, and optimization tricks, which calls for a simple and unified model. In this paper, we propose 3D-VisTA, a pre-trained Transformer for 3D Vision and Text Alignment that can be easily adapted to various downstream tasks. 3D-VisTA simply utilizes self-attention layers for both single-modal modeling and multi-modal fusion without any sophisticated task-specific design. To further enhance its performance on 3D-VL tasks, we construct ScanScribe, the first large-scale 3D scene-text pairs dataset for 3D-VL pre-training. ScanScribe contains 2,995 RGB-D scans for 1,185 unique indoor scenes originating from ScanNet and 3R-Scan datasets, along with paired 278K scene descriptions generated from existing 3D-VL tasks, templates, and GPT-3. 3D-VisTA is pre-trained on ScanScribe via masked language/object modeling and scene-text matching. It achieves state-of-the-art results on various 3D-VL tasks, ranging from visual grounding and dense captioning to question answering and situated reasoning. Moreover, 3D-VisTA demonstrates superior data efficiency, obtaining strong performance even with limited annotations during downstream task fine-tuning.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2409.13576.pdf' target='_blank'>https://arxiv.org/pdf/2409.13576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingtao Lin, Heqian Qiu, Lanxiao Wang, Ruihang Wang, Linfeng Xu, Hongliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13576">Region Prompt Tuning: Fine-grained Scene Text Detection Utilizing Region Text Prompt</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in prompt tuning have successfully adapted large-scale models like Contrastive Language-Image Pre-trained (CLIP) for downstream tasks such as scene text detection. Typically, text prompt complements the text encoder's input, focusing on global features while neglecting fine-grained details, leading to fine-grained text being ignored in task of scene text detection. In this paper, we propose the region prompt tuning (RPT) method for fine-grained scene text detection, where region text prompt proposed would help focus on fine-grained features. Region prompt tuning method decomposes region text prompt into individual characters and splits visual feature map into region visual tokens, creating a one-to-one correspondence between characters and tokens. This allows a character matches the local features of a token, thereby avoiding the omission of detailed features and fine-grained text. To achieve this, we introduce a sharing position embedding to link each character with its corresponding token and employ a bidirectional distance loss to align each region text prompt character with the target ``text''. To refine the information at fine-grained level, we implement character-token level interactions before and after encoding. Our proposed method combines a general score map from the image-text process with a region score map derived from character-token matching, producing a final score map that could balance the global and local features and be fed into DBNet to detect the text. Experiments on benchmarks like ICDAR2015, TotalText, and CTW1500 demonstrate RPT impressive performance, underscoring its effectiveness for scene text detection.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2506.04983.pdf' target='_blank'>https://arxiv.org/pdf/2506.04983.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangyang Zhong, Ji Qi, Yuan Yao, Pengxin Luo, Yunfeng Yan, Donglian Qi, Zhiyuan Liu, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04983">TextVidBench: A Benchmark for Long Video Scene Text Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent progress on the short-video Text-Visual Question Answering (ViteVQA) task - largely driven by benchmarks such as M4-ViteVQA - existing datasets still suffer from limited video duration and narrow evaluation scopes, making it difficult to adequately assess the growing capabilities of powerful multimodal large language models (MLLMs). To address these limitations, we introduce TextVidBench, the first benchmark specifically designed for long-video text question answering (>3 minutes). TextVidBench makes three key contributions: 1) Cross-domain long-video coverage: Spanning 9 categories (e.g., news, sports, gaming), with an average video length of 2306 seconds, enabling more realistic evaluation of long-video understanding. 2) A three-stage evaluation framework: "Text Needle-in-Haystack -> Temporal Grounding -> Text Dynamics Captioning". 3) High-quality fine-grained annotations: Containing over 5,000 question-answer pairs with detailed semantic labeling. Furthermore, we propose an efficient paradigm for improving large models through: (i) introducing the IT-Rope mechanism and temporal prompt engineering to enhance temporal perception, (ii) adopting non-uniform positional encoding to better handle long video sequences, and (iii) applying lightweight fine-tuning on video-text data. Extensive experiments on multiple public datasets as well as TextVidBench demonstrate that our new benchmark presents significant challenges to existing models, while our proposed method offers valuable insights into improving long-video scene text understanding capabilities.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2203.03382.pdf' target='_blank'>https://arxiv.org/pdf/2203.03382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongkun Guan, Chaochen Gu, Jingzheng Tu, Xue Yang, Qi Feng, Yudi Zhao, Xiaokang Yang, Wei Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.03382">Self-supervised Implicit Glyph Attention for Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The attention mechanism has become the \emph{de facto} module in scene text recognition (STR) methods, due to its capability of extracting character-level representations. These methods can be summarized into implicit attention based and supervised attention based, depended on how the attention is computed, i.e., implicit attention and supervised attention are learned from sequence-level text annotations and or character-level bounding box annotations, respectively. Implicit attention, as it may extract coarse or even incorrect spatial regions as character attention, is prone to suffering from an alignment-drifted issue. Supervised attention can alleviate the above issue, but it is character category-specific, which requires extra laborious character-level bounding box annotations and would be memory-intensive when handling languages with larger character categories. To address the aforementioned issues, we propose a novel attention mechanism for STR, self-supervised implicit glyph attention (SIGA). SIGA delineates the glyph structures of text images by jointly self-supervised text segmentation and implicit attention alignment, which serve as the supervision to improve attention correctness without extra character-level annotations. Experimental results demonstrate that SIGA performs consistently and significantly better than previous attention-based STR methods, in terms of both attention correctness and final recognition performance on publicly available context benchmarks and our contributed contextless benchmarks.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2506.05551.pdf' target='_blank'>https://arxiv.org/pdf/2506.05551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Shu, Hangui Lin, Yexin Liu, Yan Zhang, Gangyan Zeng, Yan Li, Yu Zhou, Ser-Nam Lim, Harry Yang, Nicu Sebe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05551">When Semantics Mislead Vision: Mitigating Large Multimodal Models Hallucinations in Scene Text Spotting and Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Multimodal Models (LMMs) have achieved impressive progress in visual perception and reasoning. However, when confronted with visually ambiguous or non-semantic scene text, they often struggle to accurately spot and understand the content, frequently generating semantically plausible yet visually incorrect answers, which we refer to as semantic hallucination. In this work, we investigate the underlying causes of semantic hallucination and identify a key finding: Transformer layers in LLM with stronger attention focus on scene text regions are less prone to producing semantic hallucinations. Thus, we propose a training-free semantic hallucination mitigation framework comprising two key components: (1) ZoomText, a coarse-to-fine strategy that identifies potential text regions without external detectors; and (2) Grounded Layer Correction, which adaptively leverages the internal representations from layers less prone to hallucination to guide decoding, correcting hallucinated outputs for non-semantic samples while preserving the semantics of meaningful ones. To enable rigorous evaluation, we introduce TextHalu-Bench, a benchmark of over 1,730 samples spanning both semantic and non-semantic cases, with manually curated question-answer pairs designed to probe model hallucinations. Extensive experiments demonstrate that our method not only effectively mitigates semantic hallucination but also achieves strong performance on public benchmarks for scene text spotting and understanding.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2409.14483.pdf' target='_blank'>https://arxiv.org/pdf/2409.14483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minyi Zhao, Yang Wang, Jihong Guan, Shuigeng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14483">One Model for Two Tasks: Cooperatively Recognizing and Recovering Low-Resolution Scene Text Images by Iterative Mutual Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition (STR) from high-resolution (HR) images has been significantly successful, however text reading on low-resolution (LR) images is still challenging due to insufficient visual information. Therefore, recently many scene text image super-resolution (STISR) models have been proposed to generate super-resolution (SR) images for the LR ones, then STR is done on the SR images, which thus boosts recognition performance. Nevertheless, these methods have two major weaknesses. On the one hand, STISR approaches may generate imperfect or even erroneous SR images, which mislead the subsequent recognition of STR models. On the other hand, as the STISR and STR models are jointly optimized, to pursue high recognition accuracy, the fidelity of SR images may be spoiled. As a result, neither the recognition performance nor the fidelity of STISR models are desirable. Then, can we achieve both high recognition performance and good fidelity? To this end, in this paper we propose a novel method called IMAGE (the abbreviation of Iterative MutuAl GuidancE) to effectively recognize and recover LR scene text images simultaneously. Concretely, IMAGE consists of a specialized STR model for recognition and a tailored STISR model to recover LR images, which are optimized separately. And we develop an iterative mutual guidance mechanism, with which the STR model provides high-level semantic information as clue to the STISR model for better super-resolution, meanwhile the STISR model offers essential low-level pixel clue to the STR model for more accurate recognition. Extensive experiments on two LR datasets demonstrate the superiority of our method over the existing works on both recognition performance and super-resolution fidelity.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2407.06084.pdf' target='_blank'>https://arxiv.org/pdf/2407.06084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dejie Yang, Zhu Xu, Wentao Mo, Qingchao Chen, Siyuan Huang, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06084">3D Vision and Language Pretraining with Large-Scale Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Vision-Language Pre-training (3D-VLP) aims to provide a pre-train model which can bridge 3D scenes with natural language, which is an important technique for embodied intelligence. However, current 3D-VLP datasets are hindered by limited scene-level diversity and insufficient fine-grained annotations (only 1.2K scenes and 280K textual annotations in ScanScribe), primarily due to the labor-intensive of collecting and annotating 3D scenes. To overcome these obstacles, we construct SynVL3D, a comprehensive synthetic scene-text corpus with 10K indoor scenes and 1M descriptions at object, view, and room levels, which has the advantages of diverse scene data, rich textual descriptions, multi-grained 3D-text associations, and low collection cost. Utilizing the rich annotations in SynVL3D, we pre-train a simple and unified Transformer for aligning 3D and language with multi-grained pretraining tasks. Moreover, we propose a synthetic-to-real domain adaptation in downstream task fine-tuning process to address the domain shift. Through extensive experiments, we verify the effectiveness of our model design by achieving state-of-the-art performance on downstream tasks including visual grounding, dense captioning, and question answering.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2307.16410.pdf' target='_blank'>https://arxiv.org/pdf/2307.16410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minyi Zhao, Yi Xu, Bingjia Li, Jie Wang, Jihong Guan, Shuigeng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.16410">HiREN: Towards Higher Supervision Quality for Better Scene Text Image Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text image super-resolution (STISR) is an important pre-processing technique for text recognition from low-resolution scene images. Nowadays, various methods have been proposed to extract text-specific information from high-resolution (HR) images to supervise STISR model training. However, due to uncontrollable factors (e.g. shooting equipment, focus, and environment) in manually photographing HR images, the quality of HR images cannot be guaranteed, which unavoidably impacts STISR performance. Observing the quality issue of HR images, in this paper we propose a novel idea to boost STISR by first enhancing the quality of HR images and then using the enhanced HR images as supervision to do STISR. Concretely, we develop a new STISR framework, called High-Resolution ENhancement (HiREN) that consists of two branches and a quality estimation module. The first branch is developed to recover the low-resolution (LR) images, and the other is an HR quality enhancement branch aiming at generating high-quality (HQ) text images based on the HR images to provide more accurate supervision to the LR images. As the degradation from HQ to HR may be diverse, and there is no pixel-level supervision for HQ image generation, we design a kernel-guided enhancement network to handle various degradation, and exploit the feedback from a recognizer and text-level annotations as weak supervision signal to train the HR enhancement branch. Then, a quality estimation module is employed to evaluate the qualities of HQ images, which are used to suppress the erroneous supervision information by weighting the loss of each image. Extensive experiments on TextZoom show that HiREN can work well with most existing STISR methods and significantly boost their performances.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/1905.04693.pdf' target='_blank'>https://arxiv.org/pdf/1905.04693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangneng Zhan, Jiaxing Huang, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1905.04693">Hierarchy Composition GAN for High-fidelity Image Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the rapid progress of generative adversarial networks (GANs) in image synthesis in recent years, the existing image synthesis approaches work in either geometry domain or appearance domain alone which often introduces various synthesis artifacts. This paper presents an innovative Hierarchical Composition GAN (HIC-GAN) that incorporates image synthesis in geometry and appearance domains into an end-to-end trainable network and achieves superior synthesis realism in both domains simultaneously. We design an innovative hierarchical composition mechanism that is capable of learning realistic composition geometry and handling occlusions while multiple foreground objects are involved in image composition. In addition, we introduce a novel attention mask mechanism that guides to adapt the appearance of foreground objects which also helps to provide better training reference for learning in geometry domain. Extensive experiments on scene text image synthesis, portrait editing and indoor rendering tasks show that the proposed HIC-GAN achieves superior synthesis performance qualitatively and quantitatively.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/1901.09193.pdf' target='_blank'>https://arxiv.org/pdf/1901.09193.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changgong Zhang, Fangneng Zhan, Hongyuan Zhu, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1901.09193">Scene Text Synthesis for Efficient and Effective Deep Network Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A large amount of annotated training images is critical for training accurate and robust deep network models but the collection of a large amount of annotated training images is often time-consuming and costly. Image synthesis alleviates this constraint by generating annotated training images automatically by machines which has attracted increasing interest in the recent deep learning research. We develop an innovative image synthesis technique that composes annotated training images by realistically embedding foreground objects of interest (OOI) into background images. The proposed technique consists of two key components that in principle boost the usefulness of the synthesized images in deep network training. The first is context-aware semantic coherence which ensures that the OOI are placed around semantically coherent regions within the background image. The second is harmonious appearance adaptation which ensures that the embedded OOI are agreeable to the surrounding background from both geometry alignment and appearance realism. The proposed technique has been evaluated over two related but very different computer vision challenges, namely, scene text detection and scene text recognition. Experiments over a number of public datasets demonstrate the effectiveness of our proposed image synthesis technique - the use of our synthesized images in deep network training is capable of achieving similar or even better scene text detection and scene text recognition performance as compared with using real images.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2405.04682.pdf' target='_blank'>https://arxiv.org/pdf/2405.04682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hritik Bansal, Yonatan Bitton, Michal Yarom, Idan Szpektor, Aditya Grover, Kai-Wei Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04682">TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most of these text-to-video (T2V) generative models often produce single-scene video clips that depict an entity performing a particular action (e.g., 'a red panda climbing a tree'). However, it is pertinent to generate multi-scene videos since they are ubiquitous in the real-world (e.g., 'a red panda climbing a tree' followed by 'the red panda sleeps on the top of the tree'). To generate multi-scene videos from the pretrained T2V model, we introduce a simple and effective Time-Aligned Captions (TALC) framework. Specifically, we enhance the text-conditioning mechanism in the T2V architecture to recognize the temporal alignment between the video scenes and scene descriptions. For instance, we condition the visual features of the earlier and later scenes of the generated video with the representations of the first scene description (e.g., 'a red panda climbing a tree') and second scene description (e.g., 'the red panda sleeps on the top of the tree'), respectively. As a result, we show that the T2V model can generate multi-scene videos that adhere to the multi-scene text descriptions and be visually consistent (e.g., entity and background). Further, we finetune the pretrained T2V model with multi-scene video-text data using the TALC framework. We show that the TALC-finetuned model outperforms the baseline by achieving a relative gain of 29% in the overall score, which averages visual consistency and text adherence using human evaluation.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2209.06794.pdf' target='_blank'>https://arxiv.org/pdf/2209.06794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.06794">PaLI: A Jointly-Scaled Multilingual Language-Image Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2209.05534.pdf' target='_blank'>https://arxiv.org/pdf/2209.05534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jihyung Kil, Soravit Changpinyo, Xi Chen, Hexiang Hu, Sebastian Goodman, Wei-Lun Chao, Radu Soricut
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.05534">PreSTU: Pre-Training for Scene-Text Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to recognize and reason about text embedded in visual inputs is often lacking in vision-and-language (V&L) models, perhaps because V&L pre-training methods have often failed to include such an ability in their training objective. In this paper, we propose PreSTU, a novel pre-training recipe dedicated to scene-text understanding (STU). PreSTU introduces OCR-aware pre-training objectives that encourage the model to recognize text from an image and connect it to the rest of the image content. We implement PreSTU using a simple transformer-based encoder-decoder architecture, combined with large-scale image-text datasets with scene text obtained from an off-the-shelf OCR system. We empirically demonstrate the effectiveness of this pre-training approach on eight visual question answering and four image captioning benchmarks.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2510.02787.pdf' target='_blank'>https://arxiv.org/pdf/2510.02787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Zdenek, Wataru Shimoda, Kota Yamaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02787">OTR: Synthesizing Overlay Text Dataset for Text Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text removal is a crucial task in computer vision with applications such as privacy preservation, image editing, and media reuse. While existing research has primarily focused on scene text removal in natural images, limitations in current datasets hinder out-of-domain generalization or accurate evaluation. In particular, widely used benchmarks such as SCUT-EnsText suffer from ground truth artifacts due to manual editing, overly simplistic text backgrounds, and evaluation metrics that do not capture the quality of generated results. To address these issues, we introduce an approach to synthesizing a text removal benchmark applicable to domains other than scene texts. Our dataset features text rendered on complex backgrounds using object-aware placement and vision-language model-generated content, ensuring clean ground truth and challenging text removal scenarios. The dataset is available at https://huggingface.co/datasets/cyberagent/OTR .
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2501.15558.pdf' target='_blank'>https://arxiv.org/pdf/2501.15558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Chen, Xinyu Guo, Yadong Li, Tao Zhang, Mingan Lin, Dongdong Kuang, Youwei Zhang, Lingfeng Ming, Fengyu Zhang, Yuran Wang, Jianhua Xu, Zenan Zhou, Weipeng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15558">Ocean-OCR: Towards General OCR Application via a Vision-Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal large language models (MLLMs) have shown impressive capabilities across various domains, excelling in processing and understanding information from multiple modalities. Despite the rapid progress made previously, insufficient OCR ability hinders MLLMs from excelling in text-related tasks. In this paper, we present \textbf{Ocean-OCR}, a 3B MLLM with state-of-the-art performance on various OCR scenarios and comparable understanding ability on general tasks. We employ Native Resolution ViT to enable variable resolution input and utilize a substantial collection of high-quality OCR datasets to enhance the model performance. We demonstrate the superiority of Ocean-OCR through comprehensive experiments on open-source OCR benchmarks and across various OCR scenarios. These scenarios encompass document understanding, scene text recognition, and handwritten recognition, highlighting the robust OCR capabilities of Ocean-OCR. Note that Ocean-OCR is the first MLLM to outperform professional OCR models such as TextIn and PaddleOCR.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2412.10159.pdf' target='_blank'>https://arxiv.org/pdf/2412.10159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Lyu, Wei Wang, Dongbao Yang, Jinwen Zhong, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10159">Arbitrary Reading Order Scene Text Spotter with Local Semantics Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text spotting has attracted the enthusiasm of relative researchers in recent years. Most existing scene text spotters follow the detection-then-recognition paradigm, where the vanilla detection module hardly determines the reading order and leads to failure recognition. After rethinking the auto-regressive scene text recognition method, we find that a well-trained recognizer can implicitly perceive the local semantics of all characters in a complete word or a sentence without a character-level detection module. Local semantic knowledge not only includes text content but also spatial information in the right reading order. Motivated by the above analysis, we propose the Local Semantics Guided scene text Spotter (LSGSpotter), which auto-regressively decodes the position and content of characters guided by the local semantics. Specifically, two effective modules are proposed in LSGSpotter. On the one hand, we design a Start Point Localization Module (SPLM) for locating text start points to determine the right reading order. On the other hand, a Multi-scale Adaptive Attention Module (MAAM) is proposed to adaptively aggregate text features in a local area. In conclusion, LSGSpotter achieves the arbitrary reading order spotting task without the limitation of sophisticated detection, while alleviating the cost of computational resources with the grid sampling strategy. Extensive experiment results show LSGSpotter achieves state-of-the-art performance on the InverseText benchmark. Moreover, our spotter demonstrates superior performance on English benchmarks for arbitrary-shaped text, achieving improvements of 0.7\% and 2.5\% on Total-Text and SCUT-CTW1500, respectively. These results validate our text spotter is effective for scene texts in arbitrary reading order and shape.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2410.10133.pdf' target='_blank'>https://arxiv.org/pdf/2410.10133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weichao Zeng, Yan Shu, Zhenhang Li, Dongbao Yang, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10133">TextCtrl: Diffusion-based Scene Text Editing with Prior Guidance Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Centred on content modification and style preservation, Scene Text Editing (STE) remains a challenging task despite considerable progress in text-to-image synthesis and text-driven image manipulation recently. GAN-based STE methods generally encounter a common issue of model generalization, while Diffusion-based STE methods suffer from undesired style deviations. To address these problems, we propose TextCtrl, a diffusion-based method that edits text with prior guidance control. Our method consists of two key components: (i) By constructing fine-grained text style disentanglement and robust text glyph structure representation, TextCtrl explicitly incorporates Style-Structure guidance into model design and network training, significantly improving text style consistency and rendering accuracy. (ii) To further leverage the style prior, a Glyph-adaptive Mutual Self-attention mechanism is proposed which deconstructs the implicit fine-grained features of the source image to enhance style consistency and vision quality during inference. Furthermore, to fill the vacancy of the real-world STE evaluation benchmark, we create the first real-world image-pair dataset termed ScenePair for fair comparisons. Experiments demonstrate the effectiveness of TextCtrl compared with previous methods concerning both style fidelity and text accuracy.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2305.16172.pdf' target='_blank'>https://arxiv.org/pdf/2305.16172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaomeng Yang, Zhi Qiao, Jin Wei, Dongbao Yang, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16172">Masked and Permuted Implicit Context Learning for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Text Recognition (STR) is difficult because of the variations in text styles, shapes, and backgrounds. Though the integration of linguistic information enhances models' performance, existing methods based on either permuted language modeling (PLM) or masked language modeling (MLM) have their pitfalls. PLM's autoregressive decoding lacks foresight into subsequent characters, while MLM overlooks inter-character dependencies. Addressing these problems, we propose a masked and permuted implicit context learning network for STR, which unifies PLM and MLM within a single decoder, inheriting the advantages of both approaches. We utilize the training procedure of PLM, and to integrate MLM, we incorporate word length information into the decoding process and replace the undetermined characters with mask tokens. Besides, perturbation training is employed to train a more robust model against potential length prediction errors. Our empirical evaluations demonstrate the performance of our model. It not only achieves superior performance on the common benchmarks but also achieves a substantial improvement of $9.1\%$ on the more challenging Union14M-Benchmark.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2507.15085.pdf' target='_blank'>https://arxiv.org/pdf/2507.15085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peirong Zhang, Haowei Xu, Jiaxin Zhang, Guitao Xu, Xuhan Zheng, Zhenhua Yang, Junle Liu, Yuyi Zhang, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15085">Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text image is a unique and crucial information medium that integrates visual aesthetics and linguistic semantics in modern e-society. Due to their subtlety and complexity, the generation of text images represents a challenging and evolving frontier in the image generation field. The recent surge of specialized image generators (\emph{e.g.}, Flux-series) and unified generative models (\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a natural question: can they master the intricacies of text image generation and editing? Motivated by this, we assess current state-of-the-art generative models' capabilities in terms of text image generation and editing. We incorporate various typical optical character recognition (OCR) tasks into our evaluation and broaden the concept of text-based generation tasks into OCR generative tasks. We select 33 representative tasks and categorize them into five categories: document, handwritten text, scene text, artistic text, and complex \& layout-rich text. For comprehensive evaluation, we examine six models across both closed-source and open-source domains, using tailored, high-quality image inputs and prompts. Through this evaluation, we draw crucial observations and identify the weaknesses of current generative models for OCR tasks. We argue that photorealistic text image generation and editing should be internalized as foundational skills into general-domain generative models, rather than being delegated to specialized solutions, and we hope this empirical analysis can provide valuable insights for the community to achieve this goal. This evaluation is online and will be continuously updated at our GitHub repository.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2412.14692.pdf' target='_blank'>https://arxiv.org/pdf/2412.14692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Su, Zhineng Chen, Yongkun Du, Zhilong Ji, Kai Hu, Jinfeng Bai, Xieping Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14692">Explicit Relational Reasoning Network for Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connected component (CC) is a proper text shape representation that aligns with human reading intuition. However, CC-based text detection methods have recently faced a developmental bottleneck that their time-consuming post-processing is difficult to eliminate. To address this issue, we introduce an explicit relational reasoning network (ERRNet) to elegantly model the component relationships without post-processing. Concretely, we first represent each text instance as multiple ordered text components, and then treat these components as objects in sequential movement. In this way, scene text detection can be innovatively viewed as a tracking problem. From this perspective, we design an end-to-end tracking decoder to achieve a CC-based method dispensing with post-processing entirely. Additionally, we observe that there is an inconsistency between classification confidence and localization quality, so we propose a Polygon Monte-Carlo method to quickly and accurately evaluate the localization quality. Based on this, we introduce a position-supervised classification loss to guide the task-aligned learning of ERRNet. Experiments on challenging benchmarks demonstrate the effectiveness of our ERRNet. It consistently achieves state-of-the-art accuracy while holding highly competitive inference speed.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2412.02210.pdf' target='_blank'>https://arxiv.org/pdf/2412.02210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhibo Yang, Jun Tang, Zhaohai Li, Pengfei Wang, Jianqiang Wan, Humen Zhong, Xuejing Liu, Mingkun Yang, Peng Wang, Shuai Bai, LianWen Jin, Junyang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02210">CC-OCR: A Comprehensive and Challenging OCR Benchmark for Evaluating Large Multimodal Models in Literacy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Multimodal Models (LMMs) have demonstrated impressive performance in recognizing document images with natural language instructions. However, it remains unclear to what extent capabilities in literacy with rich structure and fine-grained visual challenges. The current landscape lacks a comprehensive benchmark to effectively measure the literate capabilities of LMMs. Existing benchmarks are often limited by narrow scenarios and specified tasks. To this end, we introduce CC-OCR, a comprehensive benchmark that possesses a diverse range of scenarios, tasks, and challenges. CC-OCR comprises four OCR-centric tracks: multi-scene text reading, multilingual text reading, document parsing, and key information extraction. It includes 39 subsets with 7,058 full annotated images, of which 41% are sourced from real applications, and released for the first time. We evaluate nine prominent LMMs and reveal both the strengths and weaknesses of these models, particularly in text grounding, multi-orientation, and hallucination of repetition. CC-OCR aims to comprehensively evaluate the capabilities of LMMs on OCR-centered tasks, facilitating continued progress in this crucial area.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2408.14998.pdf' target='_blank'>https://arxiv.org/pdf/2408.14998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alloy Das, Sanket Biswas, Umapada Pal, Josep LladÃ³s, Saumik Bhattacharya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14998">FastTextSpotter: A High-Efficiency Transformer for Multilingual Scene Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of scene text in both structured and unstructured environments presents significant challenges in optical character recognition (OCR), necessitating more efficient and robust text spotting solutions. This paper presents FastTextSpotter, a framework that integrates a Swin Transformer visual backbone with a Transformer Encoder-Decoder architecture, enhanced by a novel, faster self-attention unit, SAC2, to improve processing speeds while maintaining accuracy. FastTextSpotter has been validated across multiple datasets, including ICDAR2015 for regular texts and CTW1500 and TotalText for arbitrary-shaped texts, benchmarking against current state-of-the-art models. Our results indicate that FastTextSpotter not only achieves superior accuracy in detecting and recognizing multilingual scene text (English and Vietnamese) but also improves model efficiency, thereby setting new benchmarks in the field. This study underscores the potential of advanced transformer architectures in improving the adaptability and speed of text spotting applications in diverse real-world settings. The dataset, code, and pre-trained models have been released in our Github.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2310.00917.pdf' target='_blank'>https://arxiv.org/pdf/2310.00917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alloy Das, Sanket Biswas, Ayan Banerjee, Josep LladÃ³s, Umapada Pal, Saumik Bhattacharya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00917">Harnessing the Power of Multi-Lingual Datasets for Pre-training: Towards Enhancing Text Spotting Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The adaptation capability to a wide range of domains is crucial for scene text spotting models when deployed to real-world conditions. However, existing state-of-the-art (SOTA) approaches usually incorporate scene text detection and recognition simply by pretraining on natural scene text datasets, which do not directly exploit the intermediate feature representations between multiple domains. Here, we investigate the problem of domain-adaptive scene text spotting, i.e., training a model on multi-domain source data such that it can directly adapt to target domains rather than being specialized for a specific domain or scenario. Further, we investigate a transformer baseline called Swin-TESTR to focus on solving scene-text spotting for both regular and arbitrary-shaped scene text along with an exhaustive evaluation. The results clearly demonstrate the potential of intermediate representations to achieve significant performance on text spotting benchmarks across multiple domains (e.g. language, synth-to-real, and documents). both in terms of accuracy and efficiency.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2308.02905.pdf' target='_blank'>https://arxiv.org/pdf/2308.02905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alloy Das, Sanket Biswas, Prasun Roy, Subhankar Ghosh, Umapada Pal, Michael Blumenstein, Josep LladÃ³s, Saumik Bhattacharya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.02905">FASTER: A Font-Agnostic Scene Text Editing and Rendering Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Text Editing (STE) is a challenging research problem, that primarily aims towards modifying existing texts in an image while preserving the background and the font style of the original text. Despite its utility in numerous real-world applications, existing style-transfer-based approaches have shown sub-par editing performance due to (1) complex image backgrounds, (2) diverse font attributes, and (3) varying word lengths within the text. To address such limitations, in this paper, we propose a novel font-agnostic scene text editing and rendering framework, named FASTER, for simultaneously generating text in arbitrary styles and locations while preserving a natural and realistic appearance and structure. A combined fusion of target mask generation and style transfer units, with a cascaded self-attention mechanism has been proposed to focus on multi-level text region edits to handle varying word lengths. Extensive evaluation on a real-world database with further subjective human evaluation study indicates the superiority of FASTER in both scene text editing and rendering tasks, in terms of model performance and efficiency. Our code will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/1903.01192.pdf' target='_blank'>https://arxiv.org/pdf/1903.01192.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1903.01192">STEFANN: Scene Text Editor using Font Adaptive Neural Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Textual information in a captured scene plays an important role in scene interpretation and decision making. Though there exist methods that can successfully detect and interpret complex text regions present in a scene, to the best of our knowledge, there is no significant prior work that aims to modify the textual information in an image. The ability to edit text directly on images has several advantages including error correction, text restoration and image reusability. In this paper, we propose a method to modify text in an image at character-level. We approach the problem in two stages. At first, the unobserved character (target) is generated from an observed character (source) being modified. We propose two different neural network architectures - (a) FANnet to achieve structural consistency with source font and (b) Colornet to preserve source color. Next, we replace the source character with the generated character maintaining both geometric and visual consistency with neighboring characters. Our method works as a unified platform for modifying text in images. We present the effectiveness of our method on COCO-Text and ICDAR datasets both qualitatively and quantitatively.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2407.19507.pdf' target='_blank'>https://arxiv.org/pdf/2407.19507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjing Wu, Zhengyao Fang, Pengyuan Lyu, Chengquan Zhang, Fanglin Chen, Guangming Lu, Wenjie Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19507">WeCromCL: Weakly Supervised Cross-Modality Contrastive Learning for Transcription-only Supervised Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transcription-only Supervised Text Spotting aims to learn text spotters relying only on transcriptions but no text boundaries for supervision, thus eliminating expensive boundary annotation. The crux of this task lies in locating each transcription in scene text images without location annotations. In this work, we formulate this challenging problem as a Weakly Supervised Cross-modality Contrastive Learning problem, and design a simple yet effective model dubbed WeCromCL that is able to detect each transcription in a scene image in a weakly supervised manner. Unlike typical methods for cross-modality contrastive learning that focus on modeling the holistic semantic correlation between an entire image and a text description, our WeCromCL conducts atomistic contrastive learning to model the character-wise appearance consistency between a text transcription and its correlated region in a scene image to detect an anchor point for the transcription in a weakly supervised manner. The detected anchor points by WeCromCL are further used as pseudo location labels to guide the learning of text spotting. Extensive experiments on four challenging benchmarks demonstrate the superior performance of our model over other methods. Code will be released.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2405.07481.pdf' target='_blank'>https://arxiv.org/pdf/2405.07481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianci Bi, Xiaoyi Zhang, Zhizheng Zhang, Wenxuan Xie, Cuiling Lan, Yan Lu, Nanning Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07481">Text Grouping Adapter: Adapting Pre-trained Text Detector for Layout Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Significant progress has been made in scene text detection models since the rise of deep learning, but scene text layout analysis, which aims to group detected text instances as paragraphs, has not kept pace. Previous works either treated text detection and grouping using separate models, or train a model from scratch while using a unified one. All of them have not yet made full use of the already well-trained text detectors and easily obtainable detection datasets. In this paper, we present Text Grouping Adapter (TGA), a module that can enable the utilization of various pre-trained text detectors to learn layout analysis, allowing us to adopt a well-trained text detector right off the shelf or just fine-tune it efficiently. Designed to be compatible with various text detector architectures, TGA takes detected text regions and image features as universal inputs to assemble text instance features. To capture broader contextual information for layout analysis, we propose to predict text group masks from text instance features by one-to-many assignment. Our comprehensive experiments demonstrate that, even with frozen pre-trained models, incorporating our TGA into various pre-trained text detectors and text spotters can achieve superior layout analysis performance, simultaneously inheriting generalized text detection ability from pre-training. In the case of full parameter fine-tuning, we can further improve layout analysis performance.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2402.17134.pdf' target='_blank'>https://arxiv.org/pdf/2402.17134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nguyen Nguyen, Yapeng Tian, Chenliang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17134">Efficiently Leveraging Linguistic Priors for Scene Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Incorporating linguistic knowledge can improve scene text recognition, but it is questionable whether the same holds for scene text spotting, which typically involves text detection and recognition. This paper proposes a method that leverages linguistic knowledge from a large text corpus to replace the traditional one-hot encoding used in auto-regressive scene text spotting and recognition models. This allows the model to capture the relationship between characters in the same word. Additionally, we introduce a technique to generate text distributions that align well with scene text datasets, removing the need for in-domain fine-tuning. As a result, the newly created text distributions are more informative than pure one-hot encoding, leading to improved spotting and recognition performance. Our method is simple and efficient, and it can easily be integrated into existing auto-regressive-based approaches. Experimental results show that our method not only improves recognition accuracy but also enables more accurate localization of words. It significantly improves both state-of-the-art scene text spotting and recognition pipelines, achieving state-of-the-art results on several benchmarks.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2207.07253.pdf' target='_blank'>https://arxiv.org/pdf/2207.07253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjing Wu, Pengyuan Lyu, Guangming Lu, Chengquan Zhang, Wenjie Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.07253">Single Shot Self-Reliant Scene Text Spotter by Decoupled yet Collaborative Detection and Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Typical text spotters follow the two-stage spotting paradigm which detects the boundary for a text instance first and then performs text recognition within the detected regions. Despite the remarkable progress of such spotting paradigm, an important limitation is that the performance of text recognition depends heavily on the precision of text detection, resulting in the potential error propagation from detection to recognition. In this work, we propose the single shot Self-Reliant Scene Text Spotter v2 (SRSTS v2), which circumvents this limitation by decoupling recognition from detection while optimizing two tasks collaboratively. Specifically, our SRSTS v2 samples representative feature points around each potential text instance, and conducts both text detection and recognition in parallel guided by these sampled points. Thus, the text recognition is no longer dependent on detection, thereby alleviating the error propagation from detection to recognition. Moreover, the sampling module is learned under the supervision from both detection and recognition, which allows for the collaborative optimization and mutual enhancement between two tasks. Benefiting from such sampling-driven concurrent spotting framework, our approach is able to recognize the text instances correctly even if the precise text boundaries are challenging to detect. Extensive experiments on four benchmarks demonstrate that our method compares favorably to state-of-the-art spotters.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2505.23119.pdf' target='_blank'>https://arxiv.org/pdf/2505.23119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keren Ye, Ignacio Garcia Dorado, Michalis Raptis, Mauricio Delbracio, Irene Zhu, Peyman Milanfar, Hossein Talebi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23119">TextSR: Diffusion Super-Resolution with Multilingual OCR Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent advancements in Image Super-Resolution (SR) using diffusion models have shown promise in improving overall image quality, their application to scene text images has revealed limitations. These models often struggle with accurate text region localization and fail to effectively model image and multilingual character-to-shape priors. This leads to inconsistencies, the generation of hallucinated textures, and a decrease in the perceived quality of the super-resolved text.
  To address these issues, we introduce TextSR, a multimodal diffusion model specifically designed for Multilingual Scene Text Image Super-Resolution. TextSR leverages a text detector to pinpoint text regions within an image and then employs Optical Character Recognition (OCR) to extract multilingual text from these areas. The extracted text characters are then transformed into visual shapes using a UTF-8 based text encoder and cross-attention. Recognizing that OCR may sometimes produce inaccurate results in real-world scenarios, we have developed two innovative methods to enhance the robustness of our model. By integrating text character priors with the low-resolution text images, our model effectively guides the super-resolution process, enhancing fine details within the text and improving overall legibility. The superior performance of our model on both the TextZoom and TextVQA datasets sets a new benchmark for STISR, underscoring the efficacy of our approach.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2409.16827.pdf' target='_blank'>https://arxiv.org/pdf/2409.16827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Han, Junyu Gao, Chuang Yang, Yuan Yuan, Qi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16827">Focus Entirety and Perceive Environment for Arbitrary-Shaped Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the diversity of scene text in aspects such as font, color, shape, and size, accurately and efficiently detecting text is still a formidable challenge. Among the various detection approaches, segmentation-based approaches have emerged as prominent contenders owing to their flexible pixel-level predictions. However, these methods typically model text instances in a bottom-up manner, which is highly susceptible to noise. In addition, the prediction of pixels is isolated without introducing pixel-feature interaction, which also influences the detection performance. To alleviate these problems, we propose a multi-information level arbitrary-shaped text detector consisting of a focus entirety module (FEM) and a perceive environment module (PEM). The former extracts instance-level features and adopts a top-down scheme to model texts to reduce the influence of noises. Specifically, it assigns consistent entirety information to pixels within the same instance to improve their cohesion. In addition, it emphasizes the scale information, enabling the model to distinguish varying scale texts effectively. The latter extracts region-level information and encourages the model to focus on the distribution of positive samples in the vicinity of a pixel, which perceives environment information. It treats the kernel pixels as positive samples and helps the model differentiate text and kernel features. Extensive experiments demonstrate the FEM's ability to efficiently support the model in handling different scale texts and confirm the PEM can assist in perceiving pixels more accurately by focusing on pixel vicinities. Comparisons show the proposed model outperforms existing state-of-the-art approaches on four public datasets.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2409.16820.pdf' target='_blank'>https://arxiv.org/pdf/2409.16820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Han, Junyu Gao, Chuang Yang, Yuan Yuan, Qi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16820">Spotlight Text Detector: Spotlight on Candidate Regions Like a Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The irregular contour representation is one of the tough challenges in scene text detection. Although segmentation-based methods have achieved significant progress with the help of flexible pixel prediction, the overlap of geographically close texts hinders detecting them separately. To alleviate this problem, some shrink-based methods predict text kernels and expand them to restructure texts. However, the text kernel is an artificial object with incomplete semantic features that are prone to incorrect or missing detection. In addition, different from the general objects, the geometry features (aspect ratio, scale, and shape) of scene texts vary significantly, which makes it difficult to detect them accurately. To consider the above problems, we propose an effective spotlight text detector (STD), which consists of a spotlight calibration module (SCM) and a multivariate information extraction module (MIEM). The former concentrates efforts on the candidate kernel, like a camera focus on the target. It obtains candidate features through a mapping filter and calibrates them precisely to eliminate some false positive samples. The latter designs different shape schemes to explore multiple geometric features for scene texts. It helps extract various spatial relationships to improve the model's ability to recognize kernel regions. Ablation studies prove the effectiveness of the designed SCM and MIEM. Extensive experiments verify that our STD is superior to existing state-of-the-art methods on various datasets, including ICDAR2015, CTW1500, MSRA-TD500, and Total-Text.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2408.11523.pdf' target='_blank'>https://arxiv.org/pdf/2408.11523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizhong Wan, Bin Yin, Junjie Xie, Fei Jiang, Xiang Li, Wei Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11523">LARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Click-Through Rate (CTR) prediction is crucial for Recommendation System(RS), aiming to provide personalized recommendation services for users in many aspects such as food delivery, e-commerce and so on. However, traditional RS relies on collaborative signals, which lacks semantic understanding to real-time scenes. We also noticed that a major challenge in utilizing Large Language Models (LLMs) for practical recommendation purposes is their efficiency in dealing with long text input. To break through the problems above, we propose Large Language Model Aided Real-time Scene Recommendation(LARR), adopt LLMs for semantic understanding, utilizing real-time scene information in RS without requiring LLM to process the entire real-time scene text directly, thereby enhancing the efficiency of LLM-based CTR modeling. Specifically, recommendation domain-specific knowledge is injected into LLM and then RS employs an aggregation encoder to build real-time scene information from separate LLM's outputs. Firstly, a LLM is continual pretrained on corpus built from recommendation data with the aid of special tokens. Subsequently, the LLM is fine-tuned via contrastive learning on three kinds of sample construction strategies. Through this step, LLM is transformed into a text embedding model. Finally, LLM's separate outputs for different scene features are aggregated by an encoder, aligning to collaborative signals in RS, enhancing the performance of recommendation model.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2408.02036.pdf' target='_blank'>https://arxiv.org/pdf/2408.02036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujin Ren, Jiaxin Zhang, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02036">LEGO: Self-Supervised Representation Learning for Scene Text Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, significant progress has been made in scene text recognition by data-driven methods. However, due to the scarcity of annotated real-world data, the training of these methods predominantly relies on synthetic data. The distribution gap between synthetic and real data constrains the further performance improvement of these methods in real-world applications. To tackle this problem, a highly promising approach is to utilize massive amounts of unlabeled real data for self-supervised training, which has been widely proven effective in many NLP and CV tasks. Nevertheless, generic self-supervised methods are unsuitable for scene text images due to their sequential nature. To address this issue, we propose a Local Explicit and Global Order-aware self-supervised representation learning method (LEGO) that accounts for the characteristics of scene text images. Inspired by the human cognitive process of learning words, which involves spelling, reading, and writing, we propose three novel pre-text tasks for LEGO to model sequential, semantic, and structural features, respectively. The entire pre-training process is optimized by using a consistent Text Knowledge Codebook. Extensive experiments validate that LEGO outperforms previous scene text self-supervised methods. The recognizer incorporated with our pre-trained model achieves superior or comparable performance compared to state-of-the-art scene text recognition methods on six benchmarks. Furthermore, we demonstrate that LEGO can achieve superior performance in other text-related tasks.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2401.03637.pdf' target='_blank'>https://arxiv.org/pdf/2401.03637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shi-Xue Zhang, Chun Yang, Xiaobin Zhu, Hongyang Zhou, Hongfa Wang, Xu-Cheng Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03637">Inverse-like Antagonistic Scene Text Spotting via Reading-Order Estimation and Dynamic Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text spotting is a challenging task, especially for inverse-like scene text, which has complex layouts, e.g., mirrored, symmetrical, or retro-flexed. In this paper, we propose a unified end-to-end trainable inverse-like antagonistic text spotting framework dubbed IATS, which can effectively spot inverse-like scene texts without sacrificing general ones. Specifically, we propose an innovative reading-order estimation module (REM) that extracts reading-order information from the initial text boundary generated by an initial boundary module (IBM). To optimize and train REM, we propose a joint reading-order estimation loss consisting of a classification loss, an orthogonality loss, and a distribution loss. With the help of IBM, we can divide the initial text boundary into two symmetric control points and iteratively refine the new text boundary using a lightweight boundary refinement module (BRM) for adapting to various shapes and scales. To alleviate the incompatibility between text detection and recognition, we propose a dynamic sampling module (DSM) with a thin-plate spline that can dynamically sample appropriate features for recognition in the detected text region. Without extra supervision, the DSM can proactively learn to sample appropriate features for text recognition through the gradient returned by the recognition module. Extensive experiments on both challenging scene text and inverse-like scene text datasets demonstrate that our method achieves superior performance both on irregular and inverse-like text spotting.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2310.00558.pdf' target='_blank'>https://arxiv.org/pdf/2310.00558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alloy Das, Sanket Biswas, Umapada Pal, Josep LladÃ³s
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00558">Diving into the Depths of Spotting Text in Multi-Domain Noisy Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When used in a real-world noisy environment, the capacity to generalize to multiple domains is essential for any autonomous scene text spotting system. However, existing state-of-the-art methods employ pretraining and fine-tuning strategies on natural scene datasets, which do not exploit the feature interaction across other complex domains. In this work, we explore and investigate the problem of domain-agnostic scene text spotting, i.e., training a model on multi-domain source data such that it can directly generalize to target domains rather than being specialized for a specific domain or scenario. In this regard, we present the community a text spotting validation benchmark called Under-Water Text (UWT) for noisy underwater scenes to establish an important case study. Moreover, we also design an efficient super-resolution based end-to-end transformer baseline called DA-TextSpotter which achieves comparable or superior performance over existing text spotting architectures for both regular and arbitrary-shaped scene text spotting benchmarks in terms of both accuracy and model efficiency. The dataset, code and pre-trained models will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2307.08723.pdf' target='_blank'>https://arxiv.org/pdf/2307.08723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qing Jiang, Jiapeng Wang, Dezhi Peng, Chongyu Liu, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.08723">Revisiting Scene Text Recognition: A Data Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to re-assess scene text recognition (STR) from a data-oriented perspective. We begin by revisiting the six commonly used benchmarks in STR and observe a trend of performance saturation, whereby only 2.91% of the benchmark images cannot be accurately recognized by an ensemble of 13 representative models. While these results are impressive and suggest that STR could be considered solved, however, we argue that this is primarily due to the less challenging nature of the common benchmarks, thus concealing the underlying issues that STR faces. To this end, we consolidate a large-scale real STR dataset, namely Union14M, which comprises 4 million labeled images and 10 million unlabeled images, to assess the performance of STR models in more complex real-world scenarios. Our experiments demonstrate that the 13 models can only achieve an average accuracy of 66.53% on the 4 million labeled images, indicating that STR still faces numerous challenges in the real world. By analyzing the error patterns of the 13 models, we identify seven open challenges in STR and develop a challenge-driven benchmark consisting of eight distinct subsets to facilitate further progress in the field. Our exploration demonstrates that STR is far from being solved and leveraging data may be a promising solution. In this regard, we find that utilizing the 10 million unlabeled images through self-supervised pre-training can significantly improve the robustness of STR model in real-world scenarios and leads to state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2305.11373.pdf' target='_blank'>https://arxiv.org/pdf/2305.11373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shohei Uchigasaki, Tomo Miyazaki, Shinichiro Omachi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11373">Deep Image Compression Using Scene Text Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image compression is a fundamental technology for Internet communication engineering. However, a high compression rate with general methods may degrade images, resulting in unreadable texts. In this paper, we propose an image compression method for maintaining text quality. We developed a scene text image quality assessment model to assess text quality in compressed images. The assessment model iteratively searches for the best-compressed image holding high-quality text. Objective and subjective results showed that the proposed method was superior to existing methods. Furthermore, the proposed assessment model outperformed other deep-learning regression models.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2209.02397.pdf' target='_blank'>https://arxiv.org/pdf/2209.02397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengmi Tang, Tomo Miyazaki, Shinichiro Omachi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.02397">A Scene-Text Synthesis Engine Achieved Through Learning from Decomposed Real-World Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene-text image synthesis techniques that aim to naturally compose text instances on background scene images are very appealing for training deep neural networks due to their ability to provide accurate and comprehensive annotation information. Prior studies have explored generating synthetic text images on two-dimensional and three-dimensional surfaces using rules derived from real-world observations. Some of these studies have proposed generating scene-text images through learning; however, owing to the absence of a suitable training dataset, unsupervised frameworks have been explored to learn from existing real-world data, which might not yield reliable performance. To ease this dilemma and facilitate research on learning-based scene text synthesis, we introduce DecompST, a real-world dataset prepared from some public benchmarks, containing three types of annotations: quadrilateral-level BBoxes, stroke-level text masks, and text-erased images. Leveraging the DecompST dataset, we propose a Learning-Based Text Synthesis engine (LBTS) that includes a text location proposal network (TLPNet) and a text appearance adaptation network (TAANet). TLPNet first predicts the suitable regions for text embedding, after which TAANet adaptively adjusts the geometry and color of the text instance to match the background context. After training, those networks can be integrated and utilized to generate the synthetic dataset for scene text analysis tasks. Comprehensive experiments were conducted to validate the effectiveness of the proposed LBTS along with existing methods, and the experimental results indicate the proposed LBTS can generate better pretraining data for scene text detectors.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2310.13366.pdf' target='_blank'>https://arxiv.org/pdf/2310.13366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Liawi, Yun-Da Tsai, Guan-Lun Lu, Shou-De Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13366">PSGText: Stroke-Guided Scene Text Editing with PSP Module</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Text Editing (STE) aims to substitute text in an image with new desired text while preserving the background and styles of the original text. However, present techniques present a notable challenge in the generation of edited text images that exhibit a high degree of clarity and legibility. This challenge primarily stems from the inherent diversity found within various text types and the intricate textures of complex backgrounds. To address this challenge, this paper introduces a three-stage framework for transferring texts across text images. Initially, we introduce a text-swapping network that seamlessly substitutes the original text with the desired replacement. Subsequently, we incorporate a background inpainting network into our framework. This specialized network is designed to skillfully reconstruct background images, effectively addressing the voids left after the removal of the original text. This process meticulously preserves visual harmony and coherence in the background. Ultimately, the synthesis of outcomes from the text-swapping network and the background inpainting network is achieved through a fusion network, culminating in the creation of the meticulously edited final image. A demo video is included in the supplementary material.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2303.15737.pdf' target='_blank'>https://arxiv.org/pdf/2303.15737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao He, Sheng Huang, Wenhao Tang, Bo Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15737">Deformable Kernel Expansion Model for Efficient Arbitrary-shaped Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text detection is a challenging computer vision task due to the high variation in text shapes and ratios. In this work, we propose a scene text detector named Deformable Kernel Expansion (DKE), which incorporates the merits of both segmentation and contour-based detectors. DKE employs a segmentation module to segment the shrunken text region as the text kernel, then expands the text kernel contour to obtain text boundary by regressing the vertex-wise offsets. Generating the text kernel by segmentation enables DKE to inherit the arbitrary-shaped text region modeling capability of segmentation-based detectors. Regressing the kernel contour with some sampled vertices enables DKE to avoid the complicated pixel-level post-processing and better learn contour deformation as the contour-based detectors. Moreover, we propose an Optimal Bipartite Graph Matching Loss (OBGML) that measures the matching error between the predicted contour and the ground truth, which efficiently minimizes the global contour matching distance. Extensive experiments on CTW1500, Total-Text, MSRA-TD500, and ICDAR2015 demonstrate that DKE achieves a good tradeoff between accuracy and efficiency in scene text detection.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2510.21590.pdf' target='_blank'>https://arxiv.org/pdf/2510.21590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minxing Luo, Linlong Fan, Wang Qiushi, Ge Wu, Yiyan Luo, Yuhang Yu, Jinwei Chen, Yaxing Wang, Qingnan Fan, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21590">Restore Text First, Enhance Image Later: Two-Stage Scene Text Image Super-Resolution with Glyph Structure Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current generative super-resolution methods show strong performance on natural images but distort text, creating a fundamental trade-off between image quality and textual readability. To address this, we introduce \textbf{TIGER} (\textbf{T}ext-\textbf{I}mage \textbf{G}uided sup\textbf{E}r-\textbf{R}esolution), a novel two-stage framework that breaks this trade-off through a \textit{"text-first, image-later"} paradigm. \textbf{TIGER} explicitly decouples glyph restoration from image enhancement: it first reconstructs precise text structures and then uses them to guide subsequent full-image super-resolution. This glyph-to-image guidance ensures both high fidelity and visual consistency. To support comprehensive training and evaluation, we also contribute the \textbf{UltraZoom-ST} (UltraZoom-Scene Text), the first scene text dataset with extreme zoom (\textbf{$\times$14.29}). Extensive experiments show that \textbf{TIGER} achieves \textbf{state-of-the-art} performance, enhancing readability while preserving overall image quality.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2505.04915.pdf' target='_blank'>https://arxiv.org/pdf/2505.04915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Wang, Ting Liu, Xiaochao Qu, Chengjing Wu, Luoqi Liu, Xiaolin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04915">GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text editing, a subfield of image editing, requires modifying texts in images while preserving style consistency and visual coherence with the surrounding environment. While diffusion-based methods have shown promise in text generation, they still struggle to produce high-quality results. These methods often generate distorted or unrecognizable characters, particularly when dealing with complex characters like Chinese. In such systems, characters are composed of intricate stroke patterns and spatial relationships that must be precisely maintained. We present GlyphMastero, a specialized glyph encoder designed to guide the latent diffusion model for generating texts with stroke-level precision. Our key insight is that existing methods, despite using pretrained OCR models for feature extraction, fail to capture the hierarchical nature of text structures - from individual strokes to stroke-level interactions to overall character-level structure. To address this, our glyph encoder explicitly models and captures the cross-level interactions between local-level individual characters and global-level text lines through our novel glyph attention module. Meanwhile, our model implements a feature pyramid network to fuse the multi-scale OCR backbone features at the global-level. Through these cross-level and multi-scale fusions, we obtain more detailed glyph-aware guidance, enabling precise control over the scene text generation process. Our method achieves an 18.02\% improvement in sentence accuracy over the state-of-the-art multi-lingual scene text editing baseline, while simultaneously reducing the text-region FrÃ©chet inception distance by 53.28\%.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2406.01062.pdf' target='_blank'>https://arxiv.org/pdf/2406.01062.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qilong Zhangli, Jindong Jiang, Di Liu, Licheng Yu, Xiaoliang Dai, Ankit Ramchandani, Guan Pang, Dimitris N. Metaxas, Praveen Krishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01062">Layout Agnostic Scene Text Image Synthesis with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While diffusion models have significantly advanced the quality of image generation their capability to accurately and coherently render text within these images remains a substantial challenge. Conventional diffusion-based methods for scene text generation are typically limited by their reliance on an intermediate layout output. This dependency often results in a constrained diversity of text styles and fonts an inherent limitation stemming from the deterministic nature of the layout generation phase. To address these challenges this paper introduces SceneTextGen a novel diffusion-based model specifically designed to circumvent the need for a predefined layout stage. By doing so SceneTextGen facilitates a more natural and varied representation of text. The novelty of SceneTextGen lies in its integration of three key components: a character-level encoder for capturing detailed typographic properties coupled with a character-level instance segmentation model and a word-level spotting model to address the issues of unwanted text generation and minor character inaccuracies. We validate the performance of our method by demonstrating improved character recognition rates on generated images across different public visual text datasets in comparison to both standard diffusion based methods and text specific methods.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2510.01651.pdf' target='_blank'>https://arxiv.org/pdf/2510.01651.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rixin Zhou, Peiqiang Qiu, Qian Zhang, Chuntao Li, Xi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01651">LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial stage of early Chinese writing and provide indispensable evidence for archaeological and historical studies. However, automatic BI recognition remains difficult due to severe visual degradation, multi-domain variability across photographs, rubbings, and tracings, and an extremely long-tailed character distribution. To address these challenges, we curate a large-scale BI dataset comprising 22454 full-page images and 198598 annotated characters spanning 6658 unique categories, enabling robust cross-domain evaluation. Building on this resource, we develop a two-stage detection-recognition pipeline that first localizes inscriptions and then transcribes individual characters. To handle heterogeneous domains and rare classes, we equip the pipeline with LadderMoE, which augments a pretrained CLIP encoder with ladder-style MoE adapters, enabling dynamic expert specialization and stronger robustness. Comprehensive experiments on single-character and full-page recognition tasks demonstrate that our method substantially outperforms state-of-the-art scene text recognition baselines, achieving superior accuracy across head, mid, and tail categories as well as all acquisition modalities. These results establish a strong foundation for bronze inscription recognition and downstream archaeological analysis.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2509.12543.pdf' target='_blank'>https://arxiv.org/pdf/2509.12543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harshit Rajgarhia, Shivali Dalmia, Mengyang Zhao, Mukherji Abhishek, Kiran Ganesh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12543">Human + AI for Accelerating Ad Localization Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adapting advertisements for multilingual audiences requires more than simple text translation; it demands preservation of visual consistency, spatial alignment, and stylistic integrity across diverse languages and formats. We introduce a structured framework that combines automated components with human oversight to address the complexities of advertisement localization. To the best of our knowledge, this is the first work to integrate scene text detection, inpainting, machine translation (MT), and text reimposition specifically for accelerating ad localization evaluation workflows. Qualitative results across six locales demonstrate that our approach produces semantically accurate and visually coherent localized advertisements, suitable for deployment in real-world workflows.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2403.09622.pdf' target='_blank'>https://arxiv.org/pdf/2403.09622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Liu, Weicong Liang, Zhanhao Liang, Chong Luo, Ji Li, Gao Huang, Yuhui Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09622">Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual text rendering poses a fundamental challenge for contemporary text-to-image generation models, with the core problem lying in text encoder deficiencies. To achieve accurate text rendering, we identify two crucial requirements for text encoders: character awareness and alignment with glyphs. Our solution involves crafting a series of customized text encoder, Glyph-ByT5, by fine-tuning the character-aware ByT5 encoder using a meticulously curated paired glyph-text dataset. We present an effective method for integrating Glyph-ByT5 with SDXL, resulting in the creation of the Glyph-SDXL model for design image generation. This significantly enhances text rendering accuracy, improving it from less than $20\%$ to nearly $90\%$ on our design image benchmark. Noteworthy is Glyph-SDXL's newfound ability for text paragraph rendering, achieving high spelling accuracy for tens to hundreds of characters with automated multi-line layouts. Finally, through fine-tuning Glyph-SDXL with a small set of high-quality, photorealistic images featuring visual text, we showcase a substantial improvement in scene text rendering capabilities in open-domain real images. These compelling outcomes aim to encourage further exploration in designing customized text encoders for diverse and challenging tasks.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2304.08592.pdf' target='_blank'>https://arxiv.org/pdf/2304.08592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunghyun Park, Sunghyo Chung, Jungsoo Lee, Jaegul Choo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08592">Improving Scene Text Recognition for Character-Level Long-Tailed Distribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the recent remarkable improvements in scene text recognition (STR), the majority of the studies focused mainly on the English language, which only includes few number of characters. However, STR models show a large performance degradation on languages with a numerous number of characters (e.g., Chinese and Korean), especially on characters that rarely appear due to the long-tailed distribution of characters in such languages. To address such an issue, we conducted an empirical analysis using synthetic datasets with different character-level distributions (e.g., balanced and long-tailed distributions). While increasing a substantial number of tail classes without considering the context helps the model to correctly recognize characters individually, training with such a synthetic dataset interferes the model with learning the contextual information (i.e., relation among characters), which is also important for predicting the whole word. Based on this motivation, we propose a novel Context-Aware and Free Experts Network (CAFE-Net) using two experts: 1) context-aware expert learns the contextual representation trained with a long-tailed dataset composed of common words used in everyday life and 2) context-free expert focuses on correctly predicting individual characters by utilizing a dataset with a balanced number of characters. By training two experts to focus on learning contextual and visual representations, respectively, we propose a novel confidence ensemble method to compensate the limitation of each expert. Through the experiments, we demonstrate that CAFE-Net improves the STR performance on languages containing numerous number of characters. Moreover, we show that CAFE-Net is easily applicable to various STR models.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2510.03200.pdf' target='_blank'>https://arxiv.org/pdf/2510.03200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Collorone, Matteo Gioia, Massimiliano Pappa, Paolo Leoni, Giovanni Ficarra, Or Litany, Indro Spinelli, Fabio Galasso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03200">MonSTeR: a Unified Model for Motion, Scene, Text Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intention drives human movement in complex environments, but such movement can only happen if the surrounding context supports it. Despite the intuitive nature of this mechanism, existing research has not yet provided tools to evaluate the alignment between skeletal movement (motion), intention (text), and the surrounding context (scene). In this work, we introduce MonSTeR, the first MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of higher-order relations, MonSTeR constructs a unified latent space by leveraging unimodal and cross-modal representations. This allows MonSTeR to capture the intricate dependencies between modalities, enabling flexible but robust retrieval across various tasks. Our results show that MonSTeR outperforms trimodal models that rely solely on unimodal representations. Furthermore, we validate the alignment of our retrieval scores with human preferences through a dedicated user study. We demonstrate the versatility of MonSTeR's latent space on zero-shot in-Scene Object Placement and Motion Captioning. Code and pre-trained models are available at github.com/colloroneluca/MonSTeR.
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2506.21276.pdf' target='_blank'>https://arxiv.org/pdf/2506.21276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenda Shi, Yiren Song, Zihan Rao, Dengming Zhang, Jiaming Liu, Xingxing Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21276">WordCon: Word-level Typography Control in Scene Text Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving precise word-level typography control within generated images remains a persistent challenge. To address it, we newly construct a word-level controlled scene text dataset and introduce the Text-Image Alignment (TIA) framework. This framework leverages cross-modal correspondence between text and local image regions provided by grounding models to enhance the Text-to-Image (T2I) model training. Furthermore, we propose WordCon, a hybrid parameter-efficient fine-tuning (PEFT) method. WordCon reparameterizes selective key parameters, improving both efficiency and portability. This allows seamless integration into diverse pipelines, including artistic text rendering, text editing, and image-conditioned text rendering. To further enhance controllability, the masked loss at the latent level is applied to guide the model to concentrate on learning the text region in the image, and the joint-attention loss provides feature-level supervision to promote disentanglement between different words. Both qualitative and quantitative results demonstrate the superiority of our method to the state of the art. The datasets and source code will be available for academic use.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2504.11164.pdf' target='_blank'>https://arxiv.org/pdf/2504.11164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenming Li, Chengxu Liu, Yuanting Fan, Xiao Jin, Xingsong Hou, Xueming Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11164">TSAL: Few-shot Text Segmentation Based on Attribute Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently supervised learning rapidly develops in scene text segmentation. However, the lack of high-quality datasets and the high cost of pixel annotation greatly limit the development of them. Considering the well-performed few-shot learning methods for downstream tasks, we investigate the application of the few-shot learning method to scene text segmentation. We propose TSAL, which leverages CLIP's prior knowledge to learn text attributes for segmentation. To fully utilize the semantic and texture information in the image, a visual-guided branch is proposed to separately extract text and background features. To reduce data dependency and improve text detection accuracy, the adaptive prompt-guided branch employs effective adaptive prompt templates to capture various text attributes. To enable adaptive prompts capture distinctive text features and complex background distribution, we propose Adaptive Feature Alignment module(AFA). By aligning learnable tokens of different attributes with visual features and prompt prototypes, AFA enables adaptive prompts to capture both general and distinctive attribute information. TSAL can capture the unique attributes of text and achieve precise segmentation using only few images. Experiments demonstrate that our method achieves SOTA performance on multiple text segmentation datasets under few-shot settings and show great potential in text-related domains.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2412.12502.pdf' target='_blank'>https://arxiv.org/pdf/2412.12502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Zhang, Gangyan Zeng, Huawen Shen, Daiqing Wu, Yu Zhou, Can Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12502">Track the Answer: Extending TextVQA from Image to Video with Spatio-Temporal Clues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video text-based visual question answering (Video TextVQA) is a practical task that aims to answer questions by jointly reasoning textual and visual information in a given video. Inspired by the development of TextVQA in image domain, existing Video TextVQA approaches leverage a language model (e.g. T5) to process text-rich multiple frames and generate answers auto-regressively. Nevertheless, the spatio-temporal relationships among visual entities (including scene text and objects) will be disrupted and models are susceptible to interference from unrelated information, resulting in irrational reasoning and inaccurate answering. To tackle these challenges, we propose the TEA (stands for ``\textbf{T}rack th\textbf{E} \textbf{A}nswer'') method that better extends the generative TextVQA framework from image to video. TEA recovers the spatio-temporal relationships in a complementary way and incorporates OCR-aware clues to enhance the quality of reasoning questions. Extensive experiments on several public Video TextVQA datasets validate the effectiveness and generalization of our framework. TEA outperforms existing TextVQA methods, video-language pretraining methods and video large language models by great margins.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2405.04377.pdf' target='_blank'>https://arxiv.org/pdf/2405.04377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boqiang Zhang, Hongtao Xie, Zuan Gao, Yuxin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04377">Choose What You Need: Disentangled Representation Learning for Scene Text Recognition, Removal and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text images contain not only style information (font, background) but also content information (character, texture). Different scene text tasks need different information, but previous representation learning methods use tightly coupled features for all tasks, resulting in sub-optimal performance. We propose a Disentangled Representation Learning framework (DARLING) aimed at disentangling these two types of features for improved adaptability in better addressing various downstream tasks (choose what you really need). Specifically, we synthesize a dataset of image pairs with identical style but different content. Based on the dataset, we decouple the two types of features by the supervision design. Clearly, we directly split the visual representation into style and content features, the content features are supervised by a text recognition loss, while an alignment loss aligns the style features in the image pairs. Then, style features are employed in reconstructing the counterpart image via an image decoder with a prompt that indicates the counterpart's content. Such an operation effectively decouples the features based on their distinctive properties. To the best of our knowledge, this is the first time in the field of scene text that disentangles the inherent properties of the text images. Our method achieves state-of-the-art performance in Scene Text Recognition, Removal, and Editing.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2403.10047.pdf' target='_blank'>https://arxiv.org/pdf/2403.10047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Lyu, Jin Wei, Gangyan Zeng, Zeng Li, Enze Xie, Wei Wang, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10047">TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing scene text spotters are designed to locate and transcribe texts from images. However, it is challenging for a spotter to achieve precise detection and recognition of scene texts simultaneously. Inspired by the glimpse-focus spotting pipeline of human beings and impressive performances of Pre-trained Language Models (PLMs) on visual tasks, we ask: 1) "Can machines spot texts without precise detection just like human beings?", and if yes, 2) "Is text block another alternative for scene text spotting other than word or character?" To this end, our proposed scene text spotter leverages advanced PLMs to enhance performance without fine-grained detection. Specifically, we first use a simple detector for block-level text detection to obtain rough positional information. Then, we finetune a PLM using a large-scale OCR dataset to achieve accurate recognition. Benefiting from the comprehensive language knowledge gained during the pre-training phase, the PLM-based recognition module effectively handles complex scenarios, including multi-line, reversed, occluded, and incomplete-detection texts. Taking advantage of the fine-tuned language model on scene recognition benchmarks and the paradigm of text block detection, extensive experiments demonstrate the superior performance of our scene text spotter across multiple public benchmarks. Additionally, we attempt to spot texts directly from an entire scene image to demonstrate the potential of PLMs, even Large Language Models (LLMs).
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2309.15954.pdf' target='_blank'>https://arxiv.org/pdf/2309.15954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haichao Yu, Yu Tian, Sateesh Kumar, Linjie Yang, Heng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15954">The Devil is in the Details: A Deep Dive into the Rabbit Hole of Data Filtering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The quality of pre-training data plays a critical role in the performance of foundation models. Popular foundation models often design their own recipe for data filtering, which makes it hard to analyze and compare different data filtering approaches. DataComp is a new benchmark dedicated to evaluating different methods for data filtering. This paper describes our learning and solution when participating in the DataComp challenge. Our filtering strategy includes three stages: single-modality filtering, cross-modality filtering, and data distribution alignment. We integrate existing methods and propose new solutions, such as computing CLIP score on horizontally flipped images to mitigate the interference of scene text, using vision and language models to retrieve training samples for target downstream tasks, rebalancing the data distribution to improve the efficiency of allocating the computational budget, etc. We slice and dice our design choices, provide in-depth analysis, and discuss open questions. Our approach outperforms the best method from the DataComp paper by over 4% on the average performance of 38 tasks and by over 2% on ImageNet.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2304.01603.pdf' target='_blank'>https://arxiv.org/pdf/2304.01603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongxin Zhu, Zhen Liu, Yukang Liang, Xin Li, Hao Liu, Changcun Bao, Linli Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01603">Locate Then Generate: Bridging Vision and Language with Bounding Box for Scene-Text VQA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel multi-modal framework for Scene Text Visual Question Answering (STVQA), which requires models to read scene text in images for question answering. Apart from text or visual objects, which could exist independently, scene text naturally links text and visual modalities together by conveying linguistic semantics while being a visual object in an image simultaneously. Different to conventional STVQA models which take the linguistic semantics and visual semantics in scene text as two separate features, in this paper, we propose a paradigm of "Locate Then Generate" (LTG), which explicitly unifies this two semantics with the spatial bounding box as a bridge connecting them. Specifically, at first, LTG locates the region in an image that may contain the answer words with an answer location module (ALM) consisting of a region proposal network and a language refinement network, both of which can transform to each other with one-to-one mapping via the scene text bounding box. Next, given the answer words selected by ALM, LTG generates a readable answer sequence with an answer generation module (AGM) based on a pre-trained language model. As a benefit of the explicit alignment of the visual and linguistic semantics, even without any scene text based pre-training tasks, LTG can boost the absolute accuracy by +6.06% and +6.92% on the TextVQA dataset and the ST-VQA dataset respectively, compared with a non-pre-training baseline. We further demonstrate that LTG effectively unifies visual and text modalities through the spatial bounding box connection, which is underappreciated in previous methods.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2506.21002.pdf' target='_blank'>https://arxiv.org/pdf/2506.21002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takumi Yoshimatsu, Shumpei Takezaki, Seiichi Uchida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21002">Inverse Scene Text Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text removal (STR) aims to erase textual elements from images. It was originally intended for removing privacy-sensitiveor undesired texts from natural scene images, but is now also appliedto typographic images. STR typically detects text regions and theninpaints them. Although STR has advanced through neural networksand synthetic data, misuse risks have increased. This paper investi-gates Inverse STR (ISTR), which analyzes STR-processed images andfocuses on binary classification (detecting whether an image has un-dergone STR) and localizing removed text regions. We demonstrate inexperiments that these tasks are achievable with high accuracies, en-abling detection of potential misuse and improving STR. We also at-tempt to recover the removed text content by training a text recognizerto understand its difficulty.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2412.19917.pdf' target='_blank'>https://arxiv.org/pdf/2412.19917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enze Xie, Jiaho Lyu, Daiqing Wu, Huawen Shen, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19917">Char-SAM: Turning Segment Anything Model into Scene Text Segmentation Annotator with Character-level Visual Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent emergence of the Segment Anything Model (SAM) enables various domain-specific segmentation tasks to be tackled cost-effectively by using bounding boxes as prompts. However, in scene text segmentation, SAM can not achieve desirable performance. The word-level bounding box as prompts is too coarse for characters, while the character-level bounding box as prompts suffers from over-segmentation and under-segmentation issues. In this paper, we propose an automatic annotation pipeline named Char-SAM, that turns SAM into a low-cost segmentation annotator with a Character-level visual prompt. Specifically, leveraging some existing text detection datasets with word-level bounding box annotations, we first generate finer-grained character-level bounding box prompts using the Character Bounding-box Refinement CBR module. Next, we employ glyph information corresponding to text character categories as a new prompt in the Character Glyph Refinement (CGR) module to guide SAM in producing more accurate segmentation masks, addressing issues of over-segmentation and under-segmentation. These modules fully utilize the bbox-to-mask capability of SAM to generate high-quality text segmentation annotations automatically. Extensive experiments on TextSeg validate the effectiveness of Char-SAM. Its training-free nature also enables the generation of high-quality scene text segmentation datasets from real-world datasets like COCO-Text and MLT17.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2408.10623.pdf' target='_blank'>https://arxiv.org/pdf/2408.10623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Wang, Xiaochao Qu, Ting Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10623">TextMastero: Mastering High-Quality Scene Text Editing in Diverse Languages and Styles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text editing aims to modify texts on images while maintaining the style of newly generated text similar to the original. Given an image, a target area, and target text, the task produces an output image with the target text in the selected area, replacing the original. This task has been studied extensively, with initial success using Generative Adversarial Networks (GANs) to balance text fidelity and style similarity. However, GAN-based methods struggled with complex backgrounds or text styles. Recent works leverage diffusion models, showing improved results, yet still face challenges, especially with non-Latin languages like CJK characters (Chinese, Japanese, Korean) that have complex glyphs, often producing inaccurate or unrecognizable characters. To address these issues, we present \emph{TextMastero} - a carefully designed multilingual scene text editing architecture based on latent diffusion models (LDMs). TextMastero introduces two key modules: a glyph conditioning module for fine-grained content control in generating accurate texts, and a latent guidance module for providing comprehensive style information to ensure similarity before and after editing. Both qualitative and quantitative experiments demonstrate that our method surpasses all known existing works in text fidelity and style similarity.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2402.15806.pdf' target='_blank'>https://arxiv.org/pdf/2402.15806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingkun Yang, Biao Yang, Minghui Liao, Yingying Zhu, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15806">Sequential Visual and Semantic Consistency for Semi-supervised Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition (STR) is a challenging task that requires large-scale annotated data for training. However, collecting and labeling real text images is expensive and time-consuming, which limits the availability of real data. Therefore, most existing STR methods resort to synthetic data, which may introduce domain discrepancy and degrade the performance of STR models. To alleviate this problem, recent semi-supervised STR methods exploit unlabeled real data by enforcing character-level consistency regularization between weakly and strongly augmented views of the same image. However, these methods neglect word-level consistency, which is crucial for sequence recognition tasks. This paper proposes a novel semi-supervised learning method for STR that incorporates word-level consistency regularization from both visual and semantic aspects. Specifically, we devise a shortest path alignment module to align the sequential visual features of different views and minimize their distance. Moreover, we adopt a reinforcement learning framework to optimize the semantic similarity of the predicted strings in the embedding space. We conduct extensive experiments on several standard and challenging STR benchmarks and demonstrate the superiority of our proposed method over existing semi-supervised STR methods.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2401.10041.pdf' target='_blank'>https://arxiv.org/pdf/2401.10041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinzhi Zheng, Ruyi Ji, Libo Zhang, Yanjun Wu, Chen Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10041">CMFN: Cross-Modal Fusion Network for Irregular Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition, as a cross-modal task involving vision and text, is an important research topic in computer vision. Most existing methods use language models to extract semantic information for optimizing visual recognition. However, the guidance of visual cues is ignored in the process of semantic mining, which limits the performance of the algorithm in recognizing irregular scene text. To tackle this issue, we propose a novel cross-modal fusion network (CMFN) for irregular scene text recognition, which incorporates visual cues into the semantic mining process. Specifically, CMFN consists of a position self-enhanced encoder, a visual recognition branch and an iterative semantic recognition branch. The position self-enhanced encoder provides character sequence position encoding for both the visual recognition branch and the iterative semantic recognition branch. The visual recognition branch carries out visual recognition based on the visual features extracted by CNN and the position encoding information provided by the position self-enhanced encoder. The iterative semantic recognition branch, which consists of a language recognition module and a cross-modal fusion gate, simulates the way that human recognizes scene text and integrates cross-modal visual cues for text recognition. The experiments demonstrate that the proposed CMFN algorithm achieves comparable performance to state-of-the-art algorithms, indicating its effectiveness.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2401.10017.pdf' target='_blank'>https://arxiv.org/pdf/2401.10017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinzhi Zheng, Libo Zhang, Yanjun Wu, Chen Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10017">Text Region Multiple Information Perception Network for Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmentation-based scene text detection algorithms can handle arbitrary shape scene texts and have strong robustness and adaptability, so it has attracted wide attention. Existing segmentation-based scene text detection algorithms usually only segment the pixels in the center region of the text, while ignoring other information of the text region, such as edge information, distance information, etc., thus limiting the detection accuracy of the algorithm for scene text. This paper proposes a plug-and-play module called the Region Multiple Information Perception Module (RMIPM) to enhance the detection performance of segmentation-based algorithms. Specifically, we design an improved module that can perceive various types of information about scene text regions, such as text foreground classification maps, distance maps, direction maps, etc. Experiments on MSRA-TD500 and TotalText datasets show that our method achieves comparable performance with current state-of-the-art algorithms.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2401.09997.pdf' target='_blank'>https://arxiv.org/pdf/2401.09997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinzhi Zheng, Libo Zhang, Yanjun Wu, Chen Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09997">BPDO:Boundary Points Dynamic Optimization for Arbitrary Shape Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Arbitrary shape scene text detection is of great importance in scene understanding tasks. Due to the complexity and diversity of text in natural scenes, existing scene text algorithms have limited accuracy for detecting arbitrary shape text. In this paper, we propose a novel arbitrary shape scene text detector through boundary points dynamic optimization(BPDO). The proposed model is designed with a text aware module (TAM) and a boundary point dynamic optimization module (DOM). Specifically, the model designs a text aware module based on segmentation to obtain boundary points describing the central region of the text by extracting a priori information about the text region. Then, based on the idea of deformable attention, it proposes a dynamic optimization model for boundary points, which gradually optimizes the exact position of the boundary points based on the information of the adjacent region of each boundary point. Experiments on CTW-1500, Total-Text, and MSRA-TD500 datasets show that the model proposed in this paper achieves a performance that is better than or comparable to the state-of-the-art algorithm, proving the effectiveness of the model.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2409.11656.pdf' target='_blank'>https://arxiv.org/pdf/2409.11656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Humen Zhong, Zhibo Yang, Zhaohai Li, Peng Wang, Jun Tang, Wenqing Cheng, Cong Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11656">VL-Reader: Vision and Language Reconstructor is an Effective Scene Text Recognizer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text recognition is an inherent integration of vision and language, encompassing the visual texture in stroke patterns and the semantic context among the character sequences. Towards advanced text recognition, there are three key challenges: (1) an encoder capable of representing the visual and semantic distributions; (2) a decoder that ensures the alignment between vision and semantics; and (3) consistency in the framework during pre-training, if it exists, and fine-tuning. Inspired by masked autoencoding, a successful pre-training strategy in both vision and language, we propose an innovative scene text recognition approach, named VL-Reader. The novelty of the VL-Reader lies in the pervasive interplay between vision and language throughout the entire process. Concretely, we first introduce a Masked Visual-Linguistic Reconstruction (MVLR) objective, which aims at simultaneously modeling visual and linguistic information. Then, we design a Masked Visual-Linguistic Decoder (MVLD) to further leverage masked vision-language context and achieve bi-modal feature interaction. The architecture of VL-Reader maintains consistency from pre-training to fine-tuning. In the pre-training stage, VL-Reader reconstructs both masked visual and text tokens, while in the fine-tuning stage, the network degrades to reconstruct all characters from an image without any masked regions. VL-reader achieves an average accuracy of 97.1% on six typical datasets, surpassing the SOTA by 1.1%. The improvement was even more significant on challenging datasets. The results demonstrate that vision and language reconstructor can serve as an effective scene text recognizer.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2403.07286.pdf' target='_blank'>https://arxiv.org/pdf/2403.07286.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsin-Ju Lin, Tsu-Chun Chung, Ching-Chun Hsiao, Pin-Yu Chen, Wei-Chen Chiu, Ching-Chun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07286">MENTOR: Multilingual tExt detectioN TOward leaRning by analogy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text detection is frequently used in vision-based mobile robots when they need to interpret texts in their surroundings to perform a given task. For instance, delivery robots in multilingual cities need to be capable of doing multilingual text detection so that the robots can read traffic signs and road markings. Moreover, the target languages change from region to region, implying the need of efficiently re-training the models to recognize the novel/new languages. However, collecting and labeling training data for novel languages are cumbersome, and the efforts to re-train an existing/trained text detector are considerable. Even worse, such a routine would repeat whenever a novel language appears. This motivates us to propose a new problem setting for tackling the aforementioned challenges in a more efficient way: "We ask for a generalizable multilingual text detection framework to detect and identify both seen and unseen language regions inside scene images without the requirement of collecting supervised training data for unseen languages as well as model re-training". To this end, we propose "MENTOR", the first work to realize a learning strategy between zero-shot learning and few-shot learning for multilingual scene text detection.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2308.12817.pdf' target='_blank'>https://arxiv.org/pdf/2308.12817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu-Xiang Zeng, Jun-Wei Hsieh, Xin Li, Ming-Ching Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12817">MixNet: Toward Accurate Detection of Challenging Scene Text in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting small scene text instances in the wild is particularly challenging, where the influence of irregular positions and nonideal lighting often leads to detection errors. We present MixNet, a hybrid architecture that combines the strengths of CNNs and Transformers, capable of accurately detecting small text from challenging natural scenes, regardless of the orientations, styles, and lighting conditions. MixNet incorporates two key modules: (1) the Feature Shuffle Network (FSNet) to serve as the backbone and (2) the Central Transformer Block (CTBlock) to exploit the 1D manifold constraint of the scene text. We first introduce a novel feature shuffling strategy in FSNet to facilitate the exchange of features across multiple scales, generating high-resolution features superior to popular ResNet and HRNet. The FSNet backbone has achieved significant improvements over many existing text detection methods, including PAN, DB, and FAST. Then we design a complementary CTBlock to leverage center line based features similar to the medial axis of text regions and show that it can outperform contour-based approaches in challenging cases when small scene texts appear closely. Extensive experimental results show that MixNet, which mixes FSNet with CTBlock, achieves state-of-the-art results on multiple scene text detection datasets.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2308.12774.pdf' target='_blank'>https://arxiv.org/pdf/2308.12774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changxu Cheng, Peng Wang, Cheng Da, Qi Zheng, Cong Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12774">LISTER: Neighbor Decoding for Length-Insensitive Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The diversity in length constitutes a significant characteristic of text. Due to the long-tail distribution of text lengths, most existing methods for scene text recognition (STR) only work well on short or seen-length text, lacking the capability of recognizing longer text or performing length extrapolation. This is a crucial issue, since the lengths of the text to be recognized are usually not given in advance in real-world applications, but it has not been adequately investigated in previous works. Therefore, we propose in this paper a method called Length-Insensitive Scene TExt Recognizer (LISTER), which remedies the limitation regarding the robustness to various text lengths. Specifically, a Neighbor Decoder is proposed to obtain accurate character attention maps with the assistance of a novel neighbor matrix regardless of the text lengths. Besides, a Feature Enhancement Module is devised to model the long-range dependency with low computation cost, which is able to perform iterations with the neighbor decoder to enhance the feature map progressively. To the best of our knowledge, we are the first to achieve effective length-insensitive scene text recognition. Extensive experiments demonstrate that the proposed LISTER algorithm exhibits obvious superiority on long text recognition and the ability for length extrapolation, while comparing favourably with the previous state-of-the-art methods on standard benchmarks for STR (mainly short text).
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2308.07202.pdf' target='_blank'>https://arxiv.org/pdf/2308.07202.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xugong Qin, Pengyuan Lyu, Chengquan Zhang, Yu Zhou, Kun Yao, Peng Zhang, Hailun Lin, Weiping Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07202">Towards Robust Real-Time Scene Text Detection: From Semantic to Instance Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the flexible representation of arbitrary-shaped scene text and simple pipeline, bottom-up segmentation-based methods begin to be mainstream in real-time scene text detection. Despite great progress, these methods show deficiencies in robustness and still suffer from false positives and instance adhesion. Different from existing methods which integrate multiple-granularity features or multiple outputs, we resort to the perspective of representation learning in which auxiliary tasks are utilized to enable the encoder to jointly learn robust features with the main task of per-pixel classification during optimization. For semantic representation learning, we propose global-dense semantic contrast (GDSC), in which a vector is extracted for global semantic representation, then used to perform element-wise contrast with the dense grid features. To learn instance-aware representation, we propose to combine top-down modeling (TDM) with the bottom-up framework to provide implicit instance-level clues for the encoder. With the proposed GDSC and TDM, the encoder network learns stronger representation without introducing any parameters and computations during inference. Equipped with a very light decoder, the detector can achieve more robust real-time scene text detection. Experimental results on four public datasets show that the proposed method can outperform or be comparable to the state-of-the-art on both accuracy and speed. Specifically, the proposed method achieves 87.2% F-measure with 48.2 FPS on Total-Text and 89.6% F-measure with 36.9 FPS on MSRA-TD500 on a single GeForce RTX 2080 Ti GPU.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2306.14269.pdf' target='_blank'>https://arxiv.org/pdf/2306.14269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangchen Xie, Xinyuan Chen, Hongjian Zhan, Palaiahankote Shivakum, Bing Yin, Cong Liu, Yue Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14269">Weakly Supervised Scene Text Generation for Low-resource Languages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A large number of annotated training images is crucial for training successful scene text recognition models. However, collecting sufficient datasets can be a labor-intensive and costly process, particularly for low-resource languages. To address this challenge, auto-generating text data has shown promise in alleviating the problem. Unfortunately, existing scene text generation methods typically rely on a large amount of paired data, which is difficult to obtain for low-resource languages. In this paper, we propose a novel weakly supervised scene text generation method that leverages a few recognition-level labels as weak supervision. The proposed method is able to generate a large amount of scene text images with diverse backgrounds and font styles through cross-language generation. Our method disentangles the content and style features of scene text images, with the former representing textual information and the latter representing characteristics such as font, alignment, and background. To preserve the complete content structure of generated images, we introduce an integrated attention module. Furthermore, to bridge the style gap in the style of different languages, we incorporate a pre-trained font classifier. We evaluate our method using state-of-the-art scene text recognition models. Experiments demonstrate that our generated scene text significantly improves the scene text recognition accuracy and help achieve higher accuracy when complemented with other generative methods.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2211.05895.pdf' target='_blank'>https://arxiv.org/pdf/2211.05895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhecan Wang, Haoxuan You, Yicheng He, Wenhao Li, Kai-Wei Chang, Shih-Fu Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.05895">Understanding ME? Multimodal Evaluation for Fine-grained Visual Commonsense</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual commonsense understanding requires Vision Language (VL) models to not only understand image and text but also cross-reference in-between to fully integrate and achieve comprehension of the visual scene described. Recently, various approaches have been developed and have achieved high performance on visual commonsense benchmarks. However, it is unclear whether the models really understand the visual scene and underlying commonsense knowledge due to limited evaluation data resources. To provide an in-depth analysis, we present a Multimodal Evaluation (ME) pipeline to automatically generate question-answer pairs to test models' understanding of the visual scene, text, and related knowledge. We then take a step further to show that training with the ME data boosts the model's performance in standard VCR evaluation. Lastly, our in-depth analysis and comparison reveal interesting findings: (1) semantically low-level information can assist the learning of high-level information but not the opposite; (2) visual information is generally under utilization compared with text.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2209.00859.pdf' target='_blank'>https://arxiv.org/pdf/2209.00859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinshui Hu, Chenyu Liu, Qiandong Yan, Xuyang Zhu, Jiajia Wu, Jun Du, Lirong Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.00859">Vision-Language Adaptive Mutual Decoder for OOV-STR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works have shown huge success of deep learning models for common in vocabulary (IV) scene text recognition. However, in real-world scenarios, out-of-vocabulary (OOV) words are of great importance and SOTA recognition models usually perform poorly on OOV settings. Inspired by the intuition that the learned language prior have limited OOV preformence, we design a framework named Vision Language Adaptive Mutual Decoder (VLAMD) to tackle OOV problems partly. VLAMD consists of three main conponents. Firstly, we build an attention based LSTM decoder with two adaptively merged visual-only modules, yields a vision-language balanced main branch. Secondly, we add an auxiliary query based autoregressive transformer decoding head for common visual and language prior representation learning. Finally, we couple these two designs with bidirectional training for more diverse language modeling, and do mutual sequential decoding to get robuster results. Our approach achieved 70.31\% and 59.61\% word accuracy on IV+OOV and OOV settings respectively on Cropped Word Recognition Task of OOV-ST Challenge at ECCV 2022 TiE Workshop, where we got 1st place on both settings.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2111.03664.pdf' target='_blank'>https://arxiv.org/pdf/2111.03664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ji Won Yoon, Hyung Yong Kim, Hyeonseung Lee, Sunghwan Ahn, Nam Soo Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.03664">Oracle Teacher: Leveraging Target Information for Better Knowledge Distillation of CTC Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge distillation (KD), best known as an effective method for model compression, aims at transferring the knowledge of a bigger network (teacher) to a much smaller network (student). Conventional KD methods usually employ the teacher model trained in a supervised manner, where output labels are treated only as targets. Extending this supervised scheme further, we introduce a new type of teacher model for connectionist temporal classification (CTC)-based sequence models, namely Oracle Teacher, that leverages both the source inputs and the output labels as the teacher model's input. Since the Oracle Teacher learns a more accurate CTC alignment by referring to the target information, it can provide the student with more optimal guidance. One potential risk for the proposed approach is a trivial solution that the model's output directly copies the target input. Based on a many-to-one mapping property of the CTC algorithm, we present a training strategy that can effectively prevent the trivial solution and thus enables utilizing both source and target inputs for model training. Extensive experiments are conducted on two sequence learning tasks: speech recognition and scene text recognition. From the experimental results, we empirically show that the proposed model improves the students across these tasks while achieving a considerable speed-up in the teacher model's training time.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2503.18883.pdf' target='_blank'>https://arxiv.org/pdf/2503.18883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Savas Ozkan, Andrea Maracani, Hyowon Kim, Sijun Cho, Eunchung Noh, Jeongwon Min, Jung Min Cho, Mete Ozay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18883">Efficient and Accurate Scene Text Recognition with Cascaded-Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, vision transformers with text decoder have demonstrated remarkable performance on Scene Text Recognition (STR) due to their ability to capture long-range dependencies and contextual relationships with high learning capacity. However, the computational and memory demands of these models are significant, limiting their deployment in resource-constrained applications. To address this challenge, we propose an efficient and accurate STR system. Specifically, we focus on improving the efficiency of encoder models by introducing a cascaded-transformers structure. This structure progressively reduces the vision token size during the encoding step, effectively eliminating redundant tokens and reducing computational cost. Our experimental results confirm that our STR system achieves comparable performance to state-of-the-art baselines while substantially decreasing computational requirements. In particular, for large-models, the accuracy remains same, 92.77 to 92.68, while computational complexity is almost halved with our structure.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2503.16184.pdf' target='_blank'>https://arxiv.org/pdf/2503.16184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Maracani, Savas Ozkan, Sijun Cho, Hyowon Kim, Eunchung Noh, Jeongwon Min, Cho Jung Min, Dookun Park, Mete Ozay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16184">Accurate Scene Text Recognition with Efficient Model Scaling and Cloze Self-Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scaling architectures have been proven effective for improving Scene Text Recognition (STR), but the individual contribution of vision encoder and text decoder scaling remain under-explored. In this work, we present an in-depth empirical analysis and demonstrate that, contrary to previous observations, scaling the decoder yields significant performance gains, always exceeding those achieved by encoder scaling alone. We also identify label noise as a key challenge in STR, particularly in real-world data, which can limit the effectiveness of STR models. To address this, we propose Cloze Self-Distillation (CSD), a method that mitigates label noise by distilling a student model from context-aware soft predictions and pseudolabels generated by a teacher model. Additionally, we enhance the decoder architecture by introducing differential cross-attention for STR. Our methodology achieves state-of-the-art performance on 10 out of 11 benchmarks using only real data, while significantly reducing the parameter size and computational costs.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2407.16204.pdf' target='_blank'>https://arxiv.org/pdf/2407.16204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Zhao, Qing Guo, Xiaoguang Li, Song Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16204">CLII: Visual-Text Inpainting via Cross-Modal Predictive Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image inpainting aims to fill missing pixels in damaged images and has achieved significant progress with cut-edging learning techniques. Nevertheless, state-of-the-art inpainting methods are mainly designed for nature images and cannot correctly recover text within scene text images, and training existing models on the scene text images cannot fix the issues. In this work, we identify the visual-text inpainting task to achieve high-quality scene text image restoration and text completion: Given a scene text image with unknown missing regions and the corresponding text with unknown missing characters, we aim to complete the missing information in both images and text by leveraging their complementary information. Intuitively, the input text, even if damaged, contains language priors of the contents within the images and can guide the image inpainting. Meanwhile, the scene text image includes the appearance cues of the characters that could benefit text recovery. To this end, we design the cross-modal predictive interaction (CLII) model containing two branches, i.e., ImgBranch and TxtBranch, for scene text inpainting and text completion, respectively while leveraging their complementary effectively. Moreover, we propose to embed our model into the SOTA scene text spotting method and significantly enhance its robustness against missing pixels, which demonstrates the practicality of the newly developed task. To validate the effectiveness of our method, we construct three real datasets based on existing text-related datasets, containing 1838 images and covering three scenarios with curved, incidental, and styled texts, and conduct extensive experiments to show that our method outperforms baselines significantly.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2306.02443.pdf' target='_blank'>https://arxiv.org/pdf/2306.02443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghao Fu, Xin Man, Yihan Xu, Jie Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02443">ESTISR: Adapting Efficient Scene Text Image Super-resolution for Real-Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While scene text image super-resolution (STISR) has yielded remarkable improvements in accurately recognizing scene text, prior methodologies have placed excessive emphasis on optimizing performance, rather than paying due attention to efficiency - a crucial factor in ensuring deployment of the STISR-STR pipeline. In this work, we propose a novel Efficient Scene Text Image Super-resolution (ESTISR) Network for resource-limited deployment platform. ESTISR's functionality primarily depends on two critical components: a CNN-based feature extractor and an efficient self-attention mechanism used for decoding low-resolution images. We designed a re-parameterized inverted residual block specifically suited for resource-limited circumstances as the feature extractor. Meanwhile, we proposed a novel self-attention mechanism, softmax shrinking, based on a kernel-based approach. This innovative technique offers linear complexity while also naturally incorporating discriminating low-level features into the self-attention structure. Extensive experiments on TextZoom show that ESTISR retains a high image restoration quality and improved STR accuracy of low-resolution images. Furthermore, ESTISR consistently outperforms current methods in terms of actual running time and peak memory consumption, while achieving a better trade-off between performance and efficiency.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2211.13984.pdf' target='_blank'>https://arxiv.org/pdf/2211.13984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhao Zhou, Xiangcheng Du, Yingbin Zheng, Cheng Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.13984">Aggregated Text Transformer for Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the multi-scale aggregation strategy for scene text detection in natural images. We present the Aggregated Text TRansformer(ATTR), which is designed to represent texts in scene images with a multi-scale self-attention mechanism. Starting from the image pyramid with multiple resolutions, the features are first extracted at different scales with shared weight and then fed into an encoder-decoder architecture of Transformer. The multi-scale image representations are robust and contain rich information on text contents of various sizes. The text Transformer aggregates these features to learn the interaction across different scales and improve text representation. The proposed method detects scene texts by representing each text instance as an individual binary mask, which is tolerant of curve texts and regions with dense instances. Extensive experiments on public scene text detection datasets demonstrate the effectiveness of the proposed framework.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2207.11469.pdf' target='_blank'>https://arxiv.org/pdf/2207.11469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangcheng Du, Zhao Zhou, Yingbin Zheng, Xingjiao Wu, Tianlong Ma, Cheng Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.11469">Progressive Scene Text Erasing with Self-Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text erasing seeks to erase text contents from scene images and current state-of-the-art text erasing models are trained on large-scale synthetic data. Although data synthetic engines can provide vast amounts of annotated training samples, there are differences between synthetic and real-world data. In this paper, we employ self-supervision for feature representation on unlabeled real-world scene text images. A novel pretext task is designed to keep consistent among text stroke masks of image variants. We design the Progressive Erasing Network in order to remove residual texts. The scene text is erased progressively by leveraging the intermediate generated results which provide the foundation for subsequent higher quality results. Experiments show that our method significantly improves the generalization of the text erasing task and achieves state-of-the-art performance on public benchmarks.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2506.04999.pdf' target='_blank'>https://arxiv.org/pdf/2506.04999.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gengluo Li, Huawen Shen, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04999">Beyond Cropped Regions: New Benchmark and Corresponding Baseline for Chinese Scene Text Retrieval in Diverse Layouts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Chinese scene text retrieval is a practical task that aims to search for images containing visual instances of a Chinese query text. This task is extremely challenging because Chinese text often features complex and diverse layouts in real-world scenes. Current efforts tend to inherit the solution for English scene text retrieval, failing to achieve satisfactory performance. In this paper, we establish a Diversified Layout benchmark for Chinese Street View Text Retrieval (DL-CSVTR), which is specifically designed to evaluate retrieval performance across various text layouts, including vertical, cross-line, and partial alignments. To address the limitations in existing methods, we propose Chinese Scene Text Retrieval CLIP (CSTR-CLIP), a novel model that integrates global visual information with multi-granularity alignment training. CSTR-CLIP applies a two-stage training process to overcome previous limitations, such as the exclusion of visual features outside the text region and reliance on single-granularity alignment, thereby enabling the model to effectively handle diverse text layouts. Experiments on existing benchmark show that CSTR-CLIP outperforms the previous state-of-the-art model by 18.82% accuracy and also provides faster inference speed. Further analysis on DL-CSVTR confirms the superior performance of CSTR-CLIP in handling various text layouts. The dataset and code will be publicly available to facilitate research in Chinese scene text retrieval.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2308.15004.pdf' target='_blank'>https://arxiv.org/pdf/2308.15004.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijin Liu, Ning Lu, Dapeng Chen, Cheng Li, Zejian Yuan, Wei Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15004">PBFormer: Capturing Complex Scene Text Shape with Polynomial Band Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PBFormer, an efficient yet powerful scene text detector that unifies the transformer with a novel text shape representation Polynomial Band (PB). The representation has four polynomial curves to fit a text's top, bottom, left, and right sides, which can capture a text with a complex shape by varying polynomial coefficients. PB has appealing features compared with conventional representations: 1) It can model different curvatures with a fixed number of parameters, while polygon-points-based methods need to utilize a different number of points. 2) It can distinguish adjacent or overlapping texts as they have apparent different curve coefficients, while segmentation-based or points-based methods suffer from adhesive spatial positions. PBFormer combines the PB with the transformer, which can directly generate smooth text contours sampled from predicted curves without interpolation. A parameter-free cross-scale pixel attention (CPA) module is employed to highlight the feature map of a suitable scale while suppressing the other feature maps. The simple operation can help detect small-scale texts and is compatible with the one-stage DETR framework, where no postprocessing exists for NMS. Furthermore, PBFormer is trained with a shape-contained loss, which not only enforces the piecewise alignment between the ground truth and the predicted curves but also makes curves' positions and shapes consistent with each other. Without bells and whistles about text pre-training, our method is superior to the previous state-of-the-art text detectors on the arbitrary-shaped text datasets.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2307.13310.pdf' target='_blank'>https://arxiv.org/pdf/2307.13310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwen Shao, Yuchen Su, Yong Zhou, Fanrong Meng, Hancheng Zhu, Bing Liu, Rui Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13310">CT-Net: Arbitrary-Shaped Text Detection via Contour Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contour based scene text detection methods have rapidly developed recently, but still suffer from inaccurate frontend contour initialization, multi-stage error accumulation, or deficient local information aggregation. To tackle these limitations, we propose a novel arbitrary-shaped scene text detection framework named CT-Net by progressive contour regression with contour transformers. Specifically, we first employ a contour initialization module that generates coarse text contours without any post-processing. Then, we adopt contour refinement modules to adaptively refine text contours in an iterative manner, which are beneficial for context information capturing and progressive global contour deformation. Besides, we propose an adaptive training strategy to enable the contour transformers to learn more potential deformation paths, and introduce a re-score mechanism that can effectively suppress false positives. Extensive experiments are conducted on four challenging datasets, which demonstrate the accuracy and efficiency of our CT-Net over state-of-the-art methods. Particularly, CT-Net achieves F-measure of 86.1 at 11.2 frames per second (FPS) and F-measure of 87.8 at 10.1 FPS for CTW1500 and Total-Text datasets, respectively.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2306.01733.pdf' target='_blank'>https://arxiv.org/pdf/2306.01733.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Srikar Appalaraju, Peng Tang, Qi Dong, Nishant Sankaran, Yichu Zhou, R. Manmatha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01733">DocFormerv2: Local Features for Document Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose DocFormerv2, a multi-modal transformer for Visual Document Understanding (VDU). The VDU domain entails understanding documents (beyond mere OCR predictions) e.g., extracting information from a form, VQA for documents and other tasks. VDU is challenging as it needs a model to make sense of multiple modalities (visual, language and spatial) to make a prediction. Our approach, termed DocFormerv2 is an encoder-decoder transformer which takes as input - vision, language and spatial features. DocFormerv2 is pre-trained with unsupervised tasks employed asymmetrically i.e., two novel document tasks on encoder and one on the auto-regressive decoder. The unsupervised tasks have been carefully designed to ensure that the pre-training encourages local-feature alignment between multiple modalities. DocFormerv2 when evaluated on nine datasets shows state-of-the-art performance over strong baselines e.g. TabFact (4.3%), InfoVQA (1.4%), FUNSD (1%). Furthermore, to show generalization capabilities, on three VQA tasks involving scene-text, Doc- Formerv2 outperforms previous comparably-sized models and even does better than much larger models (such as GIT2, PaLi and Flamingo) on some tasks. Extensive ablations show that due to its pre-training, DocFormerv2 understands multiple modalities better than prior-art in VDU.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/1407.7504.pdf' target='_blank'>https://arxiv.org/pdf/1407.7504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lluis Gomez, Dimosthenis Karatzas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1407.7504">A Fast Hierarchical Method for Multi-script and Arbitrary Oriented Scene Text Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Typography and layout lead to the hierarchical organisation of text in words, text lines, paragraphs. This inherent structure is a key property of text in any script and language, which has nonetheless been minimally leveraged by existing text detection methods. This paper addresses the problem of text segmentation in natural scenes from a hierarchical perspective. Contrary to existing methods, we make explicit use of text structure, aiming directly to the detection of region groupings corresponding to text within a hierarchy produced by an agglomerative similarity clustering process over individual regions. We propose an optimal way to construct such an hierarchy introducing a feature space designed to produce text group hypotheses with high recall and a novel stopping rule combining a discriminative classifier and a probabilistic measure of group meaningfulness based in perceptual organization. Results obtained over four standard datasets, covering text in variable orientations and different languages, demonstrate that our algorithm, while being trained in a single mixed dataset, outperforms state of the art methods in unconstrained scenarios.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2509.17707.pdf' target='_blank'>https://arxiv.org/pdf/2509.17707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emre GÃ¼lsoylu, Alhassan Abdelhalim, Derya Kara Boztas, Ole Grasse, Carlos Jahn, Simone Frintrop, Janick Edinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17707">Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The standardisation of Intermodal Loading Units (ILUs), such as containers, semi-trailers and swap bodies, has revolutionised global trade yet their efficient and robust identification remains a critical bottleneck in high-throughput ports and terminals. This paper reviews 63 empirical studies that propose computer vision (CV) based solutions. It covers the last 35 years (1990-2025), tracing the field's evolution from early digital image processing (DIP) and traditional machine learning (ML) to the current dominance of deep learning (DL) techniques. While CV offers cost-effective alternatives for other types of identification techniques, its development is hindered by the lack of publicly available benchmarking datasets. This results in high variance for the reported results such as end-to-end accuracy ranging from 5 % to 96 %. Beyond dataset limitations, this review highlights the emerging challenges especially introduced by the shift from character-based text recognition to scene-text spotting and the integration of mobile cameras (e.g. drones, sensor equipped ground vehicles) for dynamic terminal monitoring. To advance the field, the paper calls for standardised terminology, open-access datasets, shared source code, while outlining future research directions such as contextless text recognition optimised for ISO6346 codes.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2505.17778.pdf' target='_blank'>https://arxiv.org/pdf/2505.17778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Xie, Jielei Zhang, Pengyu Chen, Ziyue Wang, Weihang Wang, Longwen Gao, Peiyi Li, Huyang Sun, Qiang Zhang, Qian Qiao, Jiaqing Fan, Zhouhui Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17778">TextFlux: An OCR-Free DiT Model for High-Fidelity Multilingual Scene Text Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based scene text synthesis has progressed rapidly, yet existing methods commonly rely on additional visual conditioning modules and require large-scale annotated data to support multilingual generation. In this work, we revisit the necessity of complex auxiliary modules and further explore an approach that simultaneously ensures glyph accuracy and achieves high-fidelity scene integration, by leveraging diffusion models' inherent capabilities for contextual reasoning. To this end, we introduce TextFlux, a DiT-based framework that enables multilingual scene text synthesis. The advantages of TextFlux can be summarized as follows: (1) OCR-free model architecture. TextFlux eliminates the need for OCR encoders (additional visual conditioning modules) that are specifically used to extract visual text-related features. (2) Strong multilingual scalability. TextFlux is effective in low-resource multilingual settings, and achieves strong performance in newly added languages with fewer than 1,000 samples. (3) Streamlined training setup. TextFlux is trained with only 1% of the training data required by competing methods. (4) Controllable multi-line text generation. TextFlux offers flexible multi-line synthesis with precise line-level control, outperforming methods restricted to single-line or rigid layouts. Extensive experiments and visualizations demonstrate that TextFlux outperforms previous methods in both qualitative and quantitative evaluations.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2504.04001.pdf' target='_blank'>https://arxiv.org/pdf/2504.04001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuang Yang, Xu Han, Tao Han, Han Han, Bingxuan Zhao, Qi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04001">Edge Approximation Text Detector</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pursuing efficient text shape representations helps scene text detection models focus on compact foreground regions and optimize the contour reconstruction steps to simplify the whole detection pipeline. Current approaches either represent irregular shapes via box-to-polygon strategy or decomposing a contour into pieces for fitting gradually, the deficiency of coarse contours or complex pipelines always exists in these models. Considering the above issues, we introduce EdgeText to fit text contours compactly while alleviating excessive contour rebuilding processes. Concretely, it is observed that the two long edges of texts can be regarded as smooth curves. It allows us to build contours via continuous and smooth edges that cover text regions tightly instead of fitting piecewise, which helps avoid the two limitations in current models. Inspired by this observation, EdgeText formulates the text representation as the edge approximation problem via parameterized curve fitting functions. In the inference stage, our model starts with locating text centers, and then creating curve functions for approximating text edges relying on the points. Meanwhile, truncation points are determined based on the location features. In the end, extracting curve segments from curve functions by using the pixel coordinate information brought by truncation points to reconstruct text contours. Furthermore, considering the deep dependency of EdgeText on text edges, a bilateral enhanced perception (BEP) module is designed. It encourages our model to pay attention to the recognition of edge features. Additionally, to accelerate the learning of the curve function parameters, we introduce a proportional integral loss (PI-loss) to force the proposed model to focus on the curve distribution and avoid being disturbed by text scales.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2412.19504.pdf' target='_blank'>https://arxiv.org/pdf/2412.19504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Li, Bo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19504">Hear the Scene: Audio-Enhanced Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in scene text spotting have focused on end-to-end methodologies that heavily rely on precise location annotations, which are often costly and labor-intensive to procure. In this study, we introduce an innovative approach that leverages only transcription annotations for training text spotting models, substantially reducing the dependency on elaborate annotation processes. Our methodology employs a query-based paradigm that facilitates the learning of implicit location features through the interaction between text queries and image embeddings. These features are later refined during the text recognition phase using an attention activation map. Addressing the challenges associated with training a weakly-supervised model from scratch, we implement a circular curriculum learning strategy to enhance model convergence. Additionally, we introduce a coarse-to-fine cross-attention localization mechanism for more accurate text instance localization. Notably, our framework supports audio-based annotation, which significantly diminishes annotation time and provides an inclusive alternative for individuals with disabilities. Our approach achieves competitive performance against existing benchmarks, demonstrating that high accuracy in text spotting can be attained without extensive location annotations.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2411.04642.pdf' target='_blank'>https://arxiv.org/pdf/2411.04642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Fhima, Elad Ben Avraham, Oren Nuriel, Yair Kittenplon, Roy Ganz, Aviad Aberdam, Ron Litman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04642">TAP-VL: Text Layout-Aware Pre-training for Enriched Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language (VL) models have garnered considerable research interest; however, they still face challenges in effectively handling text within images. To address this limitation, researchers have developed two approaches. The first method involves utilizing external Optical Character Recognition (OCR) tools to extract textual information from images, which is then prepended to other textual inputs. The second strategy focuses on employing extremely high-resolution images to improve text recognition capabilities. In this paper, we focus on enhancing the first strategy by introducing a novel method, named TAP-VL, which treats OCR information as a distinct modality and seamlessly integrates it into any VL model. TAP-VL employs a lightweight transformer-based OCR module to receive OCR with layout information, compressing it into a short fixed-length sequence for input into the LLM. Initially, we conduct model-agnostic pretraining of the OCR module on unlabeled documents, followed by its integration into any VL architecture through brief fine-tuning. Extensive experiments demonstrate consistent performance improvements when applying TAP-VL to top-performing VL models, across scene-text and document-based VL benchmarks.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2408.00106.pdf' target='_blank'>https://arxiv.org/pdf/2408.00106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Xie, Yuzhe Li, Yang Liu, Zhifei Zhang, Zhaowen Wang, Wei Xiong, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00106">WAS: Dataset and Methods for Artistic Text Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate text segmentation results are crucial for text-related generative tasks, such as text image generation, text editing, text removal, and text style transfer. Recently, some scene text segmentation methods have made significant progress in segmenting regular text. However, these methods perform poorly in scenarios containing artistic text. Therefore, this paper focuses on the more challenging task of artistic text segmentation and constructs a real artistic text segmentation dataset. One challenge of the task is that the local stroke shapes of artistic text are changeable with diversity and complexity. We propose a decoder with the layer-wise momentum query to prevent the model from ignoring stroke regions of special shapes. Another challenge is the complexity of the global topological structure. We further design a skeleton-assisted head to guide the model to focus on the global structure. Additionally, to enhance the generalization performance of the text segmentation model, we propose a strategy for training data synthesis, based on the large multi-modal model and the diffusion model. Experimental results show that our proposed method and synthetic dataset can significantly enhance the performance of artistic text segmentation and achieve state-of-the-art results on other public datasets.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2404.17151.pdf' target='_blank'>https://arxiv.org/pdf/2404.17151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengpei Xu, Wenjing Jia, Ruomei Wang, Xiaonan Luo, Xiangjian He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17151">MorphText: Deep Morphology Regularized Arbitrary-shape Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bottom-up text detection methods play an important role in arbitrary-shape scene text detection but there are two restrictions preventing them from achieving their great potential, i.e., 1) the accumulation of false text segment detections, which affects subsequent processing, and 2) the difficulty of building reliable connections between text segments. Targeting these two problems, we propose a novel approach, named ``MorphText", to capture the regularity of texts by embedding deep morphology for arbitrary-shape text detection. Towards this end, two deep morphological modules are designed to regularize text segments and determine the linkage between them. First, a Deep Morphological Opening (DMOP) module is constructed to remove false text segment detections generated in the feature extraction process. Then, a Deep Morphological Closing (DMCL) module is proposed to allow text instances of various shapes to stretch their morphology along their most significant orientation while deriving their connections. Extensive experiments conducted on four challenging benchmark datasets (CTW1500, Total-Text, MSRA-TD500 and ICDAR2017) demonstrate that our proposed MorphText outperforms both top-down and bottom-up state-of-the-art arbitrary-shape scene text detection approaches.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2404.12734.pdf' target='_blank'>https://arxiv.org/pdf/2404.12734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Da Chang, Yu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12734">Mixed Text Recognition with Efficient Parameter Fine-Tuning and Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of OCR technology, mixed-scene text recognition has become a key technical challenge. Although deep learning models have achieved significant results in specific scenarios, their generality and stability still need improvement, and the high demand for computing resources affects flexibility. To address these issues, this paper proposes DLoRA-TrOCR, a parameter-efficient hybrid text spotting method based on a pre-trained OCR Transformer. By embedding a weight-decomposed DoRA module in the image encoder and a LoRA module in the text decoder, this method can be efficiently fine-tuned on various downstream tasks. Our method requires no more than 0.7\% trainable parameters, not only accelerating the training efficiency but also significantly improving the recognition accuracy and cross-dataset generalization performance of the OCR system in mixed text scenes. Experiments show that our proposed DLoRA-TrOCR outperforms other parameter-efficient fine-tuning methods in recognizing complex scenes with mixed handwritten, printed, and street text, achieving a CER of 4.02 on the IAM dataset, a F1 score of 94.29 on the SROIE dataset, and a WAR of 86.70 on the STR Benchmark, reaching state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2402.05472.pdf' target='_blank'>https://arxiv.org/pdf/2402.05472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roy Ganz, Yair Kittenplon, Aviad Aberdam, Elad Ben Avraham, Oren Nuriel, Shai Mazor, Ron Litman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05472">Question Aware Vision Transformer for Multimodal Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language (VL) models have gained significant research focus, enabling remarkable advances in multimodal reasoning. These architectures typically comprise a vision encoder, a Large Language Model (LLM), and a projection module that aligns visual features with the LLM's representation space. Despite their success, a critical limitation persists: the vision encoding process remains decoupled from user queries, often in the form of image-related questions. Consequently, the resulting visual features may not be optimally attuned to the query-specific elements of the image. To address this, we introduce QA-ViT, a Question Aware Vision Transformer approach for multimodal reasoning, which embeds question awareness directly within the vision encoder. This integration results in dynamic visual features focusing on relevant image aspects to the posed question. QA-ViT is model-agnostic and can be incorporated efficiently into any VL architecture. Extensive experiments demonstrate the effectiveness of applying our method to various multimodal architectures, leading to consistent improvement across diverse tasks and showcasing its potential for enhancing visual and scene-text understanding.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2311.13317.pdf' target='_blank'>https://arxiv.org/pdf/2311.13317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Zhou, Liangcai Gao, Zhi Tang, Baole Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13317">Recognition-Guided Diffusion Model for Scene Text Image Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Text Image Super-Resolution (STISR) aims to enhance the resolution and legibility of text within low-resolution (LR) images, consequently elevating recognition accuracy in Scene Text Recognition (STR). Previous methods predominantly employ discriminative Convolutional Neural Networks (CNNs) augmented with diverse forms of text guidance to address this issue. Nevertheless, they remain deficient when confronted with severely blurred images, due to their insufficient generation capability when little structural or semantic information can be extracted from original images. Therefore, we introduce RGDiffSR, a Recognition-Guided Diffusion model for scene text image Super-Resolution, which exhibits great generative diversity and fidelity even in challenging scenarios. Moreover, we propose a Recognition-Guided Denoising Network, to guide the diffusion model generating LR-consistent results through succinct semantic guidance. Experiments on the TextZoom dataset demonstrate the superiority of RGDiffSR over prior state-of-the-art methods in both text recognition accuracy and image fidelity.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2306.03377.pdf' target='_blank'>https://arxiv.org/pdf/2306.03377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukun Zhai, Xiaoqiang Zhang, Xiameng Qin, Sanyuan Zhao, Xingping Dong, Jianbing Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03377">TextFormer: A Query-based End-to-End Text Spotter with Mixed Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end text spotting is a vital computer vision task that aims to integrate scene text detection and recognition into a unified framework. Typical methods heavily rely on Region-of-Interest (RoI) operations to extract local features and complex post-processing steps to produce final predictions. To address these limitations, we propose TextFormer, a query-based end-to-end text spotter with Transformer architecture. Specifically, using query embedding per text instance, TextFormer builds upon an image encoder and a text decoder to learn a joint semantic understanding for multi-task modeling. It allows for mutual training and optimization of classification, segmentation, and recognition branches, resulting in deeper feature sharing without sacrificing flexibility or simplicity. Additionally, we design an Adaptive Global aGgregation (AGG) module to transfer global features into sequential features for reading arbitrarily-shaped texts, which overcomes the sub-optimization problem of RoI operations. Furthermore, potential corpus information is utilized from weak annotations to full labels through mixed supervision, further improving text detection and end-to-end text spotting results. Extensive experiments on various bilingual (i.e., English and Chinese) benchmarks demonstrate the superiority of our method. Especially on TDA-ReCTS dataset, TextFormer surpasses the state-of-the-art method in terms of 1-NED by 13.2%.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2302.14261.pdf' target='_blank'>https://arxiv.org/pdf/2302.14261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueming Yan, Zhihang Fang, Yaochu Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14261">Augmented Transformers with Adaptive n-grams Embedding for Multilingual Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While vision transformers have been highly successful in improving the performance in image-based tasks, not much work has been reported on applying transformers to multilingual scene text recognition due to the complexities in the visual appearance of multilingual texts. To fill the gap, this paper proposes an augmented transformer architecture with n-grams embedding and cross-language rectification (TANGER). TANGER consists of a primary transformer with single patch embeddings of visual images, and a supplementary transformer with adaptive n-grams embeddings that aims to flexibly explore the potential correlations between neighbouring visual patches, which is essential for feature extraction from multilingual scene texts. Cross-language rectification is achieved with a loss function that takes into account both language identification and contextual coherence scoring. Extensive comparative studies are conducted on four widely used benchmark datasets as well as a new multilingual scene text dataset containing Indonesian, English, and Chinese collected from tourism scenes in Indonesia. Our experimental results demonstrate that TANGER is considerably better compared to the state-of-the-art, especially in handling complex multilingual scene texts.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2301.07464.pdf' target='_blank'>https://arxiv.org/pdf/2301.07464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aviad Aberdam, David BensaÃ¯d, Alona Golts, Roy Ganz, Oren Nuriel, Royee Tichauer, Shai Mazor, Ron Litman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.07464">CLIPTER: Looking at the Bigger Picture in Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reading text in real-world scenarios often requires understanding the context surrounding it, especially when dealing with poor-quality text. However, current scene text recognizers are unaware of the bigger picture as they operate on cropped text images. In this study, we harness the representative capabilities of modern vision-language models, such as CLIP, to provide scene-level information to the crop-based recognizer. We achieve this by fusing a rich representation of the entire image, obtained from the vision-language model, with the recognizer word-level features via a gated cross-attention mechanism. This component gradually shifts to the context-enhanced representation, allowing for stable fine-tuning of a pretrained recognizer. We demonstrate the effectiveness of our model-agnostic framework, CLIPTER (CLIP TExt Recognition), on leading text recognition architectures and achieve state-of-the-art results across multiple benchmarks. Furthermore, our analysis highlights improved robustness to out-of-vocabulary words and enhanced generalization in low-data regimes.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2301.07389.pdf' target='_blank'>https://arxiv.org/pdf/2301.07389.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roy Ganz, Oren Nuriel, Aviad Aberdam, Yair Kittenplon, Shai Mazor, Ron Litman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.07389">Towards Models that Can See and Read</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Question Answering (VQA) and Image Captioning (CAP), which are among the most popular vision-language tasks, have analogous scene-text versions that require reasoning from the text in the image. Despite their obvious resemblance, the two are treated independently and, as we show, yield task-specific methods that can either see or read, but not both. In this work, we conduct an in-depth analysis of this phenomenon and propose UniTNT, a Unified Text-Non-Text approach, which grants existing multimodal architectures scene-text understanding capabilities. Specifically, we treat scene-text information as an additional modality, fusing it with any pretrained encoder-decoder-based architecture via designated modules. Thorough experiments reveal that UniTNT leads to the first single model that successfully handles both task types. Moreover, we show that scene-text understanding capabilities can boost vision-language models' performance on general VQA and CAP by up to 2.69% and 0.6 CIDEr, respectively.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2108.01809.pdf' target='_blank'>https://arxiv.org/pdf/2108.01809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengpei Xu, Wenjing Jia, Tingcheng Cui, Ruomei Wang, Yuan-fang Zhang, Xiangjian He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.01809">What's Wrong with the Bottom-up Methods in Arbitrary-shape Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The latest trend in the bottom-up perspective for arbitrary-shape scene text detection is to reason the links between text segments using Graph Convolutional Network (GCN). Notwithstanding, the performance of the best performing bottom-up method is still inferior to that of the best performing top-down method even with the help of GCN. We argue that this is not mainly caused by the limited feature capturing ability of the text proposal backbone or GCN, but by their failure to make a full use of visual-relational features for suppressing false detection, as well as the sub-optimal route-finding mechanism used for grouping text segments. In this paper, we revitalize the classic text detection frameworks by aggregating the visual-relational features of text with two effective false positive/negative suppression mechanisms. First, dense overlapping text segments depicting the `characterness' and `streamline' of text are generated for further relational reasoning and weakly supervised segment classification. Here, relational graph features are used for suppressing false positives/negatives. Then, to fuse the relational features with visual features, a Location-Aware Transfer (LAT) module is designed to transfer text's relational features into visual compatible features with a Fuse Decoding (FD) module to enhance the representation of text regions for the second step suppression. Finally, a novel multiple-text-map-aware contour-approximation strategy is developed, instead of the widely-used route-finding process. Experiments conducted on five benchmark datasets, i.e., CTW1500, Total-Text, ICDAR2015, MSRA-TD500, and MLT2017 demonstrate that our method outperforms the state-of-the-art performance when being embedded in a classic text detection framework, which revitalises the superb strength of the bottom-up methods.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2510.10910.pdf' target='_blank'>https://arxiv.org/pdf/2510.10910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Honghui Yuan, Keiji Yanai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10910">SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of diffusion models, style transfer has made remarkable progress. However, flexible and localized style editing for scene text remains an unsolved challenge. Although existing scene text editing methods have achieved text region editing, they are typically limited to content replacement and simple styles, which lack the ability of free-style transfer. In this paper, we introduce SceneTextStylizer, a novel training-free diffusion-based framework for flexible and high-fidelity style transfer of text in scene images. Unlike prior approaches that either perform global style transfer or focus solely on textual content modification, our method enables prompt-guided style transformation specifically for text regions, while preserving both text readability and stylistic consistency. To achieve this, we design a feature injection module that leverages diffusion model inversion and self-attention to transfer style features effectively. Additionally, a region control mechanism is introduced by applying a distance-based changing mask at each denoising step, enabling precise spatial control. To further enhance visual quality, we incorporate a style enhancement module based on the Fourier transform to reinforce stylistic richness. Extensive experiments demonstrate that our method achieves superior performance in scene text style transformation, outperforming existing state-of-the-art methods in both visual fidelity and text preservation.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2412.15523.pdf' target='_blank'>https://arxiv.org/pdf/2412.15523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Duan, Qianyi Jiang, Pei Fu, Jiamin Chen, Shengxi Li, Zining Wang, Shan Guo, Junfeng Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15523">InstructOCR: Instruction Boosting Scene Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of scene text spotting, previous OCR methods primarily relied on image encoders and pre-trained text information, but they often overlooked the advantages of incorporating human language instructions. To address this gap, we propose InstructOCR, an innovative instruction-based scene text spotting model that leverages human language instructions to enhance the understanding of text within images. Our framework employs both text and image encoders during training and inference, along with instructions meticulously designed based on text attributes. This approach enables the model to interpret text more accurately and flexibly. Extensive experiments demonstrate the effectiveness of our model and we achieve state-of-the-art results on widely used benchmarks. Furthermore, the proposed framework can be seamlessly applied to scene text VQA tasks. By leveraging instruction strategies during pre-training, the performance on downstream VQA tasks can be significantly improved, with a 2.6% increase on the TextVQA dataset and a 2.1% increase on the ST-VQA dataset. These experimental results provide insights into the benefits of incorporating human language instructions for OCR-related tasks.
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2401.11704.pdf' target='_blank'>https://arxiv.org/pdf/2401.11704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyuan Zhu, Fagui Liu, Xi Chen, Quan Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11704">EK-Net:Real-time Scene Text Detection with Expand Kernel Distance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, scene text detection has received significant attention due to its wide application. However, accurate detection in complex scenes of multiple scales, orientations, and curvature remains a challenge. Numerous detection methods adopt the Vatti clipping (VC) algorithm for multiple-instance training to address the issue of arbitrary-shaped text. Yet we identify several bias results from these approaches called the "shrinked kernel". Specifically, it refers to a decrease in accuracy resulting from an output that overly favors the text kernel. In this paper, we propose a new approach named Expand Kernel Network (EK-Net) with expand kernel distance to compensate for the previous deficiency, which includes three-stages regression to complete instance detection. Moreover, EK-Net not only realize the precise positioning of arbitrary-shaped text, but also achieve a trade-off between performance and speed. Evaluation results demonstrate that EK-Net achieves state-of-the-art or competitive performance compared to other advanced methods, e.g., F-measure of 85.72% at 35.42 FPS on ICDAR 2015, F-measure of 85.75% at 40.13 FPS on CTW1500.
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2401.05338.pdf' target='_blank'>https://arxiv.org/pdf/2401.05338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daqian Shao, Lukas Fesser, Marta Kwiatkowska
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05338">STR-Cert: Robustness Certification for Deep Text Recognition on Deep Learning Pipelines and Vision Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robustness certification, which aims to formally certify the predictions of neural networks against adversarial inputs, has become an integral part of important tool for safety-critical applications. Despite considerable progress, existing certification methods are limited to elementary architectures, such as convolutional networks, recurrent networks and recently Transformers, on benchmark datasets such as MNIST. In this paper, we focus on the robustness certification of scene text recognition (STR), which is a complex and extensively deployed image-based sequence prediction problem. We tackle three types of STR model architectures, including the standard STR pipelines and the Vision Transformer. We propose STR-Cert, the first certification method for STR models, by significantly extending the DeepPoly polyhedral verification framework via deriving novel polyhedral bounds and algorithms for key STR model components. Finally, we certify and compare STR models on six datasets, demonstrating the efficiency and scalability of robustness certification, particularly for the Vision Transformer.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2305.04524.pdf' target='_blank'>https://arxiv.org/pdf/2305.04524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajun Wei, Hongjian Zhan, Xiao Tu, Yue Lu, Umapada Pal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.04524">Scene Text Recognition with Image-Text Matching-guided Dictionary</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Employing a dictionary can efficiently rectify the deviation between the visual prediction and the ground truth in scene text recognition methods. However, the independence of the dictionary on the visual features may lead to incorrect rectification of accurate visual predictions. In this paper, we propose a new dictionary language model leveraging the Scene Image-Text Matching(SITM) network, which avoids the drawbacks of the explicit dictionary language model: 1) the independence of the visual features; 2) noisy choice in candidates etc. The SITM network accomplishes this by using Image-Text Contrastive (ITC) Learning to match an image with its corresponding text among candidates in the inference stage. ITC is widely used in vision-language learning to pull the positive image-text pair closer in feature space. Inspired by ITC, the SITM network combines the visual features and the text features of all candidates to identify the candidate with the minimum distance in the feature space. Our lexicon method achieves better results(93.8\% accuracy) than the ordinary method results(92.1\% accuracy) on six mainstream benchmarks. Additionally, we integrate our method with ABINet and establish new state-of-the-art results on several benchmarks.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2510.07951.pdf' target='_blank'>https://arxiv.org/pdf/2510.07951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Dong, Yurui Zhang, Changmao Li, Naomi Rue Golding, Qing Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07951">A Large-scale Dataset for Robust Complex Anime Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current text detection datasets primarily target natural or document scenes, where text typically appear in regular font and shapes, monotonous colors, and orderly layouts. The text usually arranged along straight or curved lines. However, these characteristics differ significantly from anime scenes, where text is often diverse in style, irregularly arranged, and easily confused with complex visual elements such as symbols and decorative patterns. Text in anime scene also includes a large number of handwritten and stylized fonts. Motivated by this gap, we introduce AnimeText, a large-scale dataset containing 735K images and 4.2M annotated text blocks. It features hierarchical annotations and hard negative samples tailored for anime scenarios. %Cross-dataset evaluations using state-of-the-art methods demonstrate that models trained on AnimeText achieve superior performance in anime text detection tasks compared to existing datasets. To evaluate the robustness of AnimeText in complex anime scenes, we conducted cross-dataset benchmarking using state-of-the-art text detection methods. Experimental results demonstrate that models trained on AnimeText outperform those trained on existing datasets in anime scene text detection tasks. AnimeText on HuggingFace: https://huggingface.co/datasets/deepghs/AnimeText
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2410.18277.pdf' target='_blank'>https://arxiv.org/pdf/2410.18277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vannkinh Nom, Souhail Bakkali, Muhammad Muzzamil Luqman, MickaÃ«l Coustaty, Jean-Marc Ogier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18277">KhmerST: A Low-Resource Khmer Scene Text Detection and Recognition Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing effective scene text detection and recognition models hinges on extensive training data, which can be both laborious and costly to obtain, especially for low-resourced languages. Conventional methods tailored for Latin characters often falter with non-Latin scripts due to challenges like character stacking, diacritics, and variable character widths without clear word boundaries. In this paper, we introduce the first Khmer scene-text dataset, featuring 1,544 expert-annotated images, including 997 indoor and 547 outdoor scenes. This diverse dataset includes flat text, raised text, poorly illuminated text, distant and partially obscured text. Annotations provide line-level text and polygonal bounding box coordinates for each scene. The benchmark includes baseline models for scene-text detection and recognition tasks, providing a robust starting point for future research endeavors. The KhmerST dataset is publicly accessible at https://gitlab.com/vannkinhnom123/khmerst.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2408.00355.pdf' target='_blank'>https://arxiv.org/pdf/2408.00355.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Xie, Qian Qiao, Jun Gao, Tianxiang Wu, Jiaqing Fan, Yue Zhang, Jielei Zhang, Huyang Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00355">DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved Denoising Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>More and more end-to-end text spotting methods based on Transformer architecture have demonstrated superior performance. These methods utilize a bipartite graph matching algorithm to perform one-to-one optimal matching between predicted objects and actual objects. However, the instability of bipartite graph matching can lead to inconsistent optimization targets, thereby affecting the training performance of the model. Existing literature applies denoising training to solve the problem of bipartite graph matching instability in object detection tasks. Unfortunately, this denoising training method cannot be directly applied to text spotting tasks, as these tasks need to perform irregular shape detection tasks and more complex text recognition tasks than classification. To address this issue, we propose a novel denoising training method (DNTextSpotter) for arbitrary-shaped text spotting. Specifically, we decompose the queries of the denoising part into noised positional queries and noised content queries. We use the four Bezier control points of the Bezier center curve to generate the noised positional queries. For the noised content queries, considering that the output of the text in a fixed positional order is not conducive to aligning position with content, we employ a masked character sliding method to initialize noised content queries, thereby assisting in the alignment of text content and position. To improve the model's perception of the background, we further utilize an additional loss function for background characters classification in the denoising training part.Although DNTextSpotter is conceptually simple, it outperforms the state-of-the-art methods on four benchmarks (Total-Text, SCUT-CTW1500, ICDAR15, and Inverse-Text), especially yielding an improvement of 11.3% against the best approach in Inverse-Text dataset.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2312.11923.pdf' target='_blank'>https://arxiv.org/pdf/2312.11923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaomeng Yang, Zhi Qiao, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11923">IPAD: Iterative, Parallel, and Diffusion-based Network for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, scene text recognition has attracted more and more attention due to its diverse applications. Most state-of-the-art methods adopt an encoder-decoder framework with the attention mechanism, autoregressively generating text from left to right. Despite the convincing performance, this sequential decoding strategy constrains the inference speed. Conversely, non-autoregressive models provide faster, simultaneous predictions but often sacrifice accuracy. Although utilizing an explicit language model can improve performance, it burdens the computational load. Besides, separating linguistic knowledge from vision information may harm the final prediction. In this paper, we propose an alternative solution that uses a parallel and iterative decoder that adopts an easy-first decoding strategy. Furthermore, we regard text recognition as an image-based conditional text generation task and utilize the discrete diffusion strategy, ensuring exhaustive exploration of bidirectional contextual information. Extensive experiments demonstrate that the proposed approach achieves superior results on the benchmark datasets, including both Chinese and English text images.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2309.01380.pdf' target='_blank'>https://arxiv.org/pdf/2309.01380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumya Jahagirdar, Minesh Mathew, Dimosthenis Karatzas, C. V. Jawahar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01380">Understanding Video Scenes through Text: Insights from Text-based Video Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Researchers have extensively studied the field of vision and language, discovering that both visual and textual content is crucial for understanding scenes effectively. Particularly, comprehending text in videos holds great significance, requiring both scene text understanding and temporal reasoning. This paper focuses on exploring two recently introduced datasets, NewsVideoQA and M4-ViteVQA, which aim to address video question answering based on textual content. The NewsVideoQA dataset contains question-answer pairs related to the text in news videos, while M4-ViteVQA comprises question-answer pairs from diverse categories like vlogging, traveling, and shopping. We provide an analysis of the formulation of these datasets on various levels, exploring the degree of visual understanding and multi-frame comprehension required for answering the questions. Additionally, the study includes experimentation with BERT-QA, a text-only model, which demonstrates comparable performance to the original methods on both datasets, indicating the shortcomings in the formulation of these datasets. Furthermore, we also look into the domain adaptation aspect by examining the effectiveness of training on M4-ViteVQA and evaluating on NewsVideoQA and vice-versa, thereby shedding light on the challenges and potential benefits of out-of-domain training.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2308.13173.pdf' target='_blank'>https://arxiv.org/pdf/2308.13173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mei-Yuh Hwang, Yangyang Shi, Ankit Ramchandani, Guan Pang, Praveen Krishnan, Lucas Kabela, Frank Seide, Samyak Datta, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13173">DISGO: Automatic End-to-End Evaluation for Scene Text OCR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper discusses the challenges of optical character recognition (OCR) on natural scenes, which is harder than OCR on documents due to the wild content and various image backgrounds. We propose to uniformly use word error rates (WER) as a new measurement for evaluating scene-text OCR, both end-to-end (e2e) performance and individual system component performances. Particularly for the e2e metric, we name it DISGO WER as it considers Deletion, Insertion, Substitution, and Grouping/Ordering errors. Finally we propose to utilize the concept of super blocks to automatically compute BLEU scores for e2e OCR machine translation. The small SCUT public test set is used to demonstrate WER performance by a modularized OCR system.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2307.03948.pdf' target='_blank'>https://arxiv.org/pdf/2307.03948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>George Tom, Minesh Mathew, Sergi Garcia, Dimosthenis Karatzas, C. V. Jawahar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.03948">Reading Between the Lanes: Text VideoQA on the Road</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text and signs around roads provide crucial information for drivers, vital for safe navigation and situational awareness. Scene text recognition in motion is a challenging problem, while textual cues typically appear for a short time span, and early detection at a distance is necessary. Systems that exploit such information to assist the driver should not only extract and incorporate visual and textual cues from the video stream but also reason over time. To address this issue, we introduce RoadTextVQA, a new dataset for the task of video question answering (VideoQA) in the context of driver assistance. RoadTextVQA consists of $3,222$ driving videos collected from multiple countries, annotated with $10,500$ questions, all based on text or road signs present in the driving videos. We assess the performance of state-of-the-art video question answering models on our RoadTextVQA dataset, highlighting the significant potential for improvement in this domain and the usefulness of the dataset in advancing research on in-vehicle support systems and text-aware multimodal question answering. The dataset is available at http://cvit.iiit.ac.in/research/projects/cvit-projects/roadtextvqa
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2211.05588.pdf' target='_blank'>https://arxiv.org/pdf/2211.05588.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumya Jahagirdar, Minesh Mathew, Dimosthenis Karatzas, C. V. Jawahar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.05588">Watching the News: Towards VideoQA Models that can Read</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Question Answering methods focus on commonsense reasoning and visual cognition of objects or persons and their interactions over time. Current VideoQA approaches ignore the textual information present in the video. Instead, we argue that textual information is complementary to the action and provides essential contextualisation cues to the reasoning process. To this end, we propose a novel VideoQA task that requires reading and understanding the text in the video. To explore this direction, we focus on news videos and require QA systems to comprehend and answer questions about the topics presented by combining visual and textual cues in the video. We introduce the ``NewsVideoQA'' dataset that comprises more than $8,600$ QA pairs on $3,000+$ news videos obtained from diverse news channels from around the world. We demonstrate the limitations of current Scene Text VQA and VideoQA methods and propose ways to incorporate scene text information into VideoQA methods.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2410.21721.pdf' target='_blank'>https://arxiv.org/pdf/2410.21721.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanhita Pathak, Vinay Kaushik, Brejesh Lall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21721">DiffSTR: Controlled Diffusion Models for Scene Text Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To prevent unauthorized use of text in images, Scene Text Removal (STR) has become a crucial task. It focuses on automatically removing text and replacing it with a natural, text-less background while preserving significant details such as texture, color, and contrast. Despite its importance in privacy protection, STR faces several challenges, including boundary artifacts, inconsistent texture and color, and preserving correct shadows. Most STR approaches estimate a text region mask to train a model, solving for image translation or inpainting to generate a text-free image. Thus, the quality of the generated image depends on the accuracy of the inpainting mask and the generator's capability. In this work, we leverage the superior capabilities of diffusion models in generating high-quality, consistent images to address the STR problem. We introduce a ControlNet diffusion model, treating STR as an inpainting task. To enhance the model's robustness, we develop a mask pretraining pipeline to condition our diffusion model. This involves training a masked autoencoder (MAE) using a combination of box masks and coarse stroke masks, and fine-tuning it using masks derived from our novel segmentation-based mask refinement framework. This framework iteratively refines an initial mask and segments it using the SLIC and Hierarchical Feature Selection (HFS) algorithms to produce an accurate final text mask. This improves mask prediction and utilizes rich textural information in natural scene images to provide accurate inpainting masks. Experiments on the SCUT-EnsText and SCUT-Syn datasets demonstrate that our method significantly outperforms existing state-of-the-art techniques.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2409.17747.pdf' target='_blank'>https://arxiv.org/pdf/2409.17747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chihiro Noguchi, Shun Fukuda, Shoichiro Mihara, Masao Yamanaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17747">Text Image Generation for Low-Resource Languages with Dual Translation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition in low-resource languages frequently faces challenges due to the limited availability of training datasets derived from real-world scenes. This study proposes a novel approach that generates text images in low-resource languages by emulating the style of real text images from high-resource languages. Our approach utilizes a diffusion model that is conditioned on binary states: ``synthetic'' and ``real.'' The training of this model involves dual translation tasks, where it transforms plain text images into either synthetic or real text images, based on the binary states. This approach not only effectively differentiates between the two domains but also facilitates the model's explicit recognition of characters in the target language. Furthermore, to enhance the accuracy and variety of generated text images, we introduce two guidance techniques: Fidelity-Diversity Balancing Guidance and Fidelity Enhancement Guidance. Our experimental results demonstrate that the text images generated by our proposed framework can significantly improve the performance of scene text recognition models for low-resource languages.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2405.09125.pdf' target='_blank'>https://arxiv.org/pdf/2405.09125.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Honghui Chen, Yuhang Qiu, Jiabao Wang, Pingping Chen, Nam Ling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09125">HAAP: Vision-context Hierarchical Attention Autoregressive with Adaptive Permutation for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Internal Language Model (LM)-based methods use permutation language modeling (PLM) to solve the error correction caused by conditional independence in external LM-based methods. However, random permutations of human interference cause fit oscillations in the model training, and Iterative Refinement (IR) operation to improve multimodal information decoupling also introduces additional overhead. To address these issues, this paper proposes the Hierarchical Attention autoregressive Model with Adaptive Permutation (HAAP) to enhance the location-context-image interaction capability, improving autoregressive generalization with internal LM. First, we propose Implicit Permutation Neurons (IPN) to generate adaptive attention masks to dynamically exploit token dependencies. The adaptive masks increase the diversity of training data and prevent model dependency on a specific order. It reduces the training overhead of PLM while avoiding training fit oscillations. Second, we develop Cross-modal Hierarchical Attention mechanism (CHA) to couple context and image features. This processing establishes rich positional semantic dependencies between context and image while avoiding IR. Extensive experimental results show the proposed HAAP achieves state-of-the-art (SOTA) performance in terms of accuracy, complexity, and latency on several datasets.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2403.09288.pdf' target='_blank'>https://arxiv.org/pdf/2403.09288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixuan Shen, Haonan Luo, Sijia Li, Tianrui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09288">Adversarial Training with OCR Modality Perturbation for Scene-Text Visual Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene-Text Visual Question Answering (ST-VQA) aims to understand scene text in images and answer questions related to the text content. Most existing methods heavily rely on the accuracy of Optical Character Recognition (OCR) systems, and aggressive fine-tuning based on limited spatial location information and erroneous OCR text information often leads to inevitable overfitting. In this paper, we propose a multimodal adversarial training architecture with spatial awareness capabilities. Specifically, we introduce an Adversarial OCR Enhancement (AOE) module, which leverages adversarial training in the embedding space of OCR modality to enhance fault-tolerant representation of OCR texts, thereby reducing noise caused by OCR errors. Simultaneously, We add a Spatial-Aware Self-Attention (SASA) mechanism to help the model better capture the spatial relationships among OCR tokens. Various experiments demonstrate that our method achieves significant performance improvements on both the ST-VQA and TextVQA datasets and provides a novel paradigm for multimodal adversarial training.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2402.11540.pdf' target='_blank'>https://arxiv.org/pdf/2402.11540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longhuang Wu, Shangxuan Tian, Youxin Wang, Pengfei Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11540">CPN: Complementary Proposal Network for Unconstrained Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing methods for scene text detection can be divided into two paradigms: segmentation-based and anchor-based. While Segmentation-based methods are well-suited for irregular shapes, they struggle with compact or overlapping layouts. Conversely, anchor-based approaches excel for complex layouts but suffer from irregular shapes. To strengthen their merits and overcome their respective demerits, we propose a Complementary Proposal Network (CPN) that seamlessly and parallelly integrates semantic and geometric information for superior performance. The CPN comprises two efficient networks for proposal generation: the Deformable Morphology Semantic Network, which generates semantic proposals employing an innovative deformable morphological operator, and the Balanced Region Proposal Network, which produces geometric proposals with pre-defined anchors. To further enhance the complementarity, we introduce an Interleaved Feature Attention module that enables semantic and geometric features to interact deeply before proposal generation. By leveraging both complementary proposals and features, CPN outperforms state-of-the-art approaches with significant margins under comparable computation cost. Specifically, our approach achieves improvements of 3.6%, 1.3% and 1.0% on challenging benchmarks ICDAR19-ArT, IC15, and MSRA-TD500, respectively. Code for our method will be released.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2312.15690.pdf' target='_blank'>https://arxiv.org/pdf/2312.15690.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wang, Huabing Zhou, Yanduo Zhang, Tao Lu, Jiayi Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15690">Word length-aware text spotting: Enhancing detection and recognition in dense text image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text spotting is essential in various computer vision applications, enabling extracting and interpreting textual information from images. However, existing methods often neglect the spatial semantics of word images, leading to suboptimal detection recall rates for long and short words within long-tailed word length distributions that exist prominently in dense scenes. In this paper, we present WordLenSpotter, a novel word length-aware spotter for scene text image detection and recognition, improving the spotting capabilities for long and short words, particularly in the tail data of dense text images. We first design an image encoder equipped with a dilated convolutional fusion module to integrate multiscale text image features effectively. Then, leveraging the Transformer framework, we synergistically optimize text detection and recognition accuracy after iteratively refining text region image features using the word length prior. Specially, we design a Spatial Length Predictor module (SLP) using character count prior tailored to different word lengths to constrain the regions of interest effectively. Furthermore, we introduce a specialized word Length-aware Segmentation (LenSeg) proposal head, enhancing the network's capacity to capture the distinctive features of long and short terms within categories characterized by long-tailed distributions. Comprehensive experiments on public datasets and our dense text spotting dataset DSTD1500 demonstrate the superiority of our proposed methods, particularly in dense text image detection and recognition tasks involving long-tailed word length distributions encompassing a range of long and short words.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2311.09759.pdf' target='_blank'>https://arxiv.org/pdf/2311.09759.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chihiro Noguchi, Shun Fukuda, Masao Yamanaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.09759">Scene Text Image Super-resolution based on Text-conditional Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Text Image Super-resolution (STISR) has recently achieved great success as a preprocessing method for scene text recognition. STISR aims to transform blurred and noisy low-resolution (LR) text images in real-world settings into clear high-resolution (HR) text images suitable for scene text recognition. In this study, we leverage text-conditional diffusion models (DMs), known for their impressive text-to-image synthesis capabilities, for STISR tasks. Our experimental results revealed that text-conditional DMs notably surpass existing STISR methods. Especially when texts from LR text images are given as input, the text-conditional DMs are able to produce superior quality super-resolution text images. Utilizing this capability, we propose a novel framework for synthesizing LR-HR paired text image datasets. This framework consists of three specialized text-conditional DMs, each dedicated to text image synthesis, super-resolution, and image degradation. These three modules are vital for synthesizing distinct LR and HR paired images, which are more suitable for training STISR methods. Our experiments confirmed that these synthesized image pairs significantly enhance the performance of STISR methods in the TextZoom evaluation.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2307.15991.pdf' target='_blank'>https://arxiv.org/pdf/2307.15991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prateek Keserwani, Taveena Lotey, Rohit Keshari, Partha Pratim Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15991">Separate Scene Text Detector for Unseen Scripts is Not All You Need</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text detection in the wild is a well-known problem that becomes more challenging while handling multiple scripts. In the last decade, some scripts have gained the attention of the research community and achieved good detection performance. However, many scripts are low-resourced for training deep learning-based scene text detectors. It raises a critical question: Is there a need for separate training for new scripts? It is an unexplored query in the field of scene text detection. This paper acknowledges this problem and proposes a solution to detect scripts not present during training. In this work, the analysis has been performed to understand cross-script text detection, i.e., trained on one and tested on another. We found that the identical nature of text annotation (word-level/line-level) is crucial for better cross-script text detection. The different nature of text annotation between scripts degrades cross-script text detection performance. Additionally, for unseen script detection, the proposed solution utilizes vector embedding to map the stroke information of text corresponding to the script category. The proposed method is validated with a well-known multi-lingual scene text dataset under a zero-shot setting. The results show the potential of the proposed method for unseen script detection in natural images.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2306.10804.pdf' target='_blank'>https://arxiv.org/pdf/2306.10804.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanzhi Zhu, Zhaohai Li, Tianwei Wang, Mengchao He, Cong Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10804">Conditional Text Image Generation with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current text recognition systems, including those for handwritten scripts and scene text, have relied heavily on image synthesis and augmentation, since it is difficult to realize real-world complexity and diversity through collecting and annotating enough real text images. In this paper, we explore the problem of text image generation, by taking advantage of the powerful abilities of Diffusion Models in generating photo-realistic and diverse image samples with given conditions, and propose a method called Conditional Text Image Generation with Diffusion Models (CTIG-DM for short). To conform to the characteristics of text images, we devise three conditions: image condition, text condition, and style condition, which can be used to control the attributes, contents, and styles of the samples in the image generation process. Specifically, four text image generation modes, namely: (1) synthesis mode, (2) augmentation mode, (3) recovery mode, and (4) imitation mode, can be derived by combining and configuring these three conditions. Extensive experiments on both handwritten and scene text demonstrate that the proposed CTIG-DM is able to produce image samples that simulate real-world complexity and diversity, and thus can boost the performance of existing text recognizers. Besides, CTIG-DM shows its appealing potential in domain adaptation and generating images containing Out-Of-Vocabulary (OOV) words.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2503.15639.pdf' target='_blank'>https://arxiv.org/pdf/2503.15639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ritabrata Chakraborty, Shivakumara Palaiahnakote, Umapada Pal, Cheng-Lin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15639">A Context-Driven Training-Free Network for Lightweight Scene Text Segmentation and Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern scene text recognition systems often depend on large end-to-end architectures that require extensive training and are prohibitively expensive for real-time scenarios. In such cases, the deployment of heavy models becomes impractical due to constraints on memory, computational resources, and latency. To address these challenges, we propose a novel, training-free plug-and-play framework that leverages the strengths of pre-trained text recognizers while minimizing redundant computations. Our approach uses context-based understanding and introduces an attention-based segmentation stage, which refines candidate text regions at the pixel level, improving downstream recognition. Instead of performing traditional text detection that follows a block-level comparison between feature map and source image and harnesses contextual information using pretrained captioners, allowing the framework to generate word predictions directly from scene context.Candidate texts are semantically and lexically evaluated to get a final score. Predictions that meet or exceed a pre-defined confidence threshold bypass the heavier process of end-to-end text STR profiling, ensuring faster inference and cutting down on unnecessary computations. Experiments on public benchmarks demonstrate that our paradigm achieves performance on par with state-of-the-art systems, yet requires substantially fewer resources.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2405.09942.pdf' target='_blank'>https://arxiv.org/pdf/2405.09942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siliang Ma, Yong Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09942">FPDIoU Loss: A Loss Function for Efficient Bounding Box Regression of Rotated Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bounding box regression is one of the important steps of object detection. However, rotation detectors often involve a more complicated loss based on SkewIoU which is unfriendly to gradient-based training. Most of the existing loss functions for rotated object detection calculate the difference between two bounding boxes only focus on the deviation of area or each points distance (e.g., $\mathcal{L}_{Smooth-\ell 1}$, $\mathcal{L}_{RotatedIoU}$ and $\mathcal{L}_{PIoU}$). The calculation process of some loss functions is extremely complex (e.g. $\mathcal{L}_{KFIoU}$). In order to improve the efficiency and accuracy of bounding box regression for rotated object detection, we proposed a novel metric for arbitrary shapes comparison based on minimum points distance, which takes most of the factors from existing loss functions for rotated object detection into account, i.e., the overlap or nonoverlapping area, the central points distance and the rotation angle. We also proposed a loss function called $\mathcal{L}_{FPDIoU}$ based on four points distance for accurate bounding box regression focusing on faster and high quality anchor boxes. In the experiments, $FPDIoU$ loss has been applied to state-of-the-art rotated object detection (e.g., RTMDET, H2RBox) models training with three popular benchmarks of rotated object detection including DOTA, DIOR, HRSC2016 and two benchmarks of arbitrary orientation scene text detection including ICDAR 2017 RRC-MLT and ICDAR 2019 RRC-MLT, which achieves better performance than existing loss functions.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2311.00734.pdf' target='_blank'>https://arxiv.org/pdf/2311.00734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua Santoso, Christian Simon, Williem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00734">On Manipulating Scene Text in the Wild with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have gained attention for image editing yielding impressive results in text-to-image tasks. On the downside, one might notice that generated images of stable diffusion models suffer from deteriorated details. This pitfall impacts image editing tasks that require information preservation e.g., scene text editing. As a desired result, the model must show the capability to replace the text on the source image to the target text while preserving the details e.g., color, font size, and background. To leverage the potential of diffusion models, in this work, we introduce Diffusion-BasEd Scene Text manipulation Network so-called DBEST. Specifically, we design two adaptation strategies, namely one-shot style adaptation and text-recognition guidance. In experiments, we thoroughly assess and compare our proposed method against state-of-the-arts on various scene text datasets, then provide extensive ablation studies for each granularity to analyze our performance gain. Also, we demonstrate the effectiveness of our proposed method to synthesize scene text indicated by competitive Optical Character Recognition (OCR) accuracy. Our method achieves 94.15% and 98.12% on COCO-text and ICDAR2013 datasets for character-level evaluation.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2308.00295.pdf' target='_blank'>https://arxiv.org/pdf/2308.00295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shamanthak Hegde, Soumya Jahagirdar, Shankar Gangisetty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00295">Making the V in Text-VQA Matter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-based VQA aims at answering questions by reading the text present in the images. It requires a large amount of scene-text relationship understanding compared to the VQA task. Recent studies have shown that the question-answer pairs in the dataset are more focused on the text present in the image but less importance is given to visual features and some questions do not require understanding the image. The models trained on this dataset predict biased answers due to the lack of understanding of visual context. For example, in questions like "What is written on the signboard?", the answer predicted by the model is always "STOP" which makes the model to ignore the image. To address these issues, we propose a method to learn visual features (making V matter in TextVQA) along with the OCR features and question features using VQA dataset as external knowledge for Text-based VQA. Specifically, we combine the TextVQA dataset and VQA dataset and train the model on this combined dataset. Such a simple, yet effective approach increases the understanding and correlation between the image features and text present in the image, which helps in the better answering of questions. We further test the model on different datasets and compare their qualitative and quantitative results.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2306.11351.pdf' target='_blank'>https://arxiv.org/pdf/2306.11351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Xin, Guoming Tang, Donglong Chen, Rumin Zhang, Teng Liang, Ray C. C. Cheung, Cetin Kaya Koc
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11351">A Versatility-Performance Balanced Hardware Architecture for Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting and extracting textual information from natural scene images needs Scene Text Detection (STD) algorithms. Fully Convolutional Neural Networks (FCNs) are usually utilized as the backbone model to extract features in these instance segmentation based STD algorithms. FCNs naturally come with high computational complexity. Furthermore, to keep up with the growing variety of models, flexible architectures are needed. In order to accelerate various STD algorithms efficiently, a versatility-performance balanced hardware architecture is proposed, together with a simple but efficient way of configuration. This architecture is able to compute different FCN models without hardware redesign. The optimization is focused on hardware with finely designed computing modules, while the versatility of different network reconfigurations is achieved by microcodes instead of a strenuously designed compiler. Multiple parallel techniques at different levels and several complexity-reduction methods are explored to speed up the FCN computation. Results from implementation show that, given the same tasks, the proposed system achieves a better throughput compared with the studied GPU. Particularly, our system reduces the comprehensive Operation Expense (OpEx) at GPU by 46\%, while the power efficiency is enhanced by 32\%. This work has been deployed in commercial applications and provided stable consumer text detection services.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2303.06946.pdf' target='_blank'>https://arxiv.org/pdf/2303.06946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuangping Huang, Yu Luo, Zhenzhou Zhuang, Jin-Gang Yu, Mengchao He, Yongpan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06946">Context-Aware Selective Label Smoothing for Calibrating Sequence Recognition Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the success of deep neural network (DNN) on sequential data (i.e., scene text and speech) recognition, it suffers from the over-confidence problem mainly due to overfitting in training with the cross-entropy loss, which may make the decision-making less reliable. Confidence calibration has been recently proposed as one effective solution to this problem. Nevertheless, the majority of existing confidence calibration methods aims at non-sequential data, which is limited if directly applied to sequential data since the intrinsic contextual dependency in sequences or the class-specific statistical prior is seldom exploited. To the end, we propose a Context-Aware Selective Label Smoothing (CASLS) method for calibrating sequential data. The proposed CASLS fully leverages the contextual dependency in sequences to construct confusion matrices of contextual prediction statistics over different classes. Class-specific error rates are then used to adjust the weights of smoothing strength in order to achieve adaptive calibration. Experimental results on sequence recognition tasks, including scene text recognition and speech recognition, demonstrate that our method can achieve the state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2302.01540.pdf' target='_blank'>https://arxiv.org/pdf/2302.01540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongsheng Xu, Qingbao Huang, Xingmao Zhang, Haonan Cheng, Feng Shuang, Yi Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01540">DEVICE: Depth and Visual Concepts Aware Transformer for OCR-based Image Captioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>OCR-based image captioning is an important but under-explored task, aiming to generate descriptions containing visual objects and scene text. Recent studies have made encouraging progress, but they are still suffering from a lack of overall understanding of scenes and generating inaccurate captions. One possible reason is that current studies mainly focus on constructing the plane-level geometric relationship of scene text without depth information. This leads to insufficient scene text relational reasoning so that models may describe scene text inaccurately. The other possible reason is that existing methods fail to generate fine-grained descriptions of some visual objects. In addition, they may ignore essential visual objects, leading to the scene text belonging to these ignored objects not being utilized. To address the above issues, we propose a Depth and Visual Concepts Aware Transformer (DEVICE) for OCR-based image captinong. Concretely, to construct three-dimensional geometric relations, we introduce depth information and propose a depth-enhanced feature updating module to ameliorate OCR token features. To generate more precise and comprehensive captions, we introduce semantic features of detected visual concepts as auxiliary information, and propose a semantic-guided alignment module to improve the model's ability to utilize visual concepts. Our DEVICE is capable of comprehending scenes more comprehensively and boosting the accuracy of described visual entities. Sufficient experiments demonstrate the effectiveness of our proposed DEVICE, which outperforms state-of-the-art models on the TextCaps test set.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2508.01153.pdf' target='_blank'>https://arxiv.org/pdf/2508.01153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiahan Yang, Hui Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01153">TEACH: Text Encoding as Curriculum Hints for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Text Recognition (STR) remains a challenging task due to complex visual appearances and limited semantic priors. We propose TEACH, a novel training paradigm that injects ground-truth text into the model as auxiliary input and progressively reduces its influence during training. By encoding target labels into the embedding space and applying loss-aware masking, TEACH simulates a curriculum learning process that guides the model from label-dependent learning to fully visual recognition. Unlike language model-based approaches, TEACH requires no external pretraining and introduces no inference overhead. It is model-agnostic and can be seamlessly integrated into existing encoder-decoder frameworks. Extensive experiments across multiple public benchmarks show that models trained with TEACH achieve consistently improved accuracy, especially under challenging conditions, validating its robustness and general applicability.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2505.18479.pdf' target='_blank'>https://arxiv.org/pdf/2505.18479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li-Syun Hsiung, Jun-Kai Tu, Kuan-Wu Chu, Yu-Hsuan Chiu, Yan-Tsung Peng, Sheng-Luen Chung, Gee-Sern Jison Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18479">Syn3DTxt: Embedding 3D Cues for Scene Text Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study aims to investigate the challenge of insufficient three-dimensional context in synthetic datasets for scene text rendering. Although recent advances in diffusion models and related techniques have improved certain aspects of scene text generation, most existing approaches continue to rely on 2D data, sourcing authentic training examples from movie posters and book covers, which limits their ability to capture the complex interactions among spatial layout and visual effects in real-world scenes. In particular, traditional 2D datasets do not provide the necessary geometric cues for accurately embedding text into diverse backgrounds. To address this limitation, we propose a novel standard for constructing synthetic datasets that incorporates surface normals to enrich three-dimensional scene characteristic. By adding surface normals to conventional 2D data, our approach aims to enhance the representation of spatial relationships and provide a more robust foundation for future scene text rendering methods. Extensive experiments demonstrate that datasets built under this new standard offer improved geometric context, facilitating further advancements in text rendering under complex 3D-spatial conditions.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2504.13690.pdf' target='_blank'>https://arxiv.org/pdf/2504.13690.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Usama, Syeda Aishah Asim, Syed Bilal Ali, Syed Talal Wasim, Umair Bin Mansoor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13690">Analysing the Robustness of Vision-Language-Models to Common Corruptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have demonstrated impressive capabilities in understanding and reasoning about visual and textual content. However, their robustness to common image corruptions remains under-explored. In this work, we present the first comprehensive analysis of VLM robustness across 19 corruption types from the ImageNet-C benchmark, spanning four categories: noise, blur, weather, and digital distortions. We introduce two new benchmarks, TextVQA-C and GQA-C, to systematically evaluate how corruptions affect scene text understanding and object-based reasoning, respectively. Our analysis reveals that transformer-based VLMs exhibit distinct vulnerability patterns across tasks: text recognition deteriorates most severely under blur and snow corruptions, while object reasoning shows higher sensitivity to corruptions such as frost and impulse noise. We connect these observations to the frequency-domain characteristics of different corruptions, revealing how transformers' inherent bias toward low-frequency processing explains their differential robustness patterns. Our findings provide valuable insights for developing more corruption-robust vision-language models for real-world applications.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2504.00410.pdf' target='_blank'>https://arxiv.org/pdf/2504.00410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongwoo Park, Suk Pil Ko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00410">NCAP: Scene Text Image Super-Resolution with Non-CAtegorical Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text image super-resolution (STISR) enhances the resolution and quality of low-resolution images. Unlike previous studies that treated scene text images as natural images, recent methods using a text prior (TP), extracted from a pre-trained text recognizer, have shown strong performance. However, two major issues emerge: (1) Explicit categorical priors, like TP, can negatively impact STISR if incorrect. We reveal that these explicit priors are unstable and propose replacing them with Non-CAtegorical Prior (NCAP) using penultimate layer representations. (2) Pre-trained recognizers used to generate TP struggle with low-resolution images. To address this, most studies jointly train the recognizer with the STISR network to bridge the domain gap between low- and high-resolution images, but this can cause an overconfidence phenomenon in the prior modality. We highlight this issue and propose a method to mitigate it by mixing hard and soft labels. Experiments on the TextZoom dataset demonstrate an improvement by 3.5%, while our method significantly enhances generalization performance by 14.8\% across four text recognition datasets. Our method generalizes to all TP-guided STISR networks.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2502.09026.pdf' target='_blank'>https://arxiv.org/pdf/2502.09026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Wei, Xiuzhuang Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09026">Billet Number Recognition Based on Test-Time Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>During the steel billet production process, it is essential to recognize machine-printed or manually written billet numbers on moving billets in real-time. To address the issue of low recognition accuracy for existing scene text recognition methods, caused by factors such as image distortions and distribution differences between training and test data, we propose a billet number recognition method that integrates test-time adaptation with prior knowledge. First, we introduce a test-time adaptation method into a model that uses the DB network for text detection and the SVTR network for text recognition. By minimizing the model's entropy during the testing phase, the model can adapt to the distribution of test data without the need for supervised fine-tuning. Second, we leverage the billet number encoding rules as prior knowledge to assess the validity of each recognition result. Invalid results, which do not comply with the encoding rules, are replaced. Finally, we introduce a validation mechanism into the CTC algorithm using prior knowledge to address its limitations in recognizing damaged characters. Experimental results on real datasets, including both machine-printed billet numbers and handwritten billet numbers, show significant improvements in evaluation metrics, validating the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2502.02951.pdf' target='_blank'>https://arxiv.org/pdf/2502.02951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Madhuri Latha Madaka, Chakravarthy Bhagvati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02951">VQA-Levels: A Hierarchical Approach for Classifying Questions in VQA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing datasets for Visual Question Answering (VQA) is a difficult and complex task that requires NLP for parsing and computer vision for analysing the relevant aspects of the image for answering the question asked. Several benchmark datasets have been developed by researchers but there are many issues with using them for methodical performance tests. This paper proposes a new benchmark dataset -- a pilot version called VQA-Levels is ready now -- for testing VQA systems systematically and assisting researchers in advancing the field. The questions are classified into seven levels ranging from direct answers based on low-level image features (without needing even a classifier) to those requiring high-level abstraction of the entire image content. The questions in the dataset exhibit one or many of ten properties. Each is categorised into a specific level from 1 to 7. Levels 1 - 3 are directly on the visual content while the remaining levels require extra knowledge about the objects in the image. Each question generally has a unique one or two-word answer. The questions are 'natural' in the sense that a human is likely to ask such a question when seeing the images. An example question at Level 1 is, ``What is the shape of the red colored region in the image?" while at Level 7, it is, ``Why is the man cutting the paper?". Initial testing of the proposed dataset on some of the existing VQA systems reveals that their success is high on Level 1 (low level features) and Level 2 (object classification) questions, least on Level 3 (scene text) followed by Level 6 (extrapolation) and Level 7 (whole scene analysis) questions. The work in this paper will go a long way to systematically analyze VQA systems.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2501.02584.pdf' target='_blank'>https://arxiv.org/pdf/2501.02584.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miguel Carvalho, Bruno Martins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02584">Efficient Architectures for High Resolution Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This work introduces Pheye, a novel architecture that efficiently processes high-resolution images while training fewer parameters than similarly sized VLMs. Notably, Pheye achieves a high efficiency while maintaining strong performance, particularly in tasks that demand fine-grained image understanding and/or the handling of scene-text.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2411.11150.pdf' target='_blank'>https://arxiv.org/pdf/2411.11150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raihan Kabir, Naznin Haque, Md Saiful Islam, Marium-E-Jannat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11150">A Comprehensive Survey on Visual Question Answering Datasets and Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual question answering (VQA) refers to the problem where, given an image and a natural language question about the image, a correct natural language answer has to be generated. A VQA model has to demonstrate both the visual understanding of the image and the semantic understanding of the question, demonstrating reasoning capability. Since the inception of this field, a plethora of VQA datasets and models have been published. In this article, we meticulously analyze the current state of VQA datasets and models, while cleanly dividing them into distinct categories and then summarizing the methodologies and characteristics of each category. We divide VQA datasets into four categories: (1) available datasets that contain a rich collection of authentic images, (2) synthetic datasets that contain only synthetic images produced through artificial means, (3) diagnostic datasets that are specially designed to test model performance in a particular area, e.g., understanding the scene text, and (4) KB (Knowledge-Based) datasets that are designed to measure a model's ability to utilize outside knowledge. Concurrently, we explore six main paradigms of VQA models: fusion, where we discuss different methods of fusing information between visual and textual modalities; attention, the technique of using information from one modality to filter information from another; external knowledge base, where we discuss different models utilizing outside information; composition or reasoning, where we analyze techniques to answer advanced questions that require complex reasoning steps; explanation, which is the process of generating visual and textual descriptions to verify sound reasoning; and graph models, which encode and manipulate relationships through nodes in a graph. We also discuss some miscellaneous topics, such as scene text understanding, counting, and bias reduction.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2411.00355.pdf' target='_blank'>https://arxiv.org/pdf/2411.00355.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengcheng Li, Fei Chao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00355">TextDestroyer: A Training- and Annotation-Free Diffusion Method for Destroying Anomal Text from Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose TextDestroyer, the first training- and annotation-free method for scene text destruction using a pre-trained diffusion model. Existing scene text removal models require complex annotation and retraining, and may leave faint yet recognizable text information, compromising privacy protection and content concealment. TextDestroyer addresses these issues by employing a three-stage hierarchical process to obtain accurate text masks. Our method scrambles text areas in the latent start code using a Gaussian distribution before reconstruction. During the diffusion denoising process, self-attention key and value are referenced from the original latent to restore the compromised background. Latent codes saved at each inversion step are used for replacement during reconstruction, ensuring perfect background restoration. The advantages of TextDestroyer include: (1) it eliminates labor-intensive data annotation and resource-intensive training; (2) it achieves more thorough text destruction, preventing recognizable traces; and (3) it demonstrates better generalization capabilities, performing well on both real-world scenes and generated images.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2403.08007.pdf' target='_blank'>https://arxiv.org/pdf/2403.08007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harsh Lunia, Ajoy Mondal, C V Jawahar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08007">IndicSTR12: A Dataset for Indic Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The importance of Scene Text Recognition (STR) in today's increasingly digital world cannot be overstated. Given the significance of STR, data intensive deep learning approaches that auto-learn feature mappings have primarily driven the development of STR solutions. Several benchmark datasets and substantial work on deep learning models are available for Latin languages to meet this need. On more complex, syntactically and semantically, Indian languages spoken and read by 1.3 billion people, there is less work and datasets available. This paper aims to address the Indian space's lack of a comprehensive dataset by proposing the largest and most comprehensive real dataset - IndicSTR12 - and benchmarking STR performance on 12 major Indian languages. A few works have addressed the same issue, but to the best of our knowledge, they focused on a small number of Indian languages. The size and complexity of the proposed dataset are comparable to those of existing Latin contemporaries, while its multilingualism will catalyse the development of robust text detection and recognition models. It was created specifically for a group of related languages with different scripts. The dataset contains over 27000 word-images gathered from various natural scenes, with over 1000 word-images for each language. Unlike previous datasets, the images cover a broader range of realistic conditions, including blur, illumination changes, occlusion, non-iconic texts, low resolution, perspective text etc. Along with the new dataset, we provide a high-performing baseline on three models - PARSeq, CRNN, and STARNet.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2403.07518.pdf' target='_blank'>https://arxiv.org/pdf/2403.07518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuhua Ren, Hengcan Shi, Jin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07518">Open-Vocabulary Scene Text Recognition via Pseudo-Image Labeling and Margin Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition is an important and challenging task in computer vision. However, most prior works focus on recognizing pre-defined words, while there are various out-of-vocabulary (OOV) words in real-world applications.
  In this paper, we propose a novel open-vocabulary text recognition framework, Pseudo-OCR, to recognize OOV words. The key challenge in this task is the lack of OOV training data. To solve this problem, we first propose a pseudo label generation module that leverages character detection and image inpainting to produce substantial pseudo OOV training data from real-world images. Unlike previous synthetic data, our pseudo OOV data contains real characters and backgrounds to simulate real-world applications. Secondly, to reduce noises in pseudo data, we present a semantic checking mechanism to filter semantically meaningful data. Thirdly, we introduce a quality-aware margin loss to boost the training with pseudo data. Our loss includes a margin-based part to enhance the classification ability, and a quality-aware part to penalize low-quality samples in both real and pseudo data.
  Extensive experiments demonstrate that our approach outperforms the state-of-the-art on eight datasets and achieves the first rank in the ICDAR2022 challenge.
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2311.13222.pdf' target='_blank'>https://arxiv.org/pdf/2311.13222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hasan Murad, Mohammed Eunus Ali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13222">Towards Detecting, Recognizing, and Parsing the Address Information from Bangla Signboard: A Deep Learning-based Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieving textual information from natural scene images is an active research area in the field of computer vision with numerous practical applications. Detecting text regions and extracting text from signboards is a challenging problem due to special characteristics like reflecting lights, uneven illumination, or shadows found in real-life natural scene images. With the advent of deep learning-based methods, different sophisticated techniques have been proposed for text detection and text recognition from the natural scene. Though a significant amount of effort has been devoted to extracting natural scene text for resourceful languages like English, little has been done for low-resource languages like Bangla. In this research work, we have proposed an end-to-end system with deep learning-based models for efficiently detecting, recognizing, correcting, and parsing address information from Bangla signboards. We have created manually annotated datasets and synthetic datasets to train signboard detection, address text detection, address text recognition, address text correction, and address text parser models. We have conducted a comparative study among different CTC-based and Encoder-Decoder model architectures for Bangla address text recognition. Moreover, we have designed a novel address text correction model using a sequence-to-sequence transformer-based network to improve the performance of Bangla address text recognition model by post-correction. Finally, we have developed a Bangla address text parser using the state-of-the-art transformer-based pre-trained language model.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2310.09549.pdf' target='_blank'>https://arxiv.org/pdf/2310.09549.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mark Vincent Ty, Rowel Atienza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09549">Scene Text Recognition Models Explainability Using Local Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Explainable AI (XAI) is the study on how humans can be able to understand the cause of a model's prediction. In this work, the problem of interest is Scene Text Recognition (STR) Explainability, using XAI to understand the cause of an STR model's prediction. Recent XAI literatures on STR only provide a simple analysis and do not fully explore other XAI methods. In this study, we specifically work on data explainability frameworks, called attribution-based methods, that explain the important parts of an input data in deep learning models. However, integrating them into STR produces inconsistent and ineffective explanations, because they only explain the model in the global context. To solve this problem, we propose a new method, STRExp, to take into consideration the local explanations, i.e. the individual character prediction explanations. This is then benchmarked across different attribution-based methods on different STR datasets and evaluated across different STR models.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2309.02356.pdf' target='_blank'>https://arxiv.org/pdf/2309.02356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergi Garcia-Bordils, Dimosthenis Karatzas, MarÃ§al RusiÃ±ol
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02356">STEP -- Towards Structured Scene-Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the structured scene-text spotting task, which requires a scene-text OCR system to spot text in the wild according to a query regular expression. Contrary to generic scene text OCR, structured scene-text spotting seeks to dynamically condition both scene text detection and recognition on user-provided regular expressions. To tackle this task, we propose the Structured TExt sPotter (STEP), a model that exploits the provided text structure to guide the OCR process. STEP is able to deal with regular expressions that contain spaces and it is not bound to detection at the word-level granularity. Our approach enables accurate zero-shot structured text spotting in a wide variety of real-world reading scenarios and is solely trained on publicly available data. To demonstrate the effectiveness of our approach, we introduce a new challenging test dataset that contains several types of out-of-vocabulary structured text, reflecting important reading applications of fields such as prices, dates, serial numbers, license plates etc. We demonstrate that STEP can provide specialised OCR performance on demand in all tested scenarios.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2308.13178.pdf' target='_blank'>https://arxiv.org/pdf/2308.13178.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yibo Wang, Yunhu Ye, Yuanpeng Mao, Yanwei Yu, Yuanping Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13178">Self-supervised Scene Text Segmentation with Object-centric Layered Representations Augmented by Text Regions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text segmentation tasks have a very wide range of application values, such as image editing, style transfer, watermark removal, etc.However, existing public datasets are of poor quality of pixel-level labels that have been shown to be notoriously costly to acquire, both in terms of money and time. At the same time, when pretraining is performed on synthetic datasets, the data distribution of the synthetic datasets is far from the data distribution in the real scene. These all pose a huge challenge to the current pixel-level text segmentation algorithms.To alleviate the above problems, we propose a self-supervised scene text segmentation algorithm with layered decoupling of representations derived from the object-centric manner to segment images into texts and background. In our method, we propose two novel designs which include Region Query Module and Representation Consistency Constraints adapting to the unique properties of text as complements to Auto Encoder, which improves the network's sensitivity to texts.For this unique design, we treat the polygon-level masks predicted by the text localization model as extra input information, and neither utilize any pixel-level mask annotations for training stage nor pretrain on synthetic datasets.Extensive experiments show the effectiveness of the method proposed. On several public scene text datasets, our method outperforms the state-of-the-art unsupervised segmentation algorithms.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/1811.06295.pdf' target='_blank'>https://arxiv.org/pdf/1811.06295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Du, Chunheng Wang, Yanna Wang, Cunzhao Shi, Baihua Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1811.06295">Selective Feature Connection Mechanism: Concatenating Multi-layer CNN Features with a Feature Selector</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Different layers of deep convolutional neural networks(CNNs) can encode different-level information. High-layer features always contain more semantic information, and low-layer features contain more detail information. However, low-layer features suffer from the background clutter and semantic ambiguity. During visual recognition, the feature combination of the low-layer and high-level features plays an important role in context modulation. If directly combining the high-layer and low-layer features, the background clutter and semantic ambiguity may be caused due to the introduction of detailed information. In this paper, we propose a general network architecture to concatenate CNN features of different layers in a simple and effective way, called Selective Feature Connection Mechanism (SFCM). Low-level features are selectively linked to high-level features with a feature selector which is generated by high-level features. The proposed connection mechanism can effectively overcome the above-mentioned drawbacks. We demonstrate the effectiveness, superiority, and universal applicability of this method on multiple challenging computer vision tasks, including image classification, scene text detection, and image-to-image translation.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2507.13374.pdf' target='_blank'>https://arxiv.org/pdf/2507.13374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Dela Rosa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13374">Smart Routing for Multimodal Video Retrieval: When to Search What</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce ModaRoute, an LLM-based intelligent routing system that dynamically selects optimal modalities for multimodal video retrieval. While dense text captions can achieve 75.9% Recall@5, they require expensive offline processing and miss critical visual information present in 34% of clips with scene text not captured by ASR. By analyzing query intent and predicting information needs, ModaRoute reduces computational overhead by 41% while achieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR (speech), OCR (text), and visual indices, averaging 1.78 modalities per query versus exhaustive 3.0 modality search. Evaluation on 1.8M video clips demonstrates that intelligent routing provides a practical solution for scaling multimodal retrieval systems, reducing infrastructure costs while maintaining competitive effectiveness for real-world deployment.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2404.05967.pdf' target='_blank'>https://arxiv.org/pdf/2404.05967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masato Fujitake
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05967">JSTR: Judgment Improves Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a method for enhancing the accuracy of scene text recognition tasks by judging whether the image and text match each other. While previous studies focused on generating the recognition results from input images, our approach also considers the model's misrecognition results to understand its error tendencies, thus improving the text recognition pipeline. This method boosts text recognition accuracy by providing explicit feedback on the data that the model is likely to misrecognize by predicting correct or incorrect between the image and text. The experimental results on publicly available datasets demonstrate that our proposed method outperforms the baseline and state-of-the-art methods in scene text recognition.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2308.15996.pdf' target='_blank'>https://arxiv.org/pdf/2308.15996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masato Fujitake
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15996">DTrOCR: Decoder-only Transformer for Optical Character Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Typical text recognition methods rely on an encoder-decoder structure, in which the encoder extracts features from an image, and the decoder produces recognized text from these features. In this study, we propose a simpler and more effective method for text recognition, known as the Decoder-only Transformer for Optical Character Recognition (DTrOCR). This method uses a decoder-only Transformer to take advantage of a generative language model that is pre-trained on a large corpus. We examined whether a generative language model that has been successful in natural language processing can also be effective for text recognition in computer vision. Our experiments demonstrated that DTrOCR outperforms current state-of-the-art methods by a large margin in the recognition of printed, handwritten, and scene text in both English and Chinese.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2307.15029.pdf' target='_blank'>https://arxiv.org/pdf/2307.15029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guiqin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15029">Adaptive Segmentation Network for Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by deep convolution segmentation algorithms, scene text detectors break the performance ceiling of datasets steadily. However, these methods often encounter threshold selection bottlenecks and have poor performance on text instances with extreme aspect ratios. In this paper, we propose to automatically learn the discriminate segmentation threshold, which distinguishes text pixels from background pixels for segmentation-based scene text detectors and then further reduces the time-consuming manual parameter adjustment. Besides, we design a Global-information Enhanced Feature Pyramid Network (GE-FPN) for capturing text instances with macro size and extreme aspect ratios. Following the GE-FPN, we introduce a cascade optimization structure to further refine the text instances. Finally, together with the proposed threshold learning strategy and text detection structure, we design an Adaptive Segmentation Network (ASNet) for scene text detection. Extensive experiments are carried out to demonstrate that the proposed ASNet can achieve the state-of-the-art performance on four text detection benchmarks, i.e., ICDAR 2015, MSRA-TD500, ICDAR 2017 MLT and CTW1500. The ablation experiments also verify the effectiveness of our contributions.
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2306.16707.pdf' target='_blank'>https://arxiv.org/pdf/2306.16707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masato Fujitake
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.16707">DiffusionSTR: Diffusion Model for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents Diffusion Model for Scene Text Recognition (DiffusionSTR), an end-to-end text recognition framework using diffusion models for recognizing text in the wild. While existing studies have viewed the scene text recognition task as an image-to-text transformation, we rethought it as a text-text one under images in a diffusion model. We show for the first time that the diffusion model can be applied to text recognition. Furthermore, experimental results on publicly available datasets show that the proposed method achieves competitive accuracy compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2302.10641.pdf' target='_blank'>https://arxiv.org/pdf/2302.10641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masato Fujitake
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10641">A3S: Adversarial learning of semantic representations for Scene-Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene-text spotting is a task that predicts a text area on natural scene images and recognizes its text characters simultaneously. It has attracted much attention in recent years due to its wide applications. Existing research has mainly focused on improving text region detection, not text recognition. Thus, while detection accuracy is improved, the end-to-end accuracy is insufficient. Texts in natural scene images tend to not be a random string of characters but a meaningful string of characters, a word. Therefore, we propose adversarial learning of semantic representations for scene text spotting (A3S) to improve end-to-end accuracy, including text recognition. A3S simultaneously predicts semantic features in the detected text area instead of only performing text recognition based on existing visual features. Experimental results on publicly available datasets show that the proposed method achieves better accuracy than other methods.
