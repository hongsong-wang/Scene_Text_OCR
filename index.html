<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2512.01422.pdf' target='_blank'>https://arxiv.org/pdf/2512.01422.pdf</a></span>   <span><a href='https://github.com/Topdu/OpenOCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongkun Du, Miaomiao Zhao, Songlin Fan, Zhineng Chen, Caiyan Jia, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01422">MDiff4STR: Mask Diffusion Model for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mask Diffusion Models (MDMs) have recently emerged as a promising alternative to auto-regressive models (ARMs) for vision-language tasks, owing to their flexible balance of efficiency and accuracy. In this paper, for the first time, we introduce MDMs into the Scene Text Recognition (STR) task. We show that vanilla MDM lags behind ARMs in terms of accuracy, although it improves recognition efficiency. To bridge this gap, we propose MDiff4STR, a Mask Diffusion model enhanced with two key improvement strategies tailored for STR. Specifically, we identify two key challenges in applying MDMs to STR: noising gap between training and inference, and overconfident predictions during inference. Both significantly hinder the performance of MDMs. To mitigate the first issue, we develop six noising strategies that better align training with inference behavior. For the second, we propose a token-replacement noise mechanism that provides a non-mask noise type, encouraging the model to reconsider and revise overly confident but incorrect predictions. We conduct extensive evaluations of MDiff4STR on both standard and challenging STR benchmarks, covering diverse scenarios including irregular, artistic, occluded, and Chinese text, as well as whether the use of pretraining. Across these settings, MDiff4STR consistently outperforms popular STR models, surpassing state-of-the-art ARMs in accuracy, while maintaining fast inference with only three denoising steps. Code: https://github.com/Topdu/OpenOCR.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2511.17138.pdf' target='_blank'>https://arxiv.org/pdf/2511.17138.pdf</a></span>   <span><a href='https://github.com/RedMediaTech/ODTSR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushun Fang, Yuxiang Chen, Shibo Yin, Qiang Hu, Jiangchao Yao, Ya Zhang, Xiaoyun Zhang, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17138">One-Step Diffusion Transformer for Controllable Real-World Image Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion-based real-world image super-resolution (Real-ISR) have demonstrated remarkable perceptual quality, yet the balance between fidelity and controllability remains a problem: multi-step diffusion-based methods suffer from generative diversity and randomness, resulting in low fidelity, while one-step methods lose control flexibility due to fidelity-specific finetuning. In this paper, we present ODTSR, a one-step diffusion transformer based on Qwen-Image that performs Real-ISR considering fidelity and controllability simultaneously: a newly introduced visual stream receives low-quality images (LQ) with adjustable noise (Control Noise), and the original visual stream receives LQs with consistent noise (Prior Noise), forming the Noise-hybrid Visual Stream (NVS) design. ODTSR further employs Fidelity-aware Adversarial Training (FAA) to enhance controllability and achieve one-step inference. Extensive experiments demonstrate that ODTSR not only achieves state-of-the-art (SOTA) performance on generic Real-ISR, but also enables prompt controllability on challenging scenarios such as real-world scene text image super-resolution (STISR) of Chinese characters without training on specific datasets. Codes are available at https://github.com/RedMediaTech/ODTSR.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2511.13399.pdf' target='_blank'>https://arxiv.org/pdf/2511.13399.pdf</a></span>   <span><a href='https://github.com/yusenbao01/TripleFDS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Bao, Yiting Wang, Wenjian Huang, Haowei Wang, Shen Chen, Taiping Yao, Shouhong Ding, Jianguo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13399">TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the "SCB Group", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent "shortcut" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2507.16330.pdf' target='_blank'>https://arxiv.org/pdf/2507.16330.pdf</a></span>   <span><a href='https://github.com/josepDe/Project_Aria_STR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph De Mathia, Carlos Francisco Moreno-GarcÃ­a
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16330">Scene Text Detection and Recognition "in light of" Challenging Environmental Conditions using Aria Glasses Egocentric Vision Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In an era where wearable technology is reshaping applications, Scene Text Detection and Recognition (STDR) becomes a straightforward choice through the lens of egocentric vision. Leveraging Meta's Project Aria smart glasses, this paper investigates how environmental variables, such as lighting, distance, and resolution, affect the performance of state-of-the-art STDR algorithms in real-world scenarios. We introduce a novel, custom-built dataset captured under controlled conditions and evaluate two OCR pipelines: EAST with CRNN, and EAST with PyTesseract. Our findings reveal that resolution and distance significantly influence recognition accuracy, while lighting plays a less predictable role. Notably, image upscaling emerged as a key pre-processing technique, reducing Character Error Rate (CER) from 0.65 to 0.48. We further demonstrate the potential of integrating eye-gaze tracking to optimise processing efficiency by focusing on user attention zones. This work not only benchmarks STDR performance under realistic conditions but also lays the groundwork for adaptive, user-aware AR systems. Our contributions aim to inspire future research in robust, context-sensitive text recognition for assistive and research-oriented applications, such as asset inspection and nutrition analysis. The code is available at https://github.com/josepDe/Project_Aria_STR.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2507.02200.pdf' target='_blank'>https://arxiv.org/pdf/2507.02200.pdf</a></span>   <span><a href='https://github.com/Event-AHU/ESTR-CoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Jingtao Jiang, Qiang Chen, Lan Chen, Lin Zhu, Yaowei Wang, Yonghong Tian, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02200">ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event stream based scene text recognition is a newly arising research topic in recent years which performs better than the widely used RGB cameras in extremely challenging scenarios, especially the low illumination, fast motion. Existing works either adopt end-to-end encoder-decoder framework or large language models for enhanced recognition, however, they are still limited by the challenges of insufficient interpretability and weak contextual logical reasoning. In this work, we propose a novel chain-of-thought reasoning based event stream scene text recognition framework, termed ESTR-CoT. Specifically, we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input event stream into tokens and utilize a Llama tokenizer to encode the given generation prompt. A Q-former is used to align the vision token to the pre-trained large language model Vicuna-7B and output both the answer and chain-of-thought (CoT) reasoning process simultaneously. Our framework can be optimized using supervised fine-tuning in an end-to-end manner. In addition, we also propose a large-scale CoT dataset to train our framework via a three stage processing (i.e., generation, polish, and expert verification). This dataset provides a solid data foundation for the development of subsequent reasoning-based large models. Extensive experiments on three event stream STR benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the effectiveness and interpretability of our proposed framework. The source code and pre-trained models will be released on https://github.com/Event-AHU/ESTR-CoT.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2506.10609.pdf' target='_blank'>https://arxiv.org/pdf/2506.10609.pdf</a></span>   <span><a href='https://github.com/yingift/MSTAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Yin, Xudong Xie, Zhang Li, Xiang Bai, Yuliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10609">MSTAR: Box-free Multi-query Scene Text Retrieval with Attention Recycling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text retrieval has made significant progress with the assistance of accurate text localization. However, existing approaches typically require costly bounding box annotations for training. Besides, they mostly adopt a customized retrieval strategy but struggle to unify various types of queries to meet diverse retrieval needs. To address these issues, we introduce Muti-query Scene Text retrieval with Attention Recycling (MSTAR), a box-free approach for scene text retrieval. It incorporates progressive vision embedding to dynamically capture the multi-grained representation of texts and harmonizes free-style text queries with style-aware instructions. Additionally, a multi-instance matching module is integrated to enhance vision-language alignment. Furthermore, we build the Multi-Query Text Retrieval (MQTR) dataset, the first benchmark designed to evaluate the multi-query scene text retrieval capability of models, comprising four query types and 16k images. Extensive experiments demonstrate the superiority of our method across seven public datasets and the MQTR dataset. Notably, MSTAR marginally surpasses the previous state-of-the-art model by 6.4% in MAP on Total-Text while eliminating box annotation costs. Moreover, on the MQTR benchmark, MSTAR significantly outperforms the previous models by an average of 8.5%. The code and datasets are available at https://github.com/yingift/MSTAR.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2505.15649.pdf' target='_blank'>https://arxiv.org/pdf/2505.15649.pdf</a></span>   <span><a href='https://github.com/pd162/LTB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianjiao Cao, Jiahao Lyu, Weichao Zeng, Weimin Mu, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15649">The Devil is in Fine-tuning and Long-tailed Problems:A New Benchmark for Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text detection has seen the emergence of high-performing methods that excel on academic benchmarks. However, these detectors often fail to replicate such success in real-world scenarios. We uncover two key factors contributing to this discrepancy through extensive experiments. First, a \textit{Fine-tuning Gap}, where models leverage \textit{Dataset-Specific Optimization} (DSO) paradigm for one domain at the cost of reduced effectiveness in others, leads to inflated performances on academic benchmarks. Second, the suboptimal performance in practical settings is primarily attributed to the long-tailed distribution of texts, where detectors struggle with rare and complex categories as artistic or overlapped text. Given that the DSO paradigm might undermine the generalization ability of models, we advocate for a \textit{Joint-Dataset Learning} (JDL) protocol to alleviate the Fine-tuning Gap. Additionally, an error analysis is conducted to identify three major categories and 13 subcategories of challenges in long-tailed scene text, upon which we propose a Long-Tailed Benchmark (LTB). LTB facilitates a comprehensive evaluation of ability to handle a diverse range of long-tailed challenges. We further introduce MAEDet, a self-supervised learning-based method, as a strong baseline for LTB. The code is available at https://github.com/pd162/LTB.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2505.03329.pdf' target='_blank'>https://arxiv.org/pdf/2505.03329.pdf</a></span>   <span><a href='https://github.com/AMAP-ML/FluxText' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Lan, Yancheng Bai, Xu Duan, Mingxing Li, Dongyang Jin, Ryan Xu, Lei Sun, Xiangxiang Chu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03329">FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text editing aims to modify or add texts on images while ensuring text fidelity and overall visual quality consistent with the background. Recent methods are primarily built on UNet-based diffusion models, which have improved scene text editing results, but still struggle with complex glyph structures, especially for non-Latin ones (\eg, Chinese, Korean, Japanese). To address these issues, we present \textbf{FLUX-Text}, a simple and advanced multilingual scene text editing DiT method. Specifically, our FLUX-Text enhances glyph understanding and generation through lightweight Visual and Text Embedding Modules, while preserving the original generative capability of FLUX. We further propose a Regional Text Perceptual Loss tailored for text regions, along with a matching two-stage training strategy to better balance text editing and overall image quality. Benefiting from the DiT-based architecture and lightweight feature injection modules, FLUX-Text can be trained with only $0.1$M training examples, a \textbf{97\%} reduction compared to $2.9$M required by popular methods. Extensive experiments on multiple public datasets, including English and Chinese benchmarks, demonstrate that our method surpasses other methods in visual quality and text fidelity. All the code is available at https://github.com/AMAP-ML/FluxText.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2504.09966.pdf' target='_blank'>https://arxiv.org/pdf/2504.09966.pdf</a></span>   <span><a href='https://github.com/DrLuo/SemiETS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongliang Luo, Hanshen Zhu, Ziyang Zhang, Dingkang Liang, Xudong Xie, Yuliang Liu, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09966">SemiETS: Integrating Spatial and Content Consistencies for Semi-Supervised End-to-end Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most previous scene text spotting methods rely on high-quality manual annotations to achieve promising performance. To reduce their expensive costs, we study semi-supervised text spotting (SSTS) to exploit useful information from unlabeled images. However, directly applying existing semi-supervised methods of general scenes to SSTS will face new challenges: 1) inconsistent pseudo labels between detection and recognition tasks, and 2) sub-optimal supervisions caused by inconsistency between teacher/student. Thus, we propose a new Semi-supervised framework for End-to-end Text Spotting, namely SemiETS that leverages the complementarity of text detection and recognition. Specifically, it gradually generates reliable hierarchical pseudo labels for each task, thereby reducing noisy labels. Meanwhile, it extracts important information in locations and transcriptions from bidirectional flows to improve consistency. Extensive experiments on three datasets under various settings demonstrate the effectiveness of SemiETS on arbitrary-shaped text. For example, it outperforms previous state-of-the-art SSL methods by a large margin on end-to-end spotting (+8.7%, +5.6%, and +2.6% H-mean under 0.5%, 1%, and 2% labeled data settings on Total-Text, respectively). More importantly, it still improves upon a strongly supervised text spotter trained with plenty of labeled data by 2.0%. Compelling domain adaptation ability shows practical potential. Moreover, our method demonstrates consistent improvement on different text spotters.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2503.08387.pdf' target='_blank'>https://arxiv.org/pdf/2503.08387.pdf</a></span>   <span><a href='https://github.com/ZhengyaoFang/RS-STE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyao Fang, Pengyuan Lyu, Jingjing Wu, Chengquan Zhang, Jun Yu, Guangming Lu, Wenjie Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08387">Recognition-Synergistic Scene Text Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text editing aims to modify text content within scene images while maintaining style consistency. Traditional methods achieve this by explicitly disentangling style and content from the source image and then fusing the style with the target content, while ensuring content consistency using a pre-trained recognition model. Despite notable progress, these methods suffer from complex pipelines, leading to suboptimal performance in complex scenarios. In this work, we introduce Recognition-Synergistic Scene Text Editing (RS-STE), a novel approach that fully exploits the intrinsic synergy of text recognition for editing. Our model seamlessly integrates text recognition with text editing within a unified framework, and leverages the recognition model's ability to implicitly disentangle style and content while ensuring content consistency. Specifically, our approach employs a multi-modal parallel decoder based on transformer architecture, which predicts both text content and stylized images in parallel. Additionally, our cyclic self-supervised fine-tuning strategy enables effective training on unpaired real-world data without ground truth, enhancing style and content consistency through a twice-cyclic generation process. Built on a relatively simple architecture, RS-STE achieves state-of-the-art performance on both synthetic and real-world benchmarks, and further demonstrates the effectiveness of leveraging the generated hard cases to boost the performance of downstream recognition tasks. Code is available at https://github.com/ZhengyaoFang/RS-STE.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2503.06501.pdf' target='_blank'>https://arxiv.org/pdf/2503.06501.pdf</a></span>   <span><a href='https://github.com/HqiTao/TextInPlace' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaqi Tao, Bingxi Liu, Calvin Chen, Tingjun Huang, He Li, Jinqiang Cui, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06501">TextInPlace: Indoor Visual Place Recognition in Repetitive Structures with Scene Text Spotting and Verification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Place Recognition (VPR) is a crucial capability for long-term autonomous robots, enabling them to identify previously visited locations using visual information. However, existing methods remain limited in indoor settings due to the highly repetitive structures inherent in such environments. We observe that scene texts frequently appear in indoor spaces and can help distinguish visually similar but different places. This inspires us to propose TextInPlace, a simple yet effective VPR framework that integrates Scene Text Spotting (STS) to mitigate visual perceptual ambiguity in repetitive indoor environments. Specifically, TextInPlace adopts a dual-branch architecture within a local parameter sharing network. The VPR branch employs attention-based aggregation to extract global descriptors for coarse-grained retrieval, while the STS branch utilizes a bridging text spotter to detect and recognize scene texts. Finally, the discriminative texts are filtered to compute text similarity and re-rank the top-K retrieved images. To bridge the gap between current text-based repetitive indoor scene datasets and the typical scenarios encountered in robot navigation, we establish an indoor VPR benchmark dataset, called Maze-with-Text. Extensive experiments on both custom and public datasets demonstrate that TextInPlace achieves superior performance over existing methods that rely solely on appearance information. The dataset, code, and trained models are publicly available at https://github.com/HqiTao/TextInPlace.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2502.09020.pdf' target='_blank'>https://arxiv.org/pdf/2502.09020.pdf</a></span>   <span><a href='https://github.com/Event-AHU/EventSTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Jingtao Jiang, Dong Li, Futian Wang, Lin Zhu, Yaowei Wang, Yongyong Tian, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09020">EventSTR: A Benchmark Dataset and Baselines for Event Stream based Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mainstream Scene Text Recognition (STR) algorithms are developed based on RGB cameras which are sensitive to challenging factors such as low illumination, motion blur, and cluttered backgrounds. In this paper, we propose to recognize the scene text using bio-inspired event cameras by collecting and annotating a large-scale benchmark dataset, termed EventSTR. It contains 9,928 high-definition (1280 * 720) event samples and involves both Chinese and English characters. We also benchmark multiple STR algorithms as the baselines for future works to compare. In addition, we propose a new event-based scene text recognition framework, termed SimC-ESTR. It first extracts the event features using a visual encoder and projects them into tokens using a Q-former module. More importantly, we propose to augment the vision tokens based on a memory mechanism before feeding into the large language models. A similarity-based error correction mechanism is embedded within the large language model to correct potential minor errors fundamentally based on contextual information. Extensive experiments on the newly proposed EventSTR dataset and two simulation STR datasets fully demonstrate the effectiveness of our proposed model. We believe that the dataset and algorithmic model can innovatively propose an event-based STR task and are expected to accelerate the application of event cameras in various industries. The source code and pre-trained models will be released on https://github.com/Event-AHU/EventSTR
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2502.07411.pdf' target='_blank'>https://arxiv.org/pdf/2502.07411.pdf</a></span>   <span><a href='https://github.com/zhousheng97/EgoTextVQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Zhou, Junbin Xiao, Qingyun Li, Yicong Li, Xun Yang, Dan Guo, Meng Wang, Tat-Seng Chua, Angela Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07411">EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce EgoTextVQA, a novel and rigorously constructed benchmark for egocentric QA assistance involving scene text. EgoTextVQA contains 1.5K ego-view videos and 7K scene-text aware questions that reflect real user needs in outdoor driving and indoor house-keeping activities. The questions are designed to elicit identification and reasoning on scene text in an egocentric and dynamic environment. With EgoTextVQA, we comprehensively evaluate 10 prominent multimodal large language models. Currently, all models struggle, and the best results (Gemini 1.5 Pro) are around 33\% accuracy, highlighting the severe deficiency of these techniques in egocentric QA assistance. Our further investigations suggest that precise temporal grounding and multi-frame reasoning, along with high resolution and auxiliary scene-text inputs, are key for better performance. With thorough analyses and heuristic suggestions, we hope EgoTextVQA can serve as a solid testbed for research in egocentric scene-text QA assistance. Our dataset is released at: https://github.com/zhousheng97/EgoTextVQA.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2412.17007.pdf' target='_blank'>https://arxiv.org/pdf/2412.17007.pdf</a></span>   <span><a href='https://yejy53.github.io/CVG-Text/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyan Ye, Honglin Lin, Leyan Ou, Dairong Chen, Zihao Wang, Qi Zhu, Conghui He, Weijia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17007">Where am I? Cross-View Geo-localization with Natural Language Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-view geo-localization identifies the locations of street-view images by matching them with geo-tagged satellite images or OSM. However, most existing studies focus on image-to-image retrieval, with fewer addressing text-guided retrieval, a task vital for applications like pedestrian navigation and emergency response. In this work, we introduce a novel task for cross-view geo-localization with natural language descriptions, which aims to retrieve corresponding satellite images or OSM database based on scene text descriptions. To support this task, we construct the CVG-Text dataset by collecting cross-view data from multiple cities and employing a scene text generation approach that leverages the annotation capabilities of Large Multimodal Models to produce high-quality scene text descriptions with localization details. Additionally, we propose a novel text-based retrieval localization method, CrossText2Loc, which improves recall by 10% and demonstrates excellent long-text retrieval capabilities. In terms of explainability, it not only provides similarity scores but also offers retrieval reasons. More information can be found at https://yejy53.github.io/CVG-Text/ .
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2412.01137.pdf' target='_blank'>https://arxiv.org/pdf/2412.01137.pdf</a></span>   <span><a href='https://github.com/YesianRohn/TextSSR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingsong Ye, Yongkun Du, Yunbo Tao, Zhineng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01137">TextSSR: Diffusion-based Data Synthesis for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition (STR) suffers from challenges of either less realistic synthetic training data or the difficulty of collecting sufficient high-quality real-world data, limiting the effectiveness of trained models. Meanwhile, despite producing holistically appealing text images, diffusion-based visual text generation methods struggle to synthesize accurate and realistic instance-level text at scale. To tackle this, we introduce TextSSR: a novel pipeline for Synthesizing Scene Text Recognition training data. TextSSR targets three key synthesizing characteristics: accuracy, realism, and scalability. It achieves accuracy through a proposed region-centric text generation with position-glyph enhancement, ensuring proper character placement. It maintains realism by guiding style and appearance generation using contextual hints from surrounding text or background. This character-aware diffusion architecture enjoys precise character-level control and semantic coherence preservation, without relying on natural language prompts. Therefore, TextSSR supports large-scale generation through combinatorial text permutations. Based on these, we present TextSSR-F, a dataset of 3.55 million quality-screened text instances. Extensive experiments show that STR models trained on TextSSR-F outperform those trained on existing synthetic datasets by clear margins on common benchmarks, and further improvements are observed when mixed with real-world training data. Code is available at https://github.com/YesianRohn/TextSSR.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2411.15858.pdf' target='_blank'>https://arxiv.org/pdf/2411.15858.pdf</a></span>   <span><a href='https://github.com/Topdu/OpenOCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongkun Du, Zhineng Chen, Hongtao Xie, Caiyan Jia, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15858">SVTRv2: CTC Beats Encoder-Decoder Models in Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connectionist temporal classification (CTC)-based scene text recognition (STR) methods, e.g., SVTR, are widely employed in OCR applications, mainly due to their simple architecture, which only contains a visual model and a CTC-aligned linear classifier, and therefore fast inference. However, they generally exhibit worse accuracy than encoder-decoder-based methods (EDTRs) due to struggling with text irregularity and linguistic missing. To address these challenges, we propose SVTRv2, a CTC model endowed with the ability to handle text irregularities and model linguistic context. First, a multi-size resizing strategy is proposed to resize text instances to appropriate predefined sizes, effectively avoiding severe text distortion. Meanwhile, we introduce a feature rearrangement module to ensure that visual features accommodate the requirement of CTC, thus alleviating the alignment puzzle. Second, we propose a semantic guidance module. It integrates linguistic context into the visual features, allowing CTC model to leverage language information for accuracy improvement. This module can be omitted at the inference stage and would not increase the time cost. We extensively evaluate SVTRv2 in both standard and recent challenging benchmarks, where SVTRv2 is fairly compared to popular STR models across multiple scenarios, including different types of text irregularity, languages, long text, and whether employing pretraining. SVTRv2 surpasses most EDTRs across the scenarios in terms of accuracy and inference speed. Code: https://github.com/Topdu/OpenOCR.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2411.15585.pdf' target='_blank'>https://arxiv.org/pdf/2411.15585.pdf</a></span>   <span><a href='https://github.com/qqqyd/ViSu' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yadong Qu, Yuxin Wang, Bangbang Zhou, Zixiao Wang, Hongtao Xie, Yongdong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15585">Boosting Semi-Supervised Scene Text Recognition via Viewing and Summarizing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing scene text recognition (STR) methods struggle to recognize challenging texts, especially for artistic and severely distorted characters. The limitation lies in the insufficient exploration of character morphologies, including the monotonousness of widely used synthetic training data and the sensitivity of the model to character morphologies. To address these issues, inspired by the human learning process of viewing and summarizing, we facilitate the contrastive learning-based STR framework in a self-motivated manner by leveraging synthetic and real unlabeled data without any human cost. In the viewing process, to compensate for the simplicity of synthetic data and enrich character morphology diversity, we propose an Online Generation Strategy to generate background-free samples with diverse character styles. By excluding background noise distractions, the model is encouraged to focus on character morphology and generalize the ability to recognize complex samples when trained with only simple synthetic data. To boost the summarizing process, we theoretically demonstrate the derivation error in the previous character contrastive loss, which mistakenly causes the sparsity in the intra-class distribution and exacerbates ambiguity on challenging samples. Therefore, a new Character Unidirectional Alignment Loss is proposed to correct this error and unify the representation of the same characters in all samples by aligning the character features in the student model with the reference features in the teacher model. Extensive experiment results show that our method achieves SOTA performance (94.7\% and 70.9\% average accuracy on common benchmarks and Union14M-Benchmark). Code will be available at https://github.com/qqqyd/ViSu.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2411.11219.pdf' target='_blank'>https://arxiv.org/pdf/2411.11219.pdf</a></span>   <span><a href='https://github.com/ThunderVVV/RCMSTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiancheng Lin, Jinglei Zhang, Yi Xu, Kai Chen, Rui Zhang, Chang-Wen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11219">Relational Contrastive Learning and Masked Image Modeling for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Context-aware methods have achieved remarkable advancements in supervised scene text recognition by leveraging semantic priors from words. Considering the heterogeneity of text and background in STR, we propose that such contextual priors can be reinterpreted as the relations between textual elements, serving as effective self-supervised labels for representation learning. However, textual relations are restricted to the finite size of the dataset due to lexical dependencies, which causes over-fitting problem, thus compromising the representation quality. To address this, our work introduces a unified framework of Relational Contrastive Learning and Masked Image Modeling for STR (RCMSTR), which explicitly models the enriched textual relations. For the RCL branch, we first introduce the relational rearrangement module to cultivate new relations on the fly. Based on this, we further conduct relational contrastive learning to model the intra- and inter-hierarchical relations for frames, sub-words and words. On the other hand, MIM can naturally boost the context information via masking, where we find that the block masking strategy is more effective for STR. For the effective integration of RCL and MIM, we also introduce a novel decoupling design aimed at mitigating the impact of masked images on contrastive learning. Additionally, to enhance the compatibility of MIM with CNNs, we propose the adoption of sparse convolutions and directly sharing the weights with dense convolutions in training. The proposed RCMSTR demonstrates superior performance in various evaluation protocols for different STR-related downstream tasks, outperforming the existing state-of-the-art self-supervised STR techniques. Ablation studies and qualitative experimental results further validate the effectiveness of our method. The code and pre-trained models will be available at https://github.com/ThunderVVV/RCMSTR .
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2411.10261.pdf' target='_blank'>https://arxiv.org/pdf/2411.10261.pdf</a></span>   <span><a href='https://github.com/lanfeng4659/PSTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wang, Minghui Liao, Zhouyi Xie, Wenyu Liu, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10261">Partial Scene Text Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of partial scene text retrieval involves localizing and searching for text instances that are the same or similar to a given query text from an image gallery. However, existing methods can only handle text-line instances, leaving the problem of searching for partial patches within these text-line instances unsolved due to a lack of patch annotations in the training data. To address this issue, we propose a network that can simultaneously retrieve both text-line instances and their partial patches. Our method embeds the two types of data (query text and scene text instances) into a shared feature space and measures their cross-modal similarities. To handle partial patches, our proposed approach adopts a Multiple Instance Learning (MIL) approach to learn their similarities with query text, without requiring extra annotations. However, constructing bags, which is a standard step of conventional MIL approaches, can introduce numerous noisy samples for training, and lower inference speed. To address this issue, we propose a Ranking MIL (RankMIL) approach to adaptively filter those noisy samples. Additionally, we present a Dynamic Partial Match Algorithm (DPMA) that can directly search for the target partial patch from a text-line instance during the inference stage, without requiring bags. This greatly improves the search efficiency and the performance of retrieving partial patches. The source code and dataset are available at https://github.com/lanfeng4659/PSTR.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2411.02794.pdf' target='_blank'>https://arxiv.org/pdf/2411.02794.pdf</a></span>   <span><a href='https://github.com/fengmulin/SMNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Han, Junyu Gao, Chuang Yang, Yuan Yuan, Qi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02794">Real-Time Text Detection with Similar Mask in Traffic, Industrial, and Natural Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Texts on the intelligent transportation scene include mass information. Fully harnessing this information is one of the critical drivers for advancing intelligent transportation. Unlike the general scene, detecting text in transportation has extra demand, such as a fast inference speed, except for high accuracy. Most existing real-time text detection methods are based on the shrink mask, which loses some geometry semantic information and needs complex post-processing. In addition, the previous method usually focuses on correct output, which ignores feature correction and lacks guidance during the intermediate process. To this end, we propose an efficient multi-scene text detector that contains an effective text representation similar mask (SM) and a feature correction module (FCM). Unlike previous methods, the former aims to preserve the geometric information of the instances as much as possible. Its post-progressing saves 50$\%$ of the time, accurately and efficiently reconstructing text contours. The latter encourages false positive features to move away from the positive feature center, optimizing the predictions from the feature level. Some ablation studies demonstrate the efficiency of the SM and the effectiveness of the FCM. Moreover, the deficiency of existing traffic datasets (such as the low-quality annotation or closed source data unavailability) motivated us to collect and annotate a traffic text dataset, which introduces motion blur. In addition, to validate the scene robustness of the SM-Net, we conduct experiments on traffic, industrial, and natural scene datasets. Extensive experiments verify it achieves (SOTA) performance on several benchmarks. The code and dataset are available at: \url{https://github.com/fengmulin/SMNet}.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2410.16163.pdf' target='_blank'>https://arxiv.org/pdf/2410.16163.pdf</a></span>   <span><a href='https://github.com/jefferyZhan/Griffon' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhan, Hongyin Zhao, Yousong Zhu, Fan Yang, Ming Tang, Jinqiao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16163">Griffon-G: Bridging Vision-Language and Vision-Centric Tasks via Large Multimodal Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Multimodal Models (LMMs) have achieved significant breakthroughs in various vision-language and vision-centric tasks based on auto-regressive modeling. However, these models typically focus on either vision-centric tasks, such as visual grounding and region description, or vision-language tasks, like image caption and multi-scenario VQAs. None of the LMMs have yet comprehensively unified both types of tasks within a single model, as seen in Large Language Models in the natural language processing field. Furthermore, even with abundant multi-task instruction-following data, directly stacking these data for universal capabilities extension remains challenging. To address these issues, we introduce a novel multi-dimension curated and consolidated multimodal dataset, named CCMD-8M, which overcomes the data barriers of unifying vision-centric and vision-language tasks through multi-level data curation and multi-task consolidation. More importantly, we present Griffon-G, a general large multimodal model that addresses both vision-centric and vision-language tasks within a single end-to-end paradigm. Griffon-G resolves the training collapse issue encountered during the joint optimization of these tasks, achieving better training efficiency. Evaluations across multimodal benchmarks, general Visual Question Answering (VQA) tasks, scene text-centric VQA tasks, document-related VQA tasks, Referring Expression Comprehension, and object detection demonstrate that Griffon-G surpasses the advanced LMMs and achieves expert-level performance in complicated vision-centric tasks.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2410.15869.pdf' target='_blank'>https://arxiv.org/pdf/2410.15869.pdf</a></span>   <span><a href='https://github.com/TongxingJin/TXTLCD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongxing Jin, Thien-Minh Nguyen, Xinhang Xu, Yizhuo Yang, Shenghai Yuan, Jianping Li, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15869">Robust Loop Closure by Textual Cues in Challenging Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Loop closure is an important task in robot navigation. However, existing methods mostly rely on some implicit or heuristic features of the environment, which can still fail to work in common environments such as corridors, tunnels, and warehouses. Indeed, navigating in such featureless, degenerative, and repetitive (FDR) environments would also pose a significant challenge even for humans, but explicit text cues in the surroundings often provide the best assistance. This inspires us to propose a multi-modal loop closure method based on explicit human-readable textual cues in FDR environments. Specifically, our approach first extracts scene text entities based on Optical Character Recognition (OCR), then creates a local map of text cues based on accurate LiDAR odometry and finally identifies loop closure events by a graph-theoretic scheme. Experiment results demonstrate that this approach has superior performance over existing methods that rely solely on visual and LiDAR sensors. To benefit the community, we release the source code and datasets at \url{https://github.com/TongxingJin/TXTLCD}.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2410.11538.pdf' target='_blank'>https://arxiv.org/pdf/2410.11538.pdf</a></span>   <span><a href='https://github.com/xfey/MCTBench?tab=readme-ov-file' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Shan, Xiang Fei, Wei Shi, An-Lan Wang, Guozhi Tang, Lei Liao, Jingqun Tang, Xiang Bai, Can Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11538">MCTBench: Multimodal Cognition towards Text-Rich Visual Scenes Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The comprehension of text-rich visual scenes has become a focal point for evaluating Multi-modal Large Language Models (MLLMs) due to their widespread applications. Current benchmarks tailored to the scenario emphasize perceptual capabilities, while overlooking the assessment of cognitive abilities. To address this limitation, we introduce a Multimodal benchmark towards Text-rich visual scenes, to evaluate the Cognitive capabilities of MLLMs through visual reasoning and content-creation tasks (MCTBench). To mitigate potential evaluation bias from the varying distributions of datasets, MCTBench incorporates several perception tasks (e.g., scene text recognition) to ensure a consistent comparison of both the cognitive and perceptual capabilities of MLLMs. To improve the efficiency and fairness of content-creation evaluation, we conduct an automatic evaluation pipeline. Evaluations of various MLLMs on MCTBench reveal that, despite their impressive perceptual capabilities, their cognition abilities require enhancement. We hope MCTBench will offer the community an efficient resource to explore and enhance cognitive capabilities towards text-rich visual scenes.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2410.10168.pdf' target='_blank'>https://arxiv.org/pdf/2410.10168.pdf</a></span>   <span><a href='https://github.com/Zhenhang-Li/GlyphOnly' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhang Li, Yan Shu, Weichao Zeng, Dongbao Yang, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10168">First Creating Backgrounds Then Rendering Texts: A New Paradigm for Visual Text Blending</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models, known for their impressive image generation abilities, have played a pivotal role in the rise of visual text generation. Nevertheless, existing visual text generation methods often focus on generating entire images with text prompts, leading to imprecise control and limited practicality. A more promising direction is visual text blending, which focuses on seamlessly merging texts onto text-free backgrounds. However, existing visual text blending methods often struggle to generate high-fidelity and diverse images due to a shortage of backgrounds for synthesis and limited generalization capabilities. To overcome these challenges, we propose a new visual text blending paradigm including both creating backgrounds and rendering texts. Specifically, a background generator is developed to produce high-fidelity and text-free natural images. Moreover, a text renderer named GlyphOnly is designed for achieving visually plausible text-background integration. GlyphOnly, built on a Stable Diffusion framework, utilizes glyphs and backgrounds as conditions for accurate rendering and consistency control, as well as equipped with an adaptive text block exploration strategy for small-scale text rendering. We also explore several downstream applications based on our method, including scene text dataset synthesis for boosting scene text detectors, as well as text image customization and editing. Code and model will be available at \url{https://github.com/Zhenhang-Li/GlyphOnly}.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2410.09913.pdf' target='_blank'>https://arxiv.org/pdf/2410.09913.pdf</a></span>   <span><a href='https://github.com/KhaLee2307/StrDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kha Nhat Le, Hoang-Tuan Nguyen, Hung Tien Tran, Thanh Duc Ngo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09913">Stratified Domain Adaptation: A Progressive Self-Training Approach for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised domain adaptation (UDA) has become increasingly prevalent in scene text recognition (STR), especially where training and testing data reside in different domains. The efficacy of existing UDA approaches tends to degrade when there is a large gap between the source and target domains. To deal with this problem, gradually shifting or progressively learning to shift from domain to domain is the key issue. In this paper, we introduce the Stratified Domain Adaptation (StrDA) approach, which examines the gradual escalation of the domain gap for the learning process. The objective is to partition the training data into subsets so that the progressively self-trained model can adapt to gradual changes. We stratify the training data by evaluating the proximity of each data sample to both the source and target domains. We propose a novel method for employing domain discriminators to estimate the out-of-distribution and domain discriminative levels of data samples. Extensive experiments on benchmark scene-text datasets show that our approach significantly improves the performance of baseline (source-trained) STR models.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2409.14319.pdf' target='_blank'>https://arxiv.org/pdf/2409.14319.pdf</a></span>   <span><a href='https://github.com/zhousheng97/ViTXT-GQA.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Zhou, Junbin Xiao, Xun Yang, Peipei Song, Dan Guo, Angela Yao, Meng Wang, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14319">Scene-Text Grounding for Text-Based Video Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing efforts in text-based video question answering (TextVideoQA) are criticized for their opaque decisionmaking and heavy reliance on scene-text recognition. In this paper, we propose to study Grounded TextVideoQA by forcing models to answer questions and spatio-temporally localize the relevant scene-text regions, thus decoupling QA from scenetext recognition and promoting research towards interpretable QA. The task has three-fold significance. First, it encourages scene-text evidence versus other short-cuts for answer predictions. Second, it directly accepts scene-text regions as visual answers, thus circumventing the problem of ineffective answer evaluation by stringent string matching. Third, it isolates the challenges inherited in VideoQA and scene-text recognition. This enables the diagnosis of the root causes for failure predictions, e.g., wrong QA or wrong scene-text recognition? To achieve Grounded TextVideoQA, we propose the T2S-QA model that highlights a disentangled temporal-to-spatial contrastive learning strategy for weakly-supervised scene-text grounding and grounded TextVideoQA. To facilitate evaluation, we construct a new dataset ViTXT-GQA which features 52K scene-text bounding boxes within 2.2K temporal segments related to 2K questions and 729 videos. With ViTXT-GQA, we perform extensive experiments and demonstrate the severe limitations of existing techniques in Grounded TextVideoQA. While T2S-QA achieves superior results, the large performance gap with human leaves ample space for improvement. Our further analysis of oracle scene-text inputs posits that the major challenge is scene-text recognition. To advance the research of Grounded TextVideoQA, our dataset and code are at https://github.com/zhousheng97/ViTXT-GQA.git
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2409.13431.pdf' target='_blank'>https://arxiv.org/pdf/2409.13431.pdf</a></span>   <span><a href='https://github.com/wzx99/TMIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixiao Wang, Hongtao Xie, YuXin Wang, Yadong Qu, Fengjun Guo, Pengwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13431">Leveraging Text Localization for Scene Text Removal via Text-aware Masked Image Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing scene text removal (STR) task suffers from insufficient training data due to the expensive pixel-level labeling. In this paper, we aim to address this issue by introducing a Text-aware Masked Image Modeling algorithm (TMIM), which can pretrain STR models with low-cost text detection labels (e.g., text bounding box). Different from previous pretraining methods that use indirect auxiliary tasks only to enhance the implicit feature extraction ability, our TMIM first enables the STR task to be directly trained in a weakly supervised manner, which explores the STR knowledge explicitly and efficiently. In TMIM, first, a Background Modeling stream is built to learn background generation rules by recovering the masked non-text region. Meanwhile, it provides pseudo STR labels on the masked text region. Second, a Text Erasing stream is proposed to learn from the pseudo labels and equip the model with end-to-end STR ability. Benefiting from the two collaborative streams, our STR model can achieve impressive performance only with the public text detection datasets, which greatly alleviates the limitation of the high-cost STR labels. Experiments demonstrate that our method outperforms other pretrain methods and achieves state-of-the-art performance (37.35 PSNR on SCUT-EnsText). Code will be available at https://github.com/wzx99/TMIM.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2408.14805.pdf' target='_blank'>https://arxiv.org/pdf/2408.14805.pdf</a></span>   <span><a href='https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/Platypus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Wang, Zhaohai Li, Jun Tang, Humen Zhong, Fei Huang, Zhibo Yang, Cong Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14805">Platypus: A Generalized Specialist Model for Reading Text in Various Forms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reading text from images (either natural scenes or documents) has been a long-standing research topic for decades, due to the high technical challenge and wide application range. Previously, individual specialist models are developed to tackle the sub-tasks of text reading (e.g., scene text recognition, handwritten text recognition and mathematical expression recognition). However, such specialist models usually cannot effectively generalize across different sub-tasks. Recently, generalist models (such as GPT-4V), trained on tremendous data in a unified way, have shown enormous potential in reading text in various scenarios, but with the drawbacks of limited accuracy and low efficiency. In this work, we propose Platypus, a generalized specialist model for text reading. Specifically, Platypus combines the best of both worlds: being able to recognize text of various forms with a single unified architecture, while achieving excellent accuracy and high efficiency. To better exploit the advantage of Platypus, we also construct a text reading dataset (called Worms), the images of which are curated from previous datasets and partially re-labeled. Experiments on standard benchmarks demonstrate the effectiveness and superiority of the proposed Platypus model. Model and data will be made publicly available at https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/Platypus.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2408.05706.pdf' target='_blank'>https://arxiv.org/pdf/2408.05706.pdf</a></span>   <span><a href='https://github.com/Topdu/OpenOCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Zhao, Yongkun Du, Zhineng Chen, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05706">Decoder Pre-Training with only Text for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition (STR) pre-training methods have achieved remarkable progress, primarily relying on synthetic datasets. However, the domain gap between synthetic and real images poses a challenge in acquiring feature representations that align well with images on real scenes, thereby limiting the performance of these methods. We note that vision-language models like CLIP, pre-trained on extensive real image-text pairs, effectively align images and text in a unified embedding space, suggesting the potential to derive the representations of real images from text alone. Building upon this premise, we introduce a novel method named Decoder Pre-training with only text for STR (DPTR). DPTR treats text embeddings produced by the CLIP text encoder as pseudo visual embeddings and uses them to pre-train the decoder. An Offline Randomized Perturbation (ORP) strategy is introduced. It enriches the diversity of text embeddings by incorporating natural image embeddings extracted from the CLIP image encoder, effectively directing the decoder to acquire the potential representations of real images. In addition, we introduce a Feature Merge Unit (FMU) that guides the extracted visual embeddings focusing on the character foreground within the text image, thereby enabling the pre-trained decoder to work more efficiently and accurately. Extensive experiments across various STR decoders and language recognition tasks underscore the broad applicability and remarkable performance of DPTR, providing a novel insight for STR pre-training. Code is available at https://github.com/Topdu/OpenOCR
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2408.00441.pdf' target='_blank'>https://arxiv.org/pdf/2408.00441.pdf</a></span>   <span><a href='https://github.com/Gyann-z/FDP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gangyan Zeng, Yuan Zhang, Jin Wei, Dongbao Yang, Peng Zhang, Yiwen Gao, Xugong Qin, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00441">Focus, Distinguish, and Prompt: Unleashing CLIP for Efficient and Flexible Scene Text Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text retrieval aims to find all images containing the query text from an image gallery. Current efforts tend to adopt an Optical Character Recognition (OCR) pipeline, which requires complicated text detection and/or recognition processes, resulting in inefficient and inflexible retrieval. Different from them, in this work we propose to explore the intrinsic potential of Contrastive Language-Image Pre-training (CLIP) for OCR-free scene text retrieval. Through empirical analysis, we observe that the main challenges of CLIP as a text retriever are: 1) limited text perceptual scale, and 2) entangled visual-semantic concepts. To this end, a novel model termed FDP (Focus, Distinguish, and Prompt) is developed. FDP first focuses on scene text via shifting the attention to the text area and probing the hidden text knowledge, and then divides the query text into content word and function word for processing, in which a semantic-aware prompting scheme and a distracted queries assistance module are utilized. Extensive experiments show that FDP significantly enhances the inference speed while achieving better or competitive retrieval accuracy compared to existing methods. Notably, on the IIIT-STR benchmark, FDP surpasses the state-of-the-art model by 4.37% with a 4 times faster speed. Furthermore, additional experiments under phrase-level and attribute-aware scene text retrieval settings validate FDP's particular advantages in handling diverse forms of query text. The source code will be publicly available at https://github.com/Gyann-z/FDP.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2407.21422.pdf' target='_blank'>https://arxiv.org/pdf/2407.21422.pdf</a></span>   <span><a href='https://github.com/qcf-568/OSTF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenfan Qu, Yiwu Zhong, Fengjun Guo, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21422">Revisiting Tampered Scene Text Detection in the Era of Generative AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancements of generative AI have fueled the potential of generative text image editing, meanwhile escalating the threat of misinformation spreading. However, existing forensics methods struggle to detect unseen forgery types that they have not been trained on, underscoring the need for a model capable of generalized detection of tampered scene text. To tackle this, we propose a novel task: open-set tampered scene text detection, which evaluates forensics models on their ability to identify both seen and previously unseen forgery types. We have curated a comprehensive, high-quality dataset, featuring the texts tampered by eight text editing models, to thoroughly assess the open-set generalization capabilities. Further, we introduce a novel and effective training paradigm that subtly alters the texture of selected texts within an image and trains the model to identify these regions. This approach not only mitigates the scarcity of high-quality training data but also enhances models' fine-grained perception and open-set generalization abilities. Additionally, we present DAF, a novel framework that improves open-set generalization by distinguishing between the features of authentic and tampered text, rather than focusing solely on the tampered text's features. Our extensive experiments validate the remarkable efficacy of our methods. For example, our zero-shot performance can even beat the previous state-of-the-art full-shot model by a large margin. Our dataset and code are available at https://github.com/qcf-568/OSTF.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2407.18616.pdf' target='_blank'>https://arxiv.org/pdf/2407.18616.pdf</a></span>   <span><a href='https://github.com/lancercat/Moose/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Liu, Simon CorbillÃ©, Elisa H Barney Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18616">MOoSE: Multi-Orientation Sharing Experts for Open-set Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-set text recognition, which aims to address both novel characters and previously seen ones, is one of the rising subtopics in the text recognition field. However, the current open-set text recognition solutions only focuses on horizontal text, which fail to model the real-life challenges posed by the variety of writing directions in real-world scene text. Multi-orientation text recognition, in general, faces challenges from the diverse image aspect ratios, significant imbalance in data amount, and domain gaps between orientations. In this work, we first propose a Multi-Oriented Open-Set Text Recognition task (MOOSTR) to model the challenges of both novel characters and writing direction variety. We then propose a Multi-Orientation Sharing Experts (MOoSE) framework as a strong baseline solution. MOoSE uses a mixture-of-experts scheme to alleviate the domain gaps between orientations, while exploiting common structural knowledge among experts to alleviate the data scarcity that some experts face. The proposed MOoSE framework is validated by ablative experiments, and also tested for feasibility on the existing open-set benchmark. Code, models, and documents are available at: https://github.com/lancercat/Moose/
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2407.12317.pdf' target='_blank'>https://arxiv.org/pdf/2407.12317.pdf</a></span>   <span><a href='https://github.com/Topdu/OpenOCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongkun Du, Zhineng Chen, Caiyan Jia, Xieping Gao, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12317">Out of Length Text Recognition with Sub-String Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Text Recognition (STR) methods have demonstrated robust performance in word-level text recognition. However, in real applications the text image is sometimes long due to detected with multiple horizontal words. It triggers the requirement to build long text recognition models from readily available short (i.e., word-level) text datasets, which has been less studied previously. In this paper, we term this task Out of Length (OOL) text recognition. We establish the first Long Text Benchmark (LTB) to facilitate the assessment of different methods in long text recognition. Meanwhile, we propose a novel method called OOL Text Recognition with sub-String Matching (SMTR). SMTR comprises two cross-attention-based modules: one encodes a sub-string containing multiple characters into next and previous queries, and the other employs the queries to attend to the image features, matching the sub-string and simultaneously recognizing its next and previous character. SMTR can recognize text of arbitrary length by iterating the process above. To avoid being trapped in recognizing highly similar sub-strings, we introduce a regularization training to compel SMTR to effectively discover subtle differences between similar sub-strings for precise matching. In addition, we propose an inference augmentation strategy to alleviate confusion caused by identical sub-strings in the same text and improve the overall recognition efficiency. Extensive experimental results reveal that SMTR, even when trained exclusively on short text, outperforms existing methods in public short text benchmarks and exhibits a clear advantage on LTB. Code: https://github.com/Topdu/OpenOCR.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2407.05562.pdf' target='_blank'>https://arxiv.org/pdf/2407.05562.pdf</a></span>   <span><a href='https://github.com/bang123-box/CFE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bangbang Zhou, Yadong Qu, Zixiao Wang, Zicheng Li, Boqiang Zhang, Hongtao Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05562">Focus on the Whole Character: Discriminative Character Modeling for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, scene text recognition (STR) models have shown significant performance improvements. However, existing models still encounter difficulties in recognizing challenging texts that involve factors such as severely distorted and perspective characters. These challenging texts mainly cause two problems: (1) Large Intra-Class Variance. (2) Small Inter-Class Variance. An extremely distorted character may prominently differ visually from other characters within the same category, while the variance between characters from different classes is relatively small. To address the above issues, we propose a novel method that enriches the character features to enhance the discriminability of characters. Firstly, we propose the Character-Aware Constraint Encoder (CACE) with multiple blocks stacked. CACE introduces a decay matrix in each block to explicitly guide the attention region for each token. By continuously employing the decay matrix, CACE enables tokens to perceive morphological information at the character level. Secondly, an Intra-Inter Consistency Loss (I^2CL) is introduced to consider intra-class compactness and inter-class separability at feature space. I^2CL improves the discriminative capability of features by learning a long-term memory unit for each character category. Trained with synthetic data, our model achieves state-of-the-art performance on common benchmarks (94.1% accuracy) and Union14M-Benchmark (61.6% accuracy). Code is available at https://github.com/bang123-box/CFE.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2405.14701.pdf' target='_blank'>https://arxiv.org/pdf/2405.14701.pdf</a></span>   <span><a href='https://codegoat24.github.io/DreamText/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/CodeGoat24/DreamText,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yibin Wang, Weizhong Zhang, Honghui Xu, Cheng Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14701">DreamText: High Fidelity Scene Text Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text synthesis involves rendering specified texts onto arbitrary images. Current methods typically formulate this task in an end-to-end manner but lack effective character-level guidance during training. Besides, their text encoders, pre-trained on a single font type, struggle to adapt to the diverse font styles encountered in practical applications. Consequently, these methods suffer from character distortion, repetition, and absence, particularly in polystylistic scenarios. To this end, this paper proposes DreamText for high-fidelity scene text synthesis. Our key idea is to reconstruct the diffusion training process, introducing more refined guidance tailored to this task, to expose and rectify the model's attention at the character level and strengthen its learning of text regions. This transformation poses a hybrid optimization challenge, involving both discrete and continuous variables. To effectively tackle this challenge, we employ a heuristic alternate optimization strategy. Meanwhile, we jointly train the text encoder and generator to comprehensively learn and utilize the diverse font present in the training dataset. This joint training is seamlessly integrated into the alternate optimization process, fostering a synergistic relationship between learning character embedding and re-estimating character attention. Specifically, in each step, we first encode potential character-generated position information from cross-attention maps into latent character masks. These masks are then utilized to update the representation of specific characters in the current step, which, in turn, enables the generator to correct the character's attention in the subsequent steps. Both qualitative and quantitative results demonstrate the superiority of our method to the state of the art.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2405.13896.pdf' target='_blank'>https://arxiv.org/pdf/2405.13896.pdf</a></span>   <span><a href='https://github.com/mkoshkina/jersey-number-pipeline' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Koshkina, James H. Elder
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13896">A General Framework for Jersey Number Recognition in Sports Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Jersey number recognition is an important task in sports video analysis, partly due to its importance for long-term player tracking. It can be viewed as a variant of scene text recognition. However, there is a lack of published attempts to apply scene text recognition models on jersey number data. Here we introduce a novel public jersey number recognition dataset for hockey and study how scene text recognition methods can be adapted to this problem. We address issues of occlusions and assess the degree to which training on one sport (hockey) can be generalized to another (soccer). For the latter, we also consider how jersey number recognition at the single-image level can be aggregated across frames to yield tracklet-level jersey number labels. We demonstrate high performance on image- and tracklet-level tasks, achieving 91.4% accuracy for hockey images and 87.4% for soccer tracklets. Code, models, and data are available at https://github.com/mkoshkina/jersey-number-pipeline.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2405.12533.pdf' target='_blank'>https://arxiv.org/pdf/2405.12533.pdf</a></span>   <span><a href='https://github.com/Hiba-MeiRuan/Urdu-VQA-Dataset-/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hiba Maryam, Ling Fu, Jiajun Song, Tajrian ABM Shafayet, Qidi Luo, Xiang Bai, Yuliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12533">Dataset and Benchmark for Urdu Natural Scenes Text Detection, Recognition and Visual Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of Urdu scene text detection, recognition, and Visual Question Answering (VQA) technologies is crucial for advancing accessibility, information retrieval, and linguistic diversity in digital content, facilitating better understanding and interaction with Urdu-language visual data. This initiative seeks to bridge the gap between textual and visual comprehension. We propose a new multi-task Urdu scene text dataset comprising over 1000 natural scene images, which can be used for text detection, recognition, and VQA tasks. We provide fine-grained annotations for text instances, addressing the limitations of previous datasets for facing arbitrary-shaped texts. By incorporating additional annotation points, this dataset facilitates the development and assessment of methods that can handle diverse text layouts, intricate shapes, and non-standard orientations commonly encountered in real-world scenarios. Besides, the VQA annotations make it the first benchmark for the Urdu Text VQA method, which can prompt the development of Urdu scene text understanding. The proposed dataset is available at: https://github.com/Hiba-MeiRuan/Urdu-VQA-Dataset-/tree/main
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2405.11437.pdf' target='_blank'>https://arxiv.org/pdf/2405.11437.pdf</a></span>   <span><a href='https://github.com/FadilaW/Swahili-STR-Dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fadila Wendigoundi Douamba, Jianjun Song, Ling Fu, Yuliang Liu, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11437">The First Swahili Language Scene Text Detection and Recognition Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition is essential in many applications, including automated translation, information retrieval, driving assistance, and enhancing accessibility for individuals with visual impairments. Much research has been done to improve the accuracy and performance of scene text detection and recognition models. However, most of this research has been conducted in the most common languages, English and Chinese. There is a significant gap in low-resource languages, especially the Swahili Language. Swahili is widely spoken in East African countries but is still an under-explored language in scene text recognition. No studies have been focused explicitly on Swahili natural scene text detection and recognition, and no dataset for Swahili language scene text detection and recognition is publicly available. We propose a comprehensive dataset of Swahili scene text images and evaluate the dataset on different scene text detection and recognition models. The dataset contains 976 images collected in different places and under various circumstances. Each image has its annotation at the word level. The proposed dataset can also serve as a benchmark dataset specific to the Swahili language for evaluating and comparing different approaches and fostering future research endeavors. The dataset is available on GitHub via this link: https://github.com/FadilaW/Swahili-STR-Dataset
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2405.10370.pdf' target='_blank'>https://arxiv.org/pdf/2405.10370.pdf</a></span>   <span><a href='https://groundedscenellm.github.io/grounded_3d-llm.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilun Chen, Shuai Yang, Haifeng Huang, Tai Wang, Runsen Xu, Ruiyuan Lyu, Dahua Lin, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10370">Grounded 3D-LLM with Referent Tokens</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prior studies on 3D scene understanding have primarily developed specialized models for specific tasks or required task-specific fine-tuning. In this study, we propose Grounded 3D-LLM, which explores the potential of 3D large multi-modal models (3D LMMs) to consolidate various 3D vision tasks within a unified generative framework. The model uses scene referent tokens as special noun phrases to reference 3D scenes, enabling it to handle sequences that interleave 3D and textual data. Per-task instruction-following templates are employed to ensure natural and diversity in translating 3D vision tasks into language formats. To facilitate the use of referent tokens in subsequent language modeling, we provide a large-scale, automatically curated grounded scene-text dataset with over 1 million phrase-to-region correspondences and introduce Contrastive Language-Scene Pre-training (CLASP) to perform phrase-level scene-text alignment using this data. Our comprehensive evaluation covers open-ended tasks like dense captioning and 3D question answering, alongside close-ended tasks such as object detection and language grounding. Experiments across multiple 3D benchmarks reveal the leading performance and the broad applicability of Grounded 3D-LLM. Code and datasets are available at the https://groundedscenellm.github.io/grounded_3d-llm.github.io.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2405.05841.pdf' target='_blank'>https://arxiv.org/pdf/2405.05841.pdf</a></span>   <span><a href='https://github.com/FaltingsA/SSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuan Gao, Yuxin Wang, Yadong Qu, Boqiang Zhang, Zixiao Wang, Jianjun Xu, Hongtao Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05841">Self-Supervised Pre-training with Symmetric Superimposition Modeling for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In text recognition, self-supervised pre-training emerges as a good solution to reduce dependence on expansive annotated real data. Previous studies primarily focus on local visual representation by leveraging mask image modeling or sequence contrastive learning. However, they omit modeling the linguistic information in text images, which is crucial for recognizing text. To simultaneously capture local character features and linguistic information in visual space, we propose Symmetric Superimposition Modeling (SSM). The objective of SSM is to reconstruct the direction-specific pixel and feature signals from the symmetrically superimposed input. Specifically, we add the original image with its inverted views to create the symmetrically superimposed inputs. At the pixel level, we reconstruct the original and inverted images to capture character shapes and texture-level linguistic context. At the feature level, we reconstruct the feature of the same original image and inverted image with different augmentations to model the semantic-level linguistic context and the local character discrimination. In our design, we disrupt the character shape and linguistic rules. Consequently, the dual-level reconstruction facilitates understanding character shapes and linguistic information from the perspective of visual texture and feature semantics. Experiments on various text recognition benchmarks demonstrate the effectiveness and generality of SSM, with 4.1% average performance gains and 86.6% new state-of-the-art average word accuracy on Union14M benchmarks. The code is available at https://github.com/FaltingsA/SSM.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2404.14135.pdf' target='_blank'>https://arxiv.org/pdf/2404.14135.pdf</a></span>   <span><a href='https://github.com/chunchet-ng/Text-in-the-Dark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Che-Tsung Lin, Chun Chet Ng, Zhi Qin Tan, Wan Jun Nah, Xinyu Wang, Jie Long Kew, Pohao Hsu, Shang Hong Lai, Chee Seng Chan, Christopher Zach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14135">Text in the Dark: Extremely Low-Light Text Image Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extremely low-light text images are common in natural scenes, making scene text detection and recognition challenging. One solution is to enhance these images using low-light image enhancement methods before text extraction. However, previous methods often do not try to particularly address the significance of low-level features, which are crucial for optimal performance on downstream scene text tasks. Further research is also hindered by the lack of extremely low-light text datasets. To address these limitations, we propose a novel encoder-decoder framework with an edge-aware attention module to focus on scene text regions during enhancement. Our proposed method uses novel text detection and edge reconstruction losses to emphasize low-level scene text features, leading to successful text extraction. Additionally, we present a Supervised Deep Curve Estimation (Supervised-DCE) model to synthesize extremely low-light images based on publicly available scene text datasets such as ICDAR15 (IC15). We also labeled texts in the extremely low-light See In the Dark (SID) and ordinary LOw-Light (LOL) datasets to allow for objective assessment of extremely low-light image enhancement through scene text tasks. Extensive experiments show that our model outperforms state-of-the-art methods in terms of both image quality and scene text metrics on the widely-used LOL, SID, and synthetic IC15 datasets. Code and dataset will be released publicly at https://github.com/chunchet-ng/Text-in-the-Dark.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2404.10652.pdf' target='_blank'>https://arxiv.org/pdf/2404.10652.pdf</a></span>   <span><a href='https://github.com/minhquan6203/ViTextVQA-Dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Van Nguyen, Dan Quang Tran, Huy Quang Pham, Thang Kien-Bao Nguyen, Nghia Hieu Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10652">ViTextVQA: A Large-Scale Visual Question Answering Dataset for Evaluating Vietnamese Text Comprehension in Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Question Answerinng (VQA) is a complicated task that requires the capability of simultaneously processing natural language and images. This task was initially researched with a focus on developing methods to help machines understand objects and scene contexts in images. However, some scene text that carries explicit information about the full content of the image is not mentioned. Along with the continuous development of the AI era, there have been many studies on the reading comprehension ability of VQA models in the world. Therefore, we introduce the first large-scale dataset in Vietnamese specializing in the ability to understand scene text, we call it ViTextVQA (\textbf{Vi}etnamese \textbf{Text}-based \textbf{V}isual \textbf{Q}uestion \textbf{A}nswering dataset) which contains \textbf{over 16,000} images and \textbf{over 50,000} questions with answers. To tackle this task efficiently, we propose ViTextBLIP-2, an novel multimodal feature fusion Method, which optimizes Vietnamese OCR-based VQA by integrating a frozen Vision Transformer, SwinTextSpotter OCR, and ViT5 LLM with a trainable Q-Former for multimodal feature fusion. Through experiments with various state-of-the-art models, we uncover the significance of the order in which tokens in OCR text are processed and selected to formulate answers. This finding helped us significantly improve the performance of the baseline models on the ViTextVQA dataset. Our dataset is available (https://github.com/minhquan6203/ViTextVQA-Dataset) for research purposes.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2403.13330.pdf' target='_blank'>https://arxiv.org/pdf/2403.13330.pdf</a></span>   <span><a href='https://github.com/SijieLiu518/SGENet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>LeoWu TomyEnrique, Xiangcheng Du, Kangliang Liu, Han Yuan, Zhao Zhou, Cheng Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13330">Efficient scene text image super-resolution with semantic guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text image super-resolution has significantly improved the accuracy of scene text recognition. However, many existing methods emphasize performance over efficiency and ignore the practical need for lightweight solutions in deployment scenarios. Faced with the issues, our work proposes an efficient framework called SGENet to facilitate deployment on resource-limited platforms. SGENet contains two branches: super-resolution branch and semantic guidance branch. We apply a lightweight pre-trained recognizer as a semantic extractor to enhance the understanding of text information. Meanwhile, we design the visual-semantic alignment module to achieve bidirectional alignment between image features and semantics, resulting in the generation of highquality prior guidance. We conduct extensive experiments on benchmark dataset, and the proposed SGENet achieves excellent performance with fewer computational costs. Code is available at https://github.com/SijieLiu518/SGENet
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2403.04473.pdf' target='_blank'>https://arxiv.org/pdf/2403.04473.pdf</a></span>   <span><a href='https://github.com/Yuliang-Liu/Monkey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04473">TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention with zero-initialization, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model's performance. Moreover, by expanding our model's capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. It also learns to perform screenshot tasks through finetuning. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9\% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding. Code will be released at https://github.com/Yuliang-Liu/Monkey.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2403.00303.pdf' target='_blank'>https://arxiv.org/pdf/2403.00303.pdf</a></span>   <span><a href='https://github.com/PriNing/ODM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Duan, Pei Fu, Shan Guo, Qianyi Jiang, Xiaoming Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00303">ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, text-image joint pre-training techniques have shown promising results in various tasks. However, in Optical Character Recognition (OCR) tasks, aligning text instances with their corresponding text regions in images poses a challenge, as it requires effective alignment between text and OCR-Text (referring to the text in images as OCR-Text to distinguish from the text in natural language) rather than a holistic understanding of the overall image content. In this paper, we propose a new pre-training method called OCR-Text Destylization Modeling (ODM) that transfers diverse styles of text found in images to a uniform style based on the text prompt. With ODM, we achieve better alignment between text and OCR-Text and enable pre-trained models to adapt to the complex and diverse styles of scene text detection and spotting tasks. Additionally, we have designed a new labeling generation method specifically for ODM and combined it with our proposed Text-Controller module to address the challenge of annotation costs in OCR tasks, allowing a larger amount of unlabeled data to participate in pre-training. Extensive experiments on multiple public datasets demonstrate that our method significantly improves performance and outperforms current pre-training methods in scene text detection and spotting tasks. Code is available at https://github.com/PriNing/ODM.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2402.13643.pdf' target='_blank'>https://arxiv.org/pdf/2402.13643.pdf</a></span>   <span><a href='https://github.com/MelosY/CAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingkun Yang, Biao Yang, Minghui Liao, Yingying Zhu, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13643">Class-Aware Mask-Guided Feature Refinement for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition is a rapidly developing field that faces numerous challenges due to the complexity and diversity of scene text, including complex backgrounds, diverse fonts, flexible arrangements, and accidental occlusions. In this paper, we propose a novel approach called Class-Aware Mask-guided feature refinement (CAM) to address these challenges. Our approach introduces canonical class-aware glyph masks generated from a standard font to effectively suppress background and text style noise, thereby enhancing feature discrimination. Additionally, we design a feature alignment and fusion module to incorporate the canonical mask guidance for further feature refinement for text recognition. By enhancing the alignment between the canonical mask feature and the text feature, the module ensures more effective fusion, ultimately leading to improved recognition performance. We first evaluate CAM on six standard text recognition benchmarks to demonstrate its effectiveness. Furthermore, CAM exhibits superiority over the state-of-the-art method by an average performance gain of 4.1% across six more challenging datasets, despite utilizing a smaller model size. Our study highlights the importance of incorporating canonical mask guidance and aligned feature refinement techniques for robust scene text recognition. The code is available at https://github.com/MelosY/CAM.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2401.17851.pdf' target='_blank'>https://arxiv.org/pdf/2401.17851.pdf</a></span>   <span><a href='https://github.com/Topdu/OpenOCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongkun Du, Zhineng Chen, Yuchen Su, Caiyan Jia, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.17851">Instruction-Guided Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal models have shown appealing performance in visual recognition tasks, as free-form text-guided training evokes the ability to understand fine-grained visual content. However, current models cannot be trivially applied to scene text recognition (STR) due to the compositional difference between natural and text images. We propose a novel instruction-guided scene text recognition (IGTR) paradigm that formulates STR as an instruction learning problem and understands text images by predicting character attributes, e.g., character frequency, position, etc. IGTR first devises $\left \langle condition,question,answer\right \rangle$ instruction triplets, providing rich and diverse descriptions of character attributes. To effectively learn these attributes through question-answering, IGTR develops a lightweight instruction encoder, a cross-modal feature fusion module and a multi-task answer head, which guides nuanced text image understanding. Furthermore, IGTR realizes different recognition pipelines simply by using different instructions, enabling a character-understanding-based text reasoning paradigm that differs from current methods considerably. Experiments on English and Chinese benchmarks show that IGTR outperforms existing models by significant margins, while maintaining a small model size and fast inference speed. Moreover, by adjusting the sampling of instructions, IGTR offers an elegant way to tackle the recognition of rarely appearing and morphologically similar characters, which were previous challenges. Code: https://github.com/Topdu/OpenOCR.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2401.14832.pdf' target='_blank'>https://arxiv.org/pdf/2401.14832.pdf</a></span>   <span><a href='https://github.com/blackprotoss/GSDM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shipeng Zhu, Pengfei Fang, Chenjie Zhu, Zuoyan Zhao, Qiang Xu, Hui Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14832">Text Image Inpainting via Global Structure-Guided Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world text can be damaged by corrosion issues caused by environmental or human factors, which hinder the preservation of the complete styles of texts, e.g., texture and structure. These corrosion issues, such as graffiti signs and incomplete signatures, bring difficulties in understanding the texts, thereby posing significant challenges to downstream applications, e.g., scene text recognition and signature identification. Notably, current inpainting techniques often fail to adequately address this problem and have difficulties restoring accurate text images along with reasonable and consistent styles. Formulating this as an open problem of text image inpainting, this paper aims to build a benchmark to facilitate its study. In doing so, we establish two specific text inpainting datasets which contain scene text images and handwritten text images, respectively. Each of them includes images revamped by real-life and synthetic datasets, featuring pairs of original images, corrupted images, and other assistant information. On top of the datasets, we further develop a novel neural framework, Global Structure-guided Diffusion Model (GSDM), as a potential solution. Leveraging the global structure of the text as a prior, the proposed GSDM develops an efficient diffusion model to recover clean texts. The efficacy of our approach is demonstrated by thorough empirical study, including a substantial boost in both recognition accuracy and image quality. These findings not only highlight the effectiveness of our method but also underscore its potential to enhance the broader field of text image understanding and processing. Code and datasets are available at: https://github.com/blackprotoss/GSDM.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2401.10110.pdf' target='_blank'>https://arxiv.org/pdf/2401.10110.pdf</a></span>   <span><a href='https://github.com/cxfyxl/VIPTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianfu Cheng, Weixiao Zhou, Xiang Li, Jian Yang, Hang Zhang, Tao Sun, Wei Zhang, Yuying Mai, Tongliang Li, Xiaoming Chen, Zhoujun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10110">SVIPTR: Fast and Efficient Scene Text Recognition with Vision Permutable Extractor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Text Recognition (STR) is an important and challenging upstream task for building structured information databases, that involves recognizing text within images of natural scenes. Although current state-of-the-art (SOTA) models for STR exhibit high performance, they typically suffer from low inference efficiency due to their reliance on hybrid architectures comprised of visual encoders and sequence decoders. In this work, we propose a VIsion Permutable extractor for fast and efficient Scene Text Recognition (SVIPTR), which achieves an impressive balance between high performance and rapid inference speeds in the domain of STR. Specifically, SVIPTR leverages a visual-semantic extractor with a pyramid structure, characterized by the Permutation and combination of local and global self-attention layers. This design results in a lightweight and efficient model and its inference is insensitive to input length. Extensive experimental results on various standard datasets for both Chinese and English scene text recognition validate the superiority of SVIPTR. Notably, the SVIPTR-T (Tiny) variant delivers highly competitive accuracy on par with other lightweight models and achieves SOTA inference speeds. Meanwhile, the SVIPTR-L (Large) attains SOTA accuracy in single-encoder-type models, while maintaining a low parameter count and favorable inference speed. Our proposed method provides a compelling solution for the STR challenge, which greatly benefits real-world applications requiring fast and efficient STR. The code is publicly available at https://github.com/cxfyxl/VIPTR.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2401.07641.pdf' target='_blank'>https://arxiv.org/pdf/2401.07641.pdf</a></span>   <span><a href='https://github.com/mxin262/SwinTextSpotterv2' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/mxin262/SwinTextSpotterv2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxin Huang, Dezhi Peng, Hongliang Li, Zhenghao Peng, Chongyu Liu, Dahua Lin, Yuliang Liu, Xiang Bai, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.07641">SwinTextSpotter v2: Towards Better Synergy for Scene Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end scene text spotting, which aims to read the text in natural images, has garnered significant attention in recent years. However, recent state-of-the-art methods usually incorporate detection and recognition simply by sharing the backbone, which does not directly take advantage of the feature interaction between the two tasks. In this paper, we propose a new end-to-end scene text spotting framework termed SwinTextSpotter v2, which seeks to find a better synergy between text detection and recognition. Specifically, we enhance the relationship between two tasks using novel Recognition Conversion and Recognition Alignment modules. Recognition Conversion explicitly guides text localization through recognition loss, while Recognition Alignment dynamically extracts text features for recognition through the detection predictions. This simple yet effective design results in a concise framework that requires neither an additional rectification module nor character-level annotations for the arbitrarily-shaped text. Furthermore, the parameters of the detector are greatly reduced without performance degradation by introducing a Box Selection Schedule. Qualitative and quantitative experiments demonstrate that SwinTextSpotter v2 achieved state-of-the-art performance on various multilingual (English, Chinese, and Vietnamese) benchmarks. The code will be available at \href{https://github.com/mxin262/SwinTextSpotterv2}{SwinTextSpotter v2}.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2401.00028.pdf' target='_blank'>https://arxiv.org/pdf/2401.00028.pdf</a></span>   <span><a href='https://github.com/large-ocr-model/large-ocr-model.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Miao Rang, Zhenni Bi, Chuanjian Liu, Yunhe Wang, Kai Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00028">An Empirical Study of Scaling Law for OCR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The laws of model size, data volume, computation and model performance have been extensively studied in the field of Natural Language Processing (NLP). However, the scaling laws in Optical Character Recognition (OCR) have not yet been investigated. To address this, we conducted comprehensive studies that involved examining the correlation between performance and the scale of models, data volume and computation in the field of text recognition.Conclusively, the study demonstrates smooth power laws between performance and model size, as well as training data volume, when other influencing factors are held constant. Additionally, we have constructed a large-scale dataset called REBU-Syn, which comprises 6 million real samples and 18 million synthetic samples. Based on our scaling law and new dataset, we have successfully trained a scene text recognition model, achieving a new state-ofthe-art on 6 common test benchmarks with a top-1 average accuracy of 97.42%. The models and dataset are publicly available at https://github.com/large-ocr-model/large-ocr-model.github.io.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2503.20198.pdf' target='_blank'>https://arxiv.org/pdf/2503.20198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, Min Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20198">Beyond Words: Advancing Long-Text Image Generation via Multimodal Autoregressive Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in autoregressive and diffusion models have led to strong performance in image generation with short scene text words. However, generating coherent, long-form text in images, such as paragraphs in slides or documents, remains a major challenge for current generative models. We present the first work specifically focused on long text image generation, addressing a critical gap in existing text-to-image systems that typically handle only brief phrases or single sentences. Through comprehensive analysis of state-of-the-art autoregressive generation models, we identify the image tokenizer as a critical bottleneck in text generating quality. To address this, we introduce a novel text-focused, binary tokenizer optimized for capturing detailed scene text features. Leveraging our tokenizer, we develop \ModelName, a multimodal autoregressive model that excels in generating high-quality long-text images with unprecedented fidelity. Our model offers robust controllability, enabling customization of text properties such as font style, size, color, and alignment. Extensive experiments demonstrate that \ModelName~significantly outperforms SD3.5 Large~\cite{sd3} and GPT4o~\cite{gpt4o} with DALL-E 3~\cite{dalle3} in generating long text accurately, consistently, and flexibly. Beyond its technical achievements, \ModelName~opens up exciting opportunities for innovative applications like interleaved document and PowerPoint generation, establishing a new frontier in long-text image generating.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2411.16713.pdf' target='_blank'>https://arxiv.org/pdf/2411.16713.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taewook Kim, Ze Wang, Zhengyuan Yang, Jiang Wang, Lijuan Wang, Zicheng Liu, Qiang Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16713">Conditional Text-to-Image Generation with Reference Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image diffusion models have demonstrated tremendous success in synthesizing visually stunning images given textual instructions. Despite remarkable progress in creating high-fidelity visuals, text-to-image models can still struggle with precisely rendering subjects, such as text spelling. To address this challenge, this paper explores using additional conditions of an image that provides visual guidance of the particular subjects for diffusion models to generate. In addition, this reference condition empowers the model to be conditioned in ways that the vocabularies of the text tokenizer cannot adequately represent, and further extends the model's generalization to novel capabilities such as generating non-English text spellings. We develop several small-scale expert plugins that efficiently endow a Stable Diffusion model with the capability to take different references. Each plugin is trained with auxiliary networks and loss functions customized for applications such as English scene-text generation, multi-lingual scene-text generation, and logo-image generation. Our expert plugins demonstrate superior results than the existing methods on all tasks, each containing only 28.55M trainable parameters.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2407.13219.pdf' target='_blank'>https://arxiv.org/pdf/2407.13219.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Feng, Xin Wang, Hong Chen, Zeyang Zhang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13219">Multi-sentence Video Grounding for Long Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation has witnessed great success recently, but their application in generating long videos still remains challenging due to the difficulty in maintaining the temporal consistency of generated videos and the high memory cost during generation. To tackle the problems, in this paper, we propose a brave and new idea of Multi-sentence Video Grounding for Long Video Generation, connecting the massive video moment retrieval to the video generation task for the first time, providing a new paradigm for long video generation. The method of our work can be summarized as three steps: (i) We design sequential scene text prompts as the queries for video grounding, utilizing the massive video moment retrieval to search for video moment segments that meet the text requirements in the video database. (ii) Based on the source frames of retrieved video moment segments, we adopt video editing methods to create new video content while preserving the temporal consistency of the retrieved video. Since the editing can be conducted segment by segment, and even frame by frame, it largely reduces the memory cost. (iii) We also attempt video morphing and personalized generation methods to improve the subject consistency of long video generation, providing ablation experimental results for the subtasks of long video generation. Our approach seamlessly extends the development in image/video editing, video morphing and personalized generation, and video grounding to the long video generation, offering effective solutions for generating long videos at low memory cost.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2407.17020.pdf' target='_blank'>https://arxiv.org/pdf/2407.17020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyang Yu, Teng Fu, Bin Li, Xiangyang Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17020">EAFormer: Scene Text Segmentation with Edge-Aware Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text segmentation aims at cropping texts from scene images, which is usually used to help generative models edit or remove texts. The existing text segmentation methods tend to involve various text-related supervisions for better performance. However, most of them ignore the importance of text edges, which are significant for downstream applications. In this paper, we propose Edge-Aware Transformers, termed EAFormer, to segment texts more accurately, especially at the edge of texts. Specifically, we first design a text edge extractor to detect edges and filter out edges of non-text areas. Then, we propose an edge-guided encoder to make the model focus more on text edges. Finally, an MLP-based decoder is employed to predict text masks. We have conducted extensive experiments on commonly-used benchmarks to verify the effectiveness of EAFormer. The experimental results demonstrate that the proposed method can perform better than previous methods, especially on the segmentation of text edges. Considering that the annotations of several benchmarks (e.g., COCO_TS and MLT_S) are not accurate enough to fairly evaluate our methods, we have relabeled these datasets. Through experiments, we observe that our method can achieve a higher performance improvement when more accurate annotations are used for training.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2404.19652.pdf' target='_blank'>https://arxiv.org/pdf/2404.19652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuliang Liu, Mingxin Huang, Hao Yan, Linger Deng, Weijia Wu, Hao Lu, Chunhua Shen, Lianwen Jin, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19652">VimTS: A Unified Video and Image Text Spotter for Enhancing the Cross-domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text spotting, a task involving the extraction of textual information from image or video sequences, faces challenges in cross-domain adaption, such as image-to-image and image-to-video generalization. In this paper, we introduce a new method, termed VimTS, which enhances the generalization ability of the model by achieving better synergy among different tasks. Typically, we propose a Prompt Queries Generation Module and a Tasks-aware Adapter to effectively convert the original single-task model into a multi-task model suitable for both image and video scenarios with minimal additional parameters. The Prompt Queries Generation Module facilitates explicit interaction between different tasks, while the Tasks-aware Adapter helps the model dynamically learn suitable features for each task. Additionally, to further enable the model to learn temporal information at a lower cost, we propose a synthetic video text dataset (VTD-368k) by leveraging the Content Deformation Fields (CoDeF) algorithm. Notably, our method outperforms the state-of-the-art method by an average of 2.6% in six cross-domain benchmarks such as TT-to-IC15, CTW1500-to-TT, and TT-to-CTW1500. For video-level cross-domain adaption, our method even surpasses the previous end-to-end video spotting method in ICDAR2015 video and DSText v2 by an average of 5.5% on the MOTA metric, using only image-level data. We further demonstrate that existing Large Multimodal Models exhibit limitations in generating cross-domain scene text spotting, in contrast to our VimTS model which requires significantly fewer parameters and data. The code and datasets will be made available at the https://VimTextSpotter.github.io.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2405.19765.pdf' target='_blank'>https://arxiv.org/pdf/2405.19765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Wan, Chengquan Zhang, Pengyuan Lyu, Sen Fan, Zihan Ni, Kun Yao, Errui Ding, Jingdong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19765">Towards Unified Multi-granularity Text Detection with Interactive Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing OCR engines or document image analysis systems typically rely on training separate models for text detection in varying scenarios and granularities, leading to significant computational complexity and resource demands. In this paper, we introduce "Detect Any Text" (DAT), an advanced paradigm that seamlessly unifies scene text detection, layout analysis, and document page detection into a cohesive, end-to-end model. This design enables DAT to efficiently manage text instances at different granularities, including *word*, *line*, *paragraph* and *page*. A pivotal innovation in DAT is the across-granularity interactive attention module, which significantly enhances the representation learning of text instances at varying granularities by correlating structural information across different text queries. As a result, it enables the model to achieve mutually beneficial detection performances across multiple text granularities. Additionally, a prompt-based segmentation module refines detection outcomes for texts of arbitrary curvature and complex layouts, thereby improving DAT's accuracy and expanding its real-world applicability. Experimental results demonstrate that DAT achieves state-of-the-art performances across a variety of text-related benchmarks, including multi-oriented/arbitrarily-shaped scene text detection, document layout analysis and page detection tasks.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2403.20271.pdf' target='_blank'>https://arxiv.org/pdf/2403.20271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.20271">Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present the Draw-and-Understand framework, exploring how to integrate visual prompting understanding capabilities into Multimodal Large Language Models (MLLMs). Visual prompts allow users to interact through multi-modal instructions, enhancing the models' interactivity and fine-grained image comprehension. In this framework, we propose a general architecture adaptable to different pre-trained MLLMs, enabling it to recognize various types of visual prompts (such as points, bounding boxes, and free-form shapes) alongside language understanding. Additionally, we introduce MDVP-Instruct-Data, a multi-domain dataset featuring 1.2 million image-visual prompt-text triplets, including natural images, document images, scene text images, mobile/web screenshots, and remote sensing images. Building on this dataset, we introduce MDVP-Bench, a challenging benchmark designed to evaluate a model's ability to understand visual prompting instructions. The experimental results demonstrate that our framework can be easily and effectively applied to various MLLMs, such as SPHINX-X and LLaVA. After training with MDVP-Instruct-Data and image-level instruction datasets, our models exhibit impressive multimodal interaction capabilities and pixel-level understanding, while maintaining their image-level visual perception performance.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2402.08017.pdf' target='_blank'>https://arxiv.org/pdf/2402.08017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashish Shenoy, Yichao Lu, Srihari Jayakumar, Debojeet Chatterjee, Mohsen Moslehpour, Pierce Chuang, Abhay Harpale, Vikas Bhardwaj, Di Xu, Shicong Zhao, Longfang Zhao, Ankit Ramchandani, Xin Luna Dong, Anuj Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08017">Lumos : Empowering Multimodal LLMs with Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the core of Lumos is a Scene Text Recognition (STR) component that extracts text from first person point-of-view images, the output of which is used to augment input to a Multimodal Large Language Model (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2404.00852.pdf' target='_blank'>https://arxiv.org/pdf/2404.00852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hieu Nguyen, Cong-Hoang Ta, Phuong-Thuy Le-Nguyen, Minh-Triet Tran, Trung-Nghia Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00852">Ensemble Learning for Vietnamese Scene Text Spotting in Urban Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a simple yet efficient ensemble learning framework for Vietnamese scene text spotting. Leveraging the power of ensemble learning, which combines multiple models to yield more accurate predictions, our approach aims to significantly enhance the performance of scene text spotting in challenging urban settings. Through experimental evaluations on the VinText dataset, our proposed method achieves a significant improvement in accuracy compared to existing methods with an impressive accuracy of 5%. These results unequivocally demonstrate the efficacy of ensemble learning in the context of Vietnamese scene text spotting in urban environments, highlighting its potential for real world applications, such as text detection and recognition in urban signage, advertisements, and various text-rich urban scenes.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2511.20190.pdf' target='_blank'>https://arxiv.org/pdf/2511.20190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haibin He, Qihuang Zhong, Juhua Liu, Bo Du, Peng Wang, Jing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20190">SFA: Scan, Focus, and Amplify toward Guidance-aware Answering for Video TextVQA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video text-based visual question answering (Video TextVQA) task aims to answer questions about videos by leveraging the visual text appearing within the videos. This task poses significant challenges, requiring models to accurately perceive and comprehend scene text that varies in scale, orientation, and clarity across frames, while effectively integrating temporal and semantic context to generate precise answers. Moreover, the model must identify question-relevant textual cues and filter out redundant or irrelevant information to ensure answering is guided by the most relevant and informative cues. To address these challenges, we propose SFA, a training-free framework and the first Video-LLM-based method tailored for Video TextVQA, motivated by the human process of answering questions. By adaptively scanning video frames, selectively focusing on key regions, and directly amplifying them, SFA effectively guides the Video-LLM's attention toward essential cues, enabling it to generate more accurate answers. SFA achieves new state-of-the-art results across several public Video TextVQA datasets and surpasses previous methods by a substantial margin, demonstrating its effectiveness and generalizability.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2403.13307.pdf' target='_blank'>https://arxiv.org/pdf/2403.13307.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peishan Cong, Ziyi Wang, Zhiyang Dou, Yiming Ren, Wei Yin, Kai Cheng, Yujing Sun, Xiaoxiao Long, Xinge Zhu, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13307">LaserHuman: Language-guided Scene-aware Human Motion Generation in Free Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-guided scene-aware human motion generation has great significance for entertainment and robotics. In response to the limitations of existing datasets, we introduce LaserHuman, a pioneering dataset engineered to revolutionize Scene-Text-to-Motion research. LaserHuman stands out with its inclusion of genuine human motions within 3D environments, unbounded free-form natural language descriptions, a blend of indoor and outdoor scenarios, and dynamic, ever-changing scenes. Diverse modalities of capture data and rich annotations present great opportunities for the research of conditional motion generation, and can also facilitate the development of real-life applications. Moreover, to generate semantically consistent and physically plausible human motions, we propose a multi-conditional diffusion model, which is simple but effective, achieving state-of-the-art performance on existing datasets.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2503.18746.pdf' target='_blank'>https://arxiv.org/pdf/2503.18746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Zhang, Chang Liu, Jin Wei, Xiaomeng Yang, Yu Zhou, Can Ma, Xiangyang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18746">Linguistics-aware Masked Image Modeling for Self-supervised Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text images are unique in their dual nature, encompassing both visual and linguistic information. The visual component encompasses structural and appearance-based features, while the linguistic dimension incorporates contextual and semantic elements. In scenarios with degraded visual quality, linguistic patterns serve as crucial supplements for comprehension, highlighting the necessity of integrating both aspects for robust scene text recognition (STR). Contemporary STR approaches often use language models or semantic reasoning modules to capture linguistic features, typically requiring large-scale annotated datasets. Self-supervised learning, which lacks annotations, presents challenges in disentangling linguistic features related to the global context. Typically, sequence contrastive learning emphasizes the alignment of local features, while masked image modeling (MIM) tends to exploit local structures to reconstruct visual patterns, resulting in limited linguistic knowledge. In this paper, we propose a Linguistics-aware Masked Image Modeling (LMIM) approach, which channels the linguistic information into the decoding process of MIM through a separate branch. Specifically, we design a linguistics alignment module to extract vision-independent features as linguistic guidance using inputs with different visual appearances. As features extend beyond mere visual structures, LMIM must consider the global context to achieve reconstruction. Extensive experiments on various benchmarks quantitatively demonstrate our state-of-the-art performance, and attention visualizations qualitatively show the simultaneous capture of both visual and linguistic information.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2506.04983.pdf' target='_blank'>https://arxiv.org/pdf/2506.04983.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangyang Zhong, Ji Qi, Yuan Yao, Pengxin Luo, Yunfeng Yan, Donglian Qi, Zhiyuan Liu, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04983">TextVidBench: A Benchmark for Long Video Scene Text Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent progress on the short-video Text-Visual Question Answering (ViteVQA) task - largely driven by benchmarks such as M4-ViteVQA - existing datasets still suffer from limited video duration and narrow evaluation scopes, making it difficult to adequately assess the growing capabilities of powerful multimodal large language models (MLLMs). To address these limitations, we introduce TextVidBench, the first benchmark specifically designed for long-video text question answering (>3 minutes). TextVidBench makes three key contributions: 1) Cross-domain long-video coverage: Spanning 9 categories (e.g., news, sports, gaming), with an average video length of 2306 seconds, enabling more realistic evaluation of long-video understanding. 2) A three-stage evaluation framework: "Text Needle-in-Haystack -> Temporal Grounding -> Text Dynamics Captioning". 3) High-quality fine-grained annotations: Containing over 5,000 question-answer pairs with detailed semantic labeling. Furthermore, we propose an efficient paradigm for improving large models through: (i) introducing the IT-Rope mechanism and temporal prompt engineering to enhance temporal perception, (ii) adopting non-uniform positional encoding to better handle long video sequences, and (iii) applying lightweight fine-tuning on video-text data. Extensive experiments on multiple public datasets as well as TextVidBench demonstrate that our new benchmark presents significant challenges to existing models, while our proposed method offers valuable insights into improving long-video scene text understanding capabilities.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2409.13576.pdf' target='_blank'>https://arxiv.org/pdf/2409.13576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingtao Lin, Heqian Qiu, Lanxiao Wang, Ruihang Wang, Linfeng Xu, Hongliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13576">Region Prompt Tuning: Fine-grained Scene Text Detection Utilizing Region Text Prompt</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in prompt tuning have successfully adapted large-scale models like Contrastive Language-Image Pre-trained (CLIP) for downstream tasks such as scene text detection. Typically, text prompt complements the text encoder's input, focusing on global features while neglecting fine-grained details, leading to fine-grained text being ignored in task of scene text detection. In this paper, we propose the region prompt tuning (RPT) method for fine-grained scene text detection, where region text prompt proposed would help focus on fine-grained features. Region prompt tuning method decomposes region text prompt into individual characters and splits visual feature map into region visual tokens, creating a one-to-one correspondence between characters and tokens. This allows a character matches the local features of a token, thereby avoiding the omission of detailed features and fine-grained text. To achieve this, we introduce a sharing position embedding to link each character with its corresponding token and employ a bidirectional distance loss to align each region text prompt character with the target ``text''. To refine the information at fine-grained level, we implement character-token level interactions before and after encoding. Our proposed method combines a general score map from the image-text process with a region score map derived from character-token matching, producing a final score map that could balance the global and local features and be fed into DBNet to detect the text. Experiments on benchmarks like ICDAR2015, TotalText, and CTW1500 demonstrate RPT impressive performance, underscoring its effectiveness for scene text detection.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2506.05551.pdf' target='_blank'>https://arxiv.org/pdf/2506.05551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Shu, Hangui Lin, Yexin Liu, Yan Zhang, Gangyan Zeng, Yan Li, Yu Zhou, Ser-Nam Lim, Harry Yang, Nicu Sebe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05551">When Semantics Mislead Vision: Mitigating Large Multimodal Models Hallucinations in Scene Text Spotting and Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Multimodal Models (LMMs) have achieved impressive progress in visual perception and reasoning. However, when confronted with visually ambiguous or non-semantic scene text, they often struggle to accurately spot and understand the content, frequently generating semantically plausible yet visually incorrect answers, which we refer to as semantic hallucination. In this work, we investigate the underlying causes of semantic hallucination and identify a key finding: Transformer layers in LLM with stronger attention focus on scene text regions are less prone to producing semantic hallucinations. Thus, we propose a training-free semantic hallucination mitigation framework comprising two key components: (1) ZoomText, a coarse-to-fine strategy that identifies potential text regions without external detectors; and (2) Grounded Layer Correction, which adaptively leverages the internal representations from layers less prone to hallucination to guide decoding, correcting hallucinated outputs for non-semantic samples while preserving the semantics of meaningful ones. To enable rigorous evaluation, we introduce TextHalu-Bench, a benchmark of over 1,730 samples spanning both semantic and non-semantic cases, with manually curated question-answer pairs designed to probe model hallucinations. Extensive experiments demonstrate that our method not only effectively mitigates semantic hallucination but also achieves strong performance on public benchmarks for scene text spotting and understanding.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2512.03574.pdf' target='_blank'>https://arxiv.org/pdf/2512.03574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fuxiang Yang, Tonghua Su, Donglin Di, Yin Chen, Xiangqian Wu, Zhongjie Wang, Lei Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03574">Global-Local Aware Scene Text Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Text Editing (STE) involves replacing text in a scene image with new target text while preserving both the original text style and background texture. Existing methods suffer from two major challenges: inconsistency and length-insensitivity. They often fail to maintain coherence between the edited local patch and the surrounding area, and they struggle to handle significant differences in text length before and after editing. To tackle these challenges, we propose an end-to-end framework called Global-Local Aware Scene Text Editing (GLASTE), which simultaneously incorporates high-level global contextual information along with delicate local features. Specifically, we design a global-local combination structure, joint global and local losses, and enhance text image features to ensure consistency in text style within local patches while maintaining harmony between local and global areas. Additionally, we express the text style as a vector independent of the image size, which can be transferred to target text images of various sizes. We use an affine fusion to fill target text images into the editing patch while maintaining their aspect ratio unchanged. Extensive experiments on real-world datasets validate that our GLASTE model outperforms previous methods in both quantitative metrics and qualitative results and effectively mitigates the two challenges.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2409.14483.pdf' target='_blank'>https://arxiv.org/pdf/2409.14483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minyi Zhao, Yang Wang, Jihong Guan, Shuigeng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14483">One Model for Two Tasks: Cooperatively Recognizing and Recovering Low-Resolution Scene Text Images by Iterative Mutual Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition (STR) from high-resolution (HR) images has been significantly successful, however text reading on low-resolution (LR) images is still challenging due to insufficient visual information. Therefore, recently many scene text image super-resolution (STISR) models have been proposed to generate super-resolution (SR) images for the LR ones, then STR is done on the SR images, which thus boosts recognition performance. Nevertheless, these methods have two major weaknesses. On the one hand, STISR approaches may generate imperfect or even erroneous SR images, which mislead the subsequent recognition of STR models. On the other hand, as the STISR and STR models are jointly optimized, to pursue high recognition accuracy, the fidelity of SR images may be spoiled. As a result, neither the recognition performance nor the fidelity of STISR models are desirable. Then, can we achieve both high recognition performance and good fidelity? To this end, in this paper we propose a novel method called IMAGE (the abbreviation of Iterative MutuAl GuidancE) to effectively recognize and recover LR scene text images simultaneously. Concretely, IMAGE consists of a specialized STR model for recognition and a tailored STISR model to recover LR images, which are optimized separately. And we develop an iterative mutual guidance mechanism, with which the STR model provides high-level semantic information as clue to the STISR model for better super-resolution, meanwhile the STISR model offers essential low-level pixel clue to the STR model for more accurate recognition. Extensive experiments on two LR datasets demonstrate the superiority of our method over the existing works on both recognition performance and super-resolution fidelity.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2412.10159.pdf' target='_blank'>https://arxiv.org/pdf/2412.10159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Lyu, Wei Wang, Dongbao Yang, Jinwen Zhong, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10159">Arbitrary Reading Order Scene Text Spotter with Local Semantics Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text spotting has attracted the enthusiasm of relative researchers in recent years. Most existing scene text spotters follow the detection-then-recognition paradigm, where the vanilla detection module hardly determines the reading order and leads to failure recognition. After rethinking the auto-regressive scene text recognition method, we find that a well-trained recognizer can implicitly perceive the local semantics of all characters in a complete word or a sentence without a character-level detection module. Local semantic knowledge not only includes text content but also spatial information in the right reading order. Motivated by the above analysis, we propose the Local Semantics Guided scene text Spotter (LSGSpotter), which auto-regressively decodes the position and content of characters guided by the local semantics. Specifically, two effective modules are proposed in LSGSpotter. On the one hand, we design a Start Point Localization Module (SPLM) for locating text start points to determine the right reading order. On the other hand, a Multi-scale Adaptive Attention Module (MAAM) is proposed to adaptively aggregate text features in a local area. In conclusion, LSGSpotter achieves the arbitrary reading order spotting task without the limitation of sophisticated detection, while alleviating the cost of computational resources with the grid sampling strategy. Extensive experiment results show LSGSpotter achieves state-of-the-art performance on the InverseText benchmark. Moreover, our spotter demonstrates superior performance on English benchmarks for arbitrary-shaped text, achieving improvements of 0.7\% and 2.5\% on Total-Text and SCUT-CTW1500, respectively. These results validate our text spotter is effective for scene texts in arbitrary reading order and shape.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2410.10133.pdf' target='_blank'>https://arxiv.org/pdf/2410.10133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weichao Zeng, Yan Shu, Zhenhang Li, Dongbao Yang, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10133">TextCtrl: Diffusion-based Scene Text Editing with Prior Guidance Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Centred on content modification and style preservation, Scene Text Editing (STE) remains a challenging task despite considerable progress in text-to-image synthesis and text-driven image manipulation recently. GAN-based STE methods generally encounter a common issue of model generalization, while Diffusion-based STE methods suffer from undesired style deviations. To address these problems, we propose TextCtrl, a diffusion-based method that edits text with prior guidance control. Our method consists of two key components: (i) By constructing fine-grained text style disentanglement and robust text glyph structure representation, TextCtrl explicitly incorporates Style-Structure guidance into model design and network training, significantly improving text style consistency and rendering accuracy. (ii) To further leverage the style prior, a Glyph-adaptive Mutual Self-attention mechanism is proposed which deconstructs the implicit fine-grained features of the source image to enhance style consistency and vision quality during inference. Furthermore, to fill the vacancy of the real-world STE evaluation benchmark, we create the first real-world image-pair dataset termed ScenePair for fair comparisons. Experiments demonstrate the effectiveness of TextCtrl compared with previous methods concerning both style fidelity and text accuracy.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2407.06084.pdf' target='_blank'>https://arxiv.org/pdf/2407.06084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dejie Yang, Zhu Xu, Wentao Mo, Qingchao Chen, Siyuan Huang, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06084">3D Vision and Language Pretraining with Large-Scale Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Vision-Language Pre-training (3D-VLP) aims to provide a pre-train model which can bridge 3D scenes with natural language, which is an important technique for embodied intelligence. However, current 3D-VLP datasets are hindered by limited scene-level diversity and insufficient fine-grained annotations (only 1.2K scenes and 280K textual annotations in ScanScribe), primarily due to the labor-intensive of collecting and annotating 3D scenes. To overcome these obstacles, we construct SynVL3D, a comprehensive synthetic scene-text corpus with 10K indoor scenes and 1M descriptions at object, view, and room levels, which has the advantages of diverse scene data, rich textual descriptions, multi-grained 3D-text associations, and low collection cost. Utilizing the rich annotations in SynVL3D, we pre-train a simple and unified Transformer for aligning 3D and language with multi-grained pretraining tasks. Moreover, we propose a synthetic-to-real domain adaptation in downstream task fine-tuning process to address the domain shift. Through extensive experiments, we verify the effectiveness of our model design by achieving state-of-the-art performance on downstream tasks including visual grounding, dense captioning, and question answering.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2412.02210.pdf' target='_blank'>https://arxiv.org/pdf/2412.02210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhibo Yang, Jun Tang, Zhaohai Li, Pengfei Wang, Jianqiang Wan, Humen Zhong, Xuejing Liu, Mingkun Yang, Peng Wang, Shuai Bai, LianWen Jin, Junyang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02210">CC-OCR: A Comprehensive and Challenging OCR Benchmark for Evaluating Large Multimodal Models in Literacy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Multimodal Models (LMMs) have demonstrated impressive performance in recognizing document images with natural language instructions. However, it remains unclear to what extent capabilities in literacy with rich structure and fine-grained visual challenges. The current landscape lacks a comprehensive benchmark to effectively measure the literate capabilities of LMMs. Existing benchmarks are often limited by narrow scenarios and specified tasks. To this end, we introduce CC-OCR, a comprehensive benchmark that possesses a diverse range of scenarios, tasks, and challenges. CC-OCR comprises four OCR-centric tracks: multi-scene text reading, multilingual text reading, document parsing, and key information extraction. It includes 39 subsets with 7,058 full annotated images, of which 41% are sourced from real applications, and released for the first time. We evaluate nine prominent LMMs and reveal both the strengths and weaknesses of these models, particularly in text grounding, multi-orientation, and hallucination of repetition. CC-OCR aims to comprehensively evaluate the capabilities of LMMs on OCR-centered tasks, facilitating continued progress in this crucial area.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2405.04682.pdf' target='_blank'>https://arxiv.org/pdf/2405.04682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hritik Bansal, Yonatan Bitton, Michal Yarom, Idan Szpektor, Aditya Grover, Kai-Wei Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04682">TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most of these text-to-video (T2V) generative models often produce single-scene video clips that depict an entity performing a particular action (e.g., 'a red panda climbing a tree'). However, it is pertinent to generate multi-scene videos since they are ubiquitous in the real-world (e.g., 'a red panda climbing a tree' followed by 'the red panda sleeps on the top of the tree'). To generate multi-scene videos from the pretrained T2V model, we introduce a simple and effective Time-Aligned Captions (TALC) framework. Specifically, we enhance the text-conditioning mechanism in the T2V architecture to recognize the temporal alignment between the video scenes and scene descriptions. For instance, we condition the visual features of the earlier and later scenes of the generated video with the representations of the first scene description (e.g., 'a red panda climbing a tree') and second scene description (e.g., 'the red panda sleeps on the top of the tree'), respectively. As a result, we show that the T2V model can generate multi-scene videos that adhere to the multi-scene text descriptions and be visually consistent (e.g., entity and background). Further, we finetune the pretrained T2V model with multi-scene video-text data using the TALC framework. We show that the TALC-finetuned model outperforms the baseline by achieving a relative gain of 29% in the overall score, which averages visual consistency and text adherence using human evaluation.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2510.02787.pdf' target='_blank'>https://arxiv.org/pdf/2510.02787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Zdenek, Wataru Shimoda, Kota Yamaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02787">OTR: Synthesizing Overlay Text Dataset for Text Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text removal is a crucial task in computer vision with applications such as privacy preservation, image editing, and media reuse. While existing research has primarily focused on scene text removal in natural images, limitations in current datasets hinder out-of-domain generalization or accurate evaluation. In particular, widely used benchmarks such as SCUT-EnsText suffer from ground truth artifacts due to manual editing, overly simplistic text backgrounds, and evaluation metrics that do not capture the quality of generated results. To address these issues, we introduce an approach to synthesizing a text removal benchmark applicable to domains other than scene texts. Our dataset features text rendered on complex backgrounds using object-aware placement and vision-language model-generated content, ensuring clean ground truth and challenging text removal scenarios. The dataset is available at https://huggingface.co/datasets/cyberagent/OTR .
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2501.15558.pdf' target='_blank'>https://arxiv.org/pdf/2501.15558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Chen, Xinyu Guo, Yadong Li, Tao Zhang, Mingan Lin, Dongdong Kuang, Youwei Zhang, Lingfeng Ming, Fengyu Zhang, Yuran Wang, Jianhua Xu, Zenan Zhou, Weipeng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15558">Ocean-OCR: Towards General OCR Application via a Vision-Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal large language models (MLLMs) have shown impressive capabilities across various domains, excelling in processing and understanding information from multiple modalities. Despite the rapid progress made previously, insufficient OCR ability hinders MLLMs from excelling in text-related tasks. In this paper, we present \textbf{Ocean-OCR}, a 3B MLLM with state-of-the-art performance on various OCR scenarios and comparable understanding ability on general tasks. We employ Native Resolution ViT to enable variable resolution input and utilize a substantial collection of high-quality OCR datasets to enhance the model performance. We demonstrate the superiority of Ocean-OCR through comprehensive experiments on open-source OCR benchmarks and across various OCR scenarios. These scenarios encompass document understanding, scene text recognition, and handwritten recognition, highlighting the robust OCR capabilities of Ocean-OCR. Note that Ocean-OCR is the first MLLM to outperform professional OCR models such as TextIn and PaddleOCR.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2507.15085.pdf' target='_blank'>https://arxiv.org/pdf/2507.15085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peirong Zhang, Haowei Xu, Jiaxin Zhang, Guitao Xu, Xuhan Zheng, Zhenhua Yang, Junle Liu, Yuyi Zhang, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15085">Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text image is a unique and crucial information medium that integrates visual aesthetics and linguistic semantics in modern e-society. Due to their subtlety and complexity, the generation of text images represents a challenging and evolving frontier in the image generation field. The recent surge of specialized image generators (\emph{e.g.}, Flux-series) and unified generative models (\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a natural question: can they master the intricacies of text image generation and editing? Motivated by this, we assess current state-of-the-art generative models' capabilities in terms of text image generation and editing. We incorporate various typical optical character recognition (OCR) tasks into our evaluation and broaden the concept of text-based generation tasks into OCR generative tasks. We select 33 representative tasks and categorize them into five categories: document, handwritten text, scene text, artistic text, and complex \& layout-rich text. For comprehensive evaluation, we examine six models across both closed-source and open-source domains, using tailored, high-quality image inputs and prompts. Through this evaluation, we draw crucial observations and identify the weaknesses of current generative models for OCR tasks. We argue that photorealistic text image generation and editing should be internalized as foundational skills into general-domain generative models, rather than being delegated to specialized solutions, and we hope this empirical analysis can provide valuable insights for the community to achieve this goal. This evaluation is online and will be continuously updated at our GitHub repository.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2412.14692.pdf' target='_blank'>https://arxiv.org/pdf/2412.14692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Su, Zhineng Chen, Yongkun Du, Zhilong Ji, Kai Hu, Jinfeng Bai, Xieping Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14692">Explicit Relational Reasoning Network for Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connected component (CC) is a proper text shape representation that aligns with human reading intuition. However, CC-based text detection methods have recently faced a developmental bottleneck that their time-consuming post-processing is difficult to eliminate. To address this issue, we introduce an explicit relational reasoning network (ERRNet) to elegantly model the component relationships without post-processing. Concretely, we first represent each text instance as multiple ordered text components, and then treat these components as objects in sequential movement. In this way, scene text detection can be innovatively viewed as a tracking problem. From this perspective, we design an end-to-end tracking decoder to achieve a CC-based method dispensing with post-processing entirely. Additionally, we observe that there is an inconsistency between classification confidence and localization quality, so we propose a Polygon Monte-Carlo method to quickly and accurately evaluate the localization quality. Based on this, we introduce a position-supervised classification loss to guide the task-aligned learning of ERRNet. Experiments on challenging benchmarks demonstrate the effectiveness of our ERRNet. It consistently achieves state-of-the-art accuracy while holding highly competitive inference speed.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2408.14998.pdf' target='_blank'>https://arxiv.org/pdf/2408.14998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alloy Das, Sanket Biswas, Umapada Pal, Josep LladÃ³s, Saumik Bhattacharya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14998">FastTextSpotter: A High-Efficiency Transformer for Multilingual Scene Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of scene text in both structured and unstructured environments presents significant challenges in optical character recognition (OCR), necessitating more efficient and robust text spotting solutions. This paper presents FastTextSpotter, a framework that integrates a Swin Transformer visual backbone with a Transformer Encoder-Decoder architecture, enhanced by a novel, faster self-attention unit, SAC2, to improve processing speeds while maintaining accuracy. FastTextSpotter has been validated across multiple datasets, including ICDAR2015 for regular texts and CTW1500 and TotalText for arbitrary-shaped texts, benchmarking against current state-of-the-art models. Our results indicate that FastTextSpotter not only achieves superior accuracy in detecting and recognizing multilingual scene text (English and Vietnamese) but also improves model efficiency, thereby setting new benchmarks in the field. This study underscores the potential of advanced transformer architectures in improving the adaptability and speed of text spotting applications in diverse real-world settings. The dataset, code, and pre-trained models have been released in our Github.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2408.11523.pdf' target='_blank'>https://arxiv.org/pdf/2408.11523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizhong Wan, Bin Yin, Junjie Xie, Fei Jiang, Xiang Li, Wei Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11523">LARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Click-Through Rate (CTR) prediction is crucial for Recommendation System(RS), aiming to provide personalized recommendation services for users in many aspects such as food delivery, e-commerce and so on. However, traditional RS relies on collaborative signals, which lacks semantic understanding to real-time scenes. We also noticed that a major challenge in utilizing Large Language Models (LLMs) for practical recommendation purposes is their efficiency in dealing with long text input. To break through the problems above, we propose Large Language Model Aided Real-time Scene Recommendation(LARR), adopt LLMs for semantic understanding, utilizing real-time scene information in RS without requiring LLM to process the entire real-time scene text directly, thereby enhancing the efficiency of LLM-based CTR modeling. Specifically, recommendation domain-specific knowledge is injected into LLM and then RS employs an aggregation encoder to build real-time scene information from separate LLM's outputs. Firstly, a LLM is continual pretrained on corpus built from recommendation data with the aid of special tokens. Subsequently, the LLM is fine-tuned via contrastive learning on three kinds of sample construction strategies. Through this step, LLM is transformed into a text embedding model. Finally, LLM's separate outputs for different scene features are aggregated by an encoder, aligning to collaborative signals in RS, enhancing the performance of recommendation model.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2407.19507.pdf' target='_blank'>https://arxiv.org/pdf/2407.19507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjing Wu, Zhengyao Fang, Pengyuan Lyu, Chengquan Zhang, Fanglin Chen, Guangming Lu, Wenjie Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19507">WeCromCL: Weakly Supervised Cross-Modality Contrastive Learning for Transcription-only Supervised Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transcription-only Supervised Text Spotting aims to learn text spotters relying only on transcriptions but no text boundaries for supervision, thus eliminating expensive boundary annotation. The crux of this task lies in locating each transcription in scene text images without location annotations. In this work, we formulate this challenging problem as a Weakly Supervised Cross-modality Contrastive Learning problem, and design a simple yet effective model dubbed WeCromCL that is able to detect each transcription in a scene image in a weakly supervised manner. Unlike typical methods for cross-modality contrastive learning that focus on modeling the holistic semantic correlation between an entire image and a text description, our WeCromCL conducts atomistic contrastive learning to model the character-wise appearance consistency between a text transcription and its correlated region in a scene image to detect an anchor point for the transcription in a weakly supervised manner. The detected anchor points by WeCromCL are further used as pseudo location labels to guide the learning of text spotting. Extensive experiments on four challenging benchmarks demonstrate the superior performance of our model over other methods. Code will be released.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2405.07481.pdf' target='_blank'>https://arxiv.org/pdf/2405.07481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianci Bi, Xiaoyi Zhang, Zhizheng Zhang, Wenxuan Xie, Cuiling Lan, Yan Lu, Nanning Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07481">Text Grouping Adapter: Adapting Pre-trained Text Detector for Layout Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Significant progress has been made in scene text detection models since the rise of deep learning, but scene text layout analysis, which aims to group detected text instances as paragraphs, has not kept pace. Previous works either treated text detection and grouping using separate models, or train a model from scratch while using a unified one. All of them have not yet made full use of the already well-trained text detectors and easily obtainable detection datasets. In this paper, we present Text Grouping Adapter (TGA), a module that can enable the utilization of various pre-trained text detectors to learn layout analysis, allowing us to adopt a well-trained text detector right off the shelf or just fine-tune it efficiently. Designed to be compatible with various text detector architectures, TGA takes detected text regions and image features as universal inputs to assemble text instance features. To capture broader contextual information for layout analysis, we propose to predict text group masks from text instance features by one-to-many assignment. Our comprehensive experiments demonstrate that, even with frozen pre-trained models, incorporating our TGA into various pre-trained text detectors and text spotters can achieve superior layout analysis performance, simultaneously inheriting generalized text detection ability from pre-training. In the case of full parameter fine-tuning, we can further improve layout analysis performance.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2505.23119.pdf' target='_blank'>https://arxiv.org/pdf/2505.23119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keren Ye, Ignacio Garcia Dorado, Michalis Raptis, Mauricio Delbracio, Irene Zhu, Peyman Milanfar, Hossein Talebi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23119">TextSR: Diffusion Super-Resolution with Multilingual OCR Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent advancements in Image Super-Resolution (SR) using diffusion models have shown promise in improving overall image quality, their application to scene text images has revealed limitations. These models often struggle with accurate text region localization and fail to effectively model image and multilingual character-to-shape priors. This leads to inconsistencies, the generation of hallucinated textures, and a decrease in the perceived quality of the super-resolved text.
  To address these issues, we introduce TextSR, a multimodal diffusion model specifically designed for Multilingual Scene Text Image Super-Resolution. TextSR leverages a text detector to pinpoint text regions within an image and then employs Optical Character Recognition (OCR) to extract multilingual text from these areas. The extracted text characters are then transformed into visual shapes using a UTF-8 based text encoder and cross-attention. Recognizing that OCR may sometimes produce inaccurate results in real-world scenarios, we have developed two innovative methods to enhance the robustness of our model. By integrating text character priors with the low-resolution text images, our model effectively guides the super-resolution process, enhancing fine details within the text and improving overall legibility. The superior performance of our model on both the TextZoom and TextVQA datasets sets a new benchmark for STISR, underscoring the efficacy of our approach.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2510.21590.pdf' target='_blank'>https://arxiv.org/pdf/2510.21590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minxing Luo, Linlong Fan, Wang Qiushi, Ge Wu, Yiyan Luo, Yuhang Yu, Jinwei Chen, Yaxing Wang, Qingnan Fan, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21590">Restore Text First, Enhance Image Later: Two-Stage Scene Text Image Super-Resolution with Glyph Structure Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current generative super-resolution methods show strong performance on natural images but distort text, creating a fundamental trade-off between image quality and textual readability. To address this, we introduce \textbf{TIGER} (\textbf{T}ext-\textbf{I}mage \textbf{G}uided sup\textbf{E}r-\textbf{R}esolution), a novel two-stage framework that breaks this trade-off through a \textit{"text-first, image-later"} paradigm. \textbf{TIGER} explicitly decouples glyph restoration from image enhancement: it first reconstructs precise text structures and then uses them to guide subsequent full-image super-resolution. This glyph-to-image guidance ensures both high fidelity and visual consistency. To support comprehensive training and evaluation, we also contribute the \textbf{UltraZoom-ST} (UltraZoom-Scene Text), the first scene text dataset with extreme zoom (\textbf{$\times$14.29}). Extensive experiments show that \textbf{TIGER} achieves \textbf{state-of-the-art} performance, enhancing readability while preserving overall image quality.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2409.16827.pdf' target='_blank'>https://arxiv.org/pdf/2409.16827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Han, Junyu Gao, Chuang Yang, Yuan Yuan, Qi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16827">Focus Entirety and Perceive Environment for Arbitrary-Shaped Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the diversity of scene text in aspects such as font, color, shape, and size, accurately and efficiently detecting text is still a formidable challenge. Among the various detection approaches, segmentation-based approaches have emerged as prominent contenders owing to their flexible pixel-level predictions. However, these methods typically model text instances in a bottom-up manner, which is highly susceptible to noise. In addition, the prediction of pixels is isolated without introducing pixel-feature interaction, which also influences the detection performance. To alleviate these problems, we propose a multi-information level arbitrary-shaped text detector consisting of a focus entirety module (FEM) and a perceive environment module (PEM). The former extracts instance-level features and adopts a top-down scheme to model texts to reduce the influence of noises. Specifically, it assigns consistent entirety information to pixels within the same instance to improve their cohesion. In addition, it emphasizes the scale information, enabling the model to distinguish varying scale texts effectively. The latter extracts region-level information and encourages the model to focus on the distribution of positive samples in the vicinity of a pixel, which perceives environment information. It treats the kernel pixels as positive samples and helps the model differentiate text and kernel features. Extensive experiments demonstrate the FEM's ability to efficiently support the model in handling different scale texts and confirm the PEM can assist in perceiving pixels more accurately by focusing on pixel vicinities. Comparisons show the proposed model outperforms existing state-of-the-art approaches on four public datasets.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2409.16820.pdf' target='_blank'>https://arxiv.org/pdf/2409.16820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Han, Junyu Gao, Chuang Yang, Yuan Yuan, Qi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16820">Spotlight Text Detector: Spotlight on Candidate Regions Like a Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The irregular contour representation is one of the tough challenges in scene text detection. Although segmentation-based methods have achieved significant progress with the help of flexible pixel prediction, the overlap of geographically close texts hinders detecting them separately. To alleviate this problem, some shrink-based methods predict text kernels and expand them to restructure texts. However, the text kernel is an artificial object with incomplete semantic features that are prone to incorrect or missing detection. In addition, different from the general objects, the geometry features (aspect ratio, scale, and shape) of scene texts vary significantly, which makes it difficult to detect them accurately. To consider the above problems, we propose an effective spotlight text detector (STD), which consists of a spotlight calibration module (SCM) and a multivariate information extraction module (MIEM). The former concentrates efforts on the candidate kernel, like a camera focus on the target. It obtains candidate features through a mapping filter and calibrates them precisely to eliminate some false positive samples. The latter designs different shape schemes to explore multiple geometric features for scene texts. It helps extract various spatial relationships to improve the model's ability to recognize kernel regions. Ablation studies prove the effectiveness of the designed SCM and MIEM. Extensive experiments verify that our STD is superior to existing state-of-the-art methods on various datasets, including ICDAR2015, CTW1500, MSRA-TD500, and Total-Text.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2402.17134.pdf' target='_blank'>https://arxiv.org/pdf/2402.17134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nguyen Nguyen, Yapeng Tian, Chenliang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17134">Efficiently Leveraging Linguistic Priors for Scene Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Incorporating linguistic knowledge can improve scene text recognition, but it is questionable whether the same holds for scene text spotting, which typically involves text detection and recognition. This paper proposes a method that leverages linguistic knowledge from a large text corpus to replace the traditional one-hot encoding used in auto-regressive scene text spotting and recognition models. This allows the model to capture the relationship between characters in the same word. Additionally, we introduce a technique to generate text distributions that align well with scene text datasets, removing the need for in-domain fine-tuning. As a result, the newly created text distributions are more informative than pure one-hot encoding, leading to improved spotting and recognition performance. Our method is simple and efficient, and it can easily be integrated into existing auto-regressive-based approaches. Experimental results show that our method not only improves recognition accuracy but also enables more accurate localization of words. It significantly improves both state-of-the-art scene text spotting and recognition pipelines, achieving state-of-the-art results on several benchmarks.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2412.12502.pdf' target='_blank'>https://arxiv.org/pdf/2412.12502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Zhang, Gangyan Zeng, Huawen Shen, Daiqing Wu, Yu Zhou, Can Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12502">Track the Answer: Extending TextVQA from Image to Video with Spatio-Temporal Clues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video text-based visual question answering (Video TextVQA) is a practical task that aims to answer questions by jointly reasoning textual and visual information in a given video. Inspired by the development of TextVQA in image domain, existing Video TextVQA approaches leverage a language model (e.g. T5) to process text-rich multiple frames and generate answers auto-regressively. Nevertheless, the spatio-temporal relationships among visual entities (including scene text and objects) will be disrupted and models are susceptible to interference from unrelated information, resulting in irrational reasoning and inaccurate answering. To tackle these challenges, we propose the TEA (stands for ``\textbf{T}rack th\textbf{E} \textbf{A}nswer'') method that better extends the generative TextVQA framework from image to video. TEA recovers the spatio-temporal relationships in a complementary way and incorporates OCR-aware clues to enhance the quality of reasoning questions. Extensive experiments on several public Video TextVQA datasets validate the effectiveness and generalization of our framework. TEA outperforms existing TextVQA methods, video-language pretraining methods and video large language models by great margins.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2408.02036.pdf' target='_blank'>https://arxiv.org/pdf/2408.02036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujin Ren, Jiaxin Zhang, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02036">LEGO: Self-Supervised Representation Learning for Scene Text Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, significant progress has been made in scene text recognition by data-driven methods. However, due to the scarcity of annotated real-world data, the training of these methods predominantly relies on synthetic data. The distribution gap between synthetic and real data constrains the further performance improvement of these methods in real-world applications. To tackle this problem, a highly promising approach is to utilize massive amounts of unlabeled real data for self-supervised training, which has been widely proven effective in many NLP and CV tasks. Nevertheless, generic self-supervised methods are unsuitable for scene text images due to their sequential nature. To address this issue, we propose a Local Explicit and Global Order-aware self-supervised representation learning method (LEGO) that accounts for the characteristics of scene text images. Inspired by the human cognitive process of learning words, which involves spelling, reading, and writing, we propose three novel pre-text tasks for LEGO to model sequential, semantic, and structural features, respectively. The entire pre-training process is optimized by using a consistent Text Knowledge Codebook. Extensive experiments validate that LEGO outperforms previous scene text self-supervised methods. The recognizer incorporated with our pre-trained model achieves superior or comparable performance compared to state-of-the-art scene text recognition methods on six benchmarks. Furthermore, we demonstrate that LEGO can achieve superior performance in other text-related tasks.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2401.03637.pdf' target='_blank'>https://arxiv.org/pdf/2401.03637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shi-Xue Zhang, Chun Yang, Xiaobin Zhu, Hongyang Zhou, Hongfa Wang, Xu-Cheng Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03637">Inverse-like Antagonistic Scene Text Spotting via Reading-Order Estimation and Dynamic Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text spotting is a challenging task, especially for inverse-like scene text, which has complex layouts, e.g., mirrored, symmetrical, or retro-flexed. In this paper, we propose a unified end-to-end trainable inverse-like antagonistic text spotting framework dubbed IATS, which can effectively spot inverse-like scene texts without sacrificing general ones. Specifically, we propose an innovative reading-order estimation module (REM) that extracts reading-order information from the initial text boundary generated by an initial boundary module (IBM). To optimize and train REM, we propose a joint reading-order estimation loss consisting of a classification loss, an orthogonality loss, and a distribution loss. With the help of IBM, we can divide the initial text boundary into two symmetric control points and iteratively refine the new text boundary using a lightweight boundary refinement module (BRM) for adapting to various shapes and scales. To alleviate the incompatibility between text detection and recognition, we propose a dynamic sampling module (DSM) with a thin-plate spline that can dynamically sample appropriate features for recognition in the detected text region. Without extra supervision, the DSM can proactively learn to sample appropriate features for text recognition through the gradient returned by the recognition module. Extensive experiments on both challenging scene text and inverse-like scene text datasets demonstrate that our method achieves superior performance both on irregular and inverse-like text spotting.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2510.03200.pdf' target='_blank'>https://arxiv.org/pdf/2510.03200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Collorone, Matteo Gioia, Massimiliano Pappa, Paolo Leoni, Giovanni Ficarra, Or Litany, Indro Spinelli, Fabio Galasso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03200">MonSTeR: a Unified Model for Motion, Scene, Text Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intention drives human movement in complex environments, but such movement can only happen if the surrounding context supports it. Despite the intuitive nature of this mechanism, existing research has not yet provided tools to evaluate the alignment between skeletal movement (motion), intention (text), and the surrounding context (scene). In this work, we introduce MonSTeR, the first MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of higher-order relations, MonSTeR constructs a unified latent space by leveraging unimodal and cross-modal representations. This allows MonSTeR to capture the intricate dependencies between modalities, enabling flexible but robust retrieval across various tasks. Our results show that MonSTeR outperforms trimodal models that rely solely on unimodal representations. Furthermore, we validate the alignment of our retrieval scores with human preferences through a dedicated user study. We demonstrate the versatility of MonSTeR's latent space on zero-shot in-Scene Object Placement and Motion Captioning. Code and pre-trained models are available at github.com/colloroneluca/MonSTeR.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2505.04915.pdf' target='_blank'>https://arxiv.org/pdf/2505.04915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Wang, Ting Liu, Xiaochao Qu, Chengjing Wu, Luoqi Liu, Xiaolin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04915">GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text editing, a subfield of image editing, requires modifying texts in images while preserving style consistency and visual coherence with the surrounding environment. While diffusion-based methods have shown promise in text generation, they still struggle to produce high-quality results. These methods often generate distorted or unrecognizable characters, particularly when dealing with complex characters like Chinese. In such systems, characters are composed of intricate stroke patterns and spatial relationships that must be precisely maintained. We present GlyphMastero, a specialized glyph encoder designed to guide the latent diffusion model for generating texts with stroke-level precision. Our key insight is that existing methods, despite using pretrained OCR models for feature extraction, fail to capture the hierarchical nature of text structures - from individual strokes to stroke-level interactions to overall character-level structure. To address this, our glyph encoder explicitly models and captures the cross-level interactions between local-level individual characters and global-level text lines through our novel glyph attention module. Meanwhile, our model implements a feature pyramid network to fuse the multi-scale OCR backbone features at the global-level. Through these cross-level and multi-scale fusions, we obtain more detailed glyph-aware guidance, enabling precise control over the scene text generation process. Our method achieves an 18.02\% improvement in sentence accuracy over the state-of-the-art multi-lingual scene text editing baseline, while simultaneously reducing the text-region FrÃ©chet inception distance by 53.28\%.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2412.19917.pdf' target='_blank'>https://arxiv.org/pdf/2412.19917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enze Xie, Jiaho Lyu, Daiqing Wu, Huawen Shen, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19917">Char-SAM: Turning Segment Anything Model into Scene Text Segmentation Annotator with Character-level Visual Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent emergence of the Segment Anything Model (SAM) enables various domain-specific segmentation tasks to be tackled cost-effectively by using bounding boxes as prompts. However, in scene text segmentation, SAM can not achieve desirable performance. The word-level bounding box as prompts is too coarse for characters, while the character-level bounding box as prompts suffers from over-segmentation and under-segmentation issues. In this paper, we propose an automatic annotation pipeline named Char-SAM, that turns SAM into a low-cost segmentation annotator with a Character-level visual prompt. Specifically, leveraging some existing text detection datasets with word-level bounding box annotations, we first generate finer-grained character-level bounding box prompts using the Character Bounding-box Refinement CBR module. Next, we employ glyph information corresponding to text character categories as a new prompt in the Character Glyph Refinement (CGR) module to guide SAM in producing more accurate segmentation masks, addressing issues of over-segmentation and under-segmentation. These modules fully utilize the bbox-to-mask capability of SAM to generate high-quality text segmentation annotations automatically. Extensive experiments on TextSeg validate the effectiveness of Char-SAM. Its training-free nature also enables the generation of high-quality scene text segmentation datasets from real-world datasets like COCO-Text and MLT17.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2406.01062.pdf' target='_blank'>https://arxiv.org/pdf/2406.01062.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qilong Zhangli, Jindong Jiang, Di Liu, Licheng Yu, Xiaoliang Dai, Ankit Ramchandani, Guan Pang, Dimitris N. Metaxas, Praveen Krishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01062">Layout Agnostic Scene Text Image Synthesis with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While diffusion models have significantly advanced the quality of image generation their capability to accurately and coherently render text within these images remains a substantial challenge. Conventional diffusion-based methods for scene text generation are typically limited by their reliance on an intermediate layout output. This dependency often results in a constrained diversity of text styles and fonts an inherent limitation stemming from the deterministic nature of the layout generation phase. To address these challenges this paper introduces SceneTextGen a novel diffusion-based model specifically designed to circumvent the need for a predefined layout stage. By doing so SceneTextGen facilitates a more natural and varied representation of text. The novelty of SceneTextGen lies in its integration of three key components: a character-level encoder for capturing detailed typographic properties coupled with a character-level instance segmentation model and a word-level spotting model to address the issues of unwanted text generation and minor character inaccuracies. We validate the performance of our method by demonstrating improved character recognition rates on generated images across different public visual text datasets in comparison to both standard diffusion based methods and text specific methods.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2403.09622.pdf' target='_blank'>https://arxiv.org/pdf/2403.09622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Liu, Weicong Liang, Zhanhao Liang, Chong Luo, Ji Li, Gao Huang, Yuhui Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09622">Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual text rendering poses a fundamental challenge for contemporary text-to-image generation models, with the core problem lying in text encoder deficiencies. To achieve accurate text rendering, we identify two crucial requirements for text encoders: character awareness and alignment with glyphs. Our solution involves crafting a series of customized text encoder, Glyph-ByT5, by fine-tuning the character-aware ByT5 encoder using a meticulously curated paired glyph-text dataset. We present an effective method for integrating Glyph-ByT5 with SDXL, resulting in the creation of the Glyph-SDXL model for design image generation. This significantly enhances text rendering accuracy, improving it from less than $20\%$ to nearly $90\%$ on our design image benchmark. Noteworthy is Glyph-SDXL's newfound ability for text paragraph rendering, achieving high spelling accuracy for tens to hundreds of characters with automated multi-line layouts. Finally, through fine-tuning Glyph-SDXL with a small set of high-quality, photorealistic images featuring visual text, we showcase a substantial improvement in scene text rendering capabilities in open-domain real images. These compelling outcomes aim to encourage further exploration in designing customized text encoders for diverse and challenging tasks.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2510.01651.pdf' target='_blank'>https://arxiv.org/pdf/2510.01651.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rixin Zhou, Peiqiang Qiu, Qian Zhang, Chuntao Li, Xi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01651">LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial stage of early Chinese writing and provide indispensable evidence for archaeological and historical studies. However, automatic BI recognition remains difficult due to severe visual degradation, multi-domain variability across photographs, rubbings, and tracings, and an extremely long-tailed character distribution. To address these challenges, we curate a large-scale BI dataset comprising 22454 full-page images and 198598 annotated characters spanning 6658 unique categories, enabling robust cross-domain evaluation. Building on this resource, we develop a two-stage detection-recognition pipeline that first localizes inscriptions and then transcribes individual characters. To handle heterogeneous domains and rare classes, we equip the pipeline with LadderMoE, which augments a pretrained CLIP encoder with ladder-style MoE adapters, enabling dynamic expert specialization and stronger robustness. Comprehensive experiments on single-character and full-page recognition tasks demonstrate that our method substantially outperforms state-of-the-art scene text recognition baselines, achieving superior accuracy across head, mid, and tail categories as well as all acquisition modalities. These results establish a strong foundation for bronze inscription recognition and downstream archaeological analysis.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2509.12543.pdf' target='_blank'>https://arxiv.org/pdf/2509.12543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harshit Rajgarhia, Shivali Dalmia, Mengyang Zhao, Mukherji Abhishek, Kiran Ganesh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12543">Human + AI for Accelerating Ad Localization Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adapting advertisements for multilingual audiences requires more than simple text translation; it demands preservation of visual consistency, spatial alignment, and stylistic integrity across diverse languages and formats. We introduce a structured framework that combines automated components with human oversight to address the complexities of advertisement localization. To the best of our knowledge, this is the first work to integrate scene text detection, inpainting, machine translation (MT), and text reimposition specifically for accelerating ad localization evaluation workflows. Qualitative results across six locales demonstrate that our approach produces semantically accurate and visually coherent localized advertisements, suitable for deployment in real-world workflows.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2506.21276.pdf' target='_blank'>https://arxiv.org/pdf/2506.21276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenda Shi, Yiren Song, Zihan Rao, Dengming Zhang, Jiaming Liu, Xingxing Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21276">WordCon: Word-level Typography Control in Scene Text Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving precise word-level typography control within generated images remains a persistent challenge. To address it, we newly construct a word-level controlled scene text dataset and introduce the Text-Image Alignment (TIA) framework. This framework leverages cross-modal correspondence between text and local image regions provided by grounding models to enhance the Text-to-Image (T2I) model training. Furthermore, we propose WordCon, a hybrid parameter-efficient fine-tuning (PEFT) method. WordCon reparameterizes selective key parameters, improving both efficiency and portability. This allows seamless integration into diverse pipelines, including artistic text rendering, text editing, and image-conditioned text rendering. To further enhance controllability, the masked loss at the latent level is applied to guide the model to concentrate on learning the text region in the image, and the joint-attention loss provides feature-level supervision to promote disentanglement between different words. Both qualitative and quantitative results demonstrate the superiority of our method to the state of the art. The datasets and source code will be available for academic use.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2504.11164.pdf' target='_blank'>https://arxiv.org/pdf/2504.11164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenming Li, Chengxu Liu, Yuanting Fan, Xiao Jin, Xingsong Hou, Xueming Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11164">TSAL: Few-shot Text Segmentation Based on Attribute Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently supervised learning rapidly develops in scene text segmentation. However, the lack of high-quality datasets and the high cost of pixel annotation greatly limit the development of them. Considering the well-performed few-shot learning methods for downstream tasks, we investigate the application of the few-shot learning method to scene text segmentation. We propose TSAL, which leverages CLIP's prior knowledge to learn text attributes for segmentation. To fully utilize the semantic and texture information in the image, a visual-guided branch is proposed to separately extract text and background features. To reduce data dependency and improve text detection accuracy, the adaptive prompt-guided branch employs effective adaptive prompt templates to capture various text attributes. To enable adaptive prompts capture distinctive text features and complex background distribution, we propose Adaptive Feature Alignment module(AFA). By aligning learnable tokens of different attributes with visual features and prompt prototypes, AFA enables adaptive prompts to capture both general and distinctive attribute information. TSAL can capture the unique attributes of text and achieve precise segmentation using only few images. Experiments demonstrate that our method achieves SOTA performance on multiple text segmentation datasets under few-shot settings and show great potential in text-related domains.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2403.10047.pdf' target='_blank'>https://arxiv.org/pdf/2403.10047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Lyu, Jin Wei, Gangyan Zeng, Zeng Li, Enze Xie, Wei Wang, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10047">TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing scene text spotters are designed to locate and transcribe texts from images. However, it is challenging for a spotter to achieve precise detection and recognition of scene texts simultaneously. Inspired by the glimpse-focus spotting pipeline of human beings and impressive performances of Pre-trained Language Models (PLMs) on visual tasks, we ask: 1) "Can machines spot texts without precise detection just like human beings?", and if yes, 2) "Is text block another alternative for scene text spotting other than word or character?" To this end, our proposed scene text spotter leverages advanced PLMs to enhance performance without fine-grained detection. Specifically, we first use a simple detector for block-level text detection to obtain rough positional information. Then, we finetune a PLM using a large-scale OCR dataset to achieve accurate recognition. Benefiting from the comprehensive language knowledge gained during the pre-training phase, the PLM-based recognition module effectively handles complex scenarios, including multi-line, reversed, occluded, and incomplete-detection texts. Taking advantage of the fine-tuned language model on scene recognition benchmarks and the paradigm of text block detection, extensive experiments demonstrate the superior performance of our scene text spotter across multiple public benchmarks. Additionally, we attempt to spot texts directly from an entire scene image to demonstrate the potential of PLMs, even Large Language Models (LLMs).
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2403.07286.pdf' target='_blank'>https://arxiv.org/pdf/2403.07286.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsin-Ju Lin, Tsu-Chun Chung, Ching-Chun Hsiao, Pin-Yu Chen, Wei-Chen Chiu, Ching-Chun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07286">MENTOR: Multilingual tExt detectioN TOward leaRning by analogy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text detection is frequently used in vision-based mobile robots when they need to interpret texts in their surroundings to perform a given task. For instance, delivery robots in multilingual cities need to be capable of doing multilingual text detection so that the robots can read traffic signs and road markings. Moreover, the target languages change from region to region, implying the need of efficiently re-training the models to recognize the novel/new languages. However, collecting and labeling training data for novel languages are cumbersome, and the efforts to re-train an existing/trained text detector are considerable. Even worse, such a routine would repeat whenever a novel language appears. This motivates us to propose a new problem setting for tackling the aforementioned challenges in a more efficient way: "We ask for a generalizable multilingual text detection framework to detect and identify both seen and unseen language regions inside scene images without the requirement of collecting supervised training data for unseen languages as well as model re-training". To this end, we propose "MENTOR", the first work to realize a learning strategy between zero-shot learning and few-shot learning for multilingual scene text detection.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2503.18883.pdf' target='_blank'>https://arxiv.org/pdf/2503.18883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Savas Ozkan, Andrea Maracani, Hyowon Kim, Sijun Cho, Eunchung Noh, Jeongwon Min, Jung Min Cho, Mete Ozay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18883">Efficient and Accurate Scene Text Recognition with Cascaded-Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, vision transformers with text decoder have demonstrated remarkable performance on Scene Text Recognition (STR) due to their ability to capture long-range dependencies and contextual relationships with high learning capacity. However, the computational and memory demands of these models are significant, limiting their deployment in resource-constrained applications. To address this challenge, we propose an efficient and accurate STR system. Specifically, we focus on improving the efficiency of encoder models by introducing a cascaded-transformers structure. This structure progressively reduces the vision token size during the encoding step, effectively eliminating redundant tokens and reducing computational cost. Our experimental results confirm that our STR system achieves comparable performance to state-of-the-art baselines while substantially decreasing computational requirements. In particular, for large-models, the accuracy remains same, 92.77 to 92.68, while computational complexity is almost halved with our structure.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2503.16184.pdf' target='_blank'>https://arxiv.org/pdf/2503.16184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Maracani, Savas Ozkan, Sijun Cho, Hyowon Kim, Eunchung Noh, Jeongwon Min, Cho Jung Min, Dookun Park, Mete Ozay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16184">Accurate Scene Text Recognition with Efficient Model Scaling and Cloze Self-Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scaling architectures have been proven effective for improving Scene Text Recognition (STR), but the individual contribution of vision encoder and text decoder scaling remain under-explored. In this work, we present an in-depth empirical analysis and demonstrate that, contrary to previous observations, scaling the decoder yields significant performance gains, always exceeding those achieved by encoder scaling alone. We also identify label noise as a key challenge in STR, particularly in real-world data, which can limit the effectiveness of STR models. To address this, we propose Cloze Self-Distillation (CSD), a method that mitigates label noise by distilling a student model from context-aware soft predictions and pseudolabels generated by a teacher model. Additionally, we enhance the decoder architecture by introducing differential cross-attention for STR. Our methodology achieves state-of-the-art performance on 10 out of 11 benchmarks using only real data, while significantly reducing the parameter size and computational costs.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2408.10623.pdf' target='_blank'>https://arxiv.org/pdf/2408.10623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Wang, Xiaochao Qu, Ting Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10623">TextMastero: Mastering High-Quality Scene Text Editing in Diverse Languages and Styles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text editing aims to modify texts on images while maintaining the style of newly generated text similar to the original. Given an image, a target area, and target text, the task produces an output image with the target text in the selected area, replacing the original. This task has been studied extensively, with initial success using Generative Adversarial Networks (GANs) to balance text fidelity and style similarity. However, GAN-based methods struggled with complex backgrounds or text styles. Recent works leverage diffusion models, showing improved results, yet still face challenges, especially with non-Latin languages like CJK characters (Chinese, Japanese, Korean) that have complex glyphs, often producing inaccurate or unrecognizable characters. To address these issues, we present \emph{TextMastero} - a carefully designed multilingual scene text editing architecture based on latent diffusion models (LDMs). TextMastero introduces two key modules: a glyph conditioning module for fine-grained content control in generating accurate texts, and a latent guidance module for providing comprehensive style information to ensure similarity before and after editing. Both qualitative and quantitative experiments demonstrate that our method surpasses all known existing works in text fidelity and style similarity.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2405.04377.pdf' target='_blank'>https://arxiv.org/pdf/2405.04377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boqiang Zhang, Hongtao Xie, Zuan Gao, Yuxin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04377">Choose What You Need: Disentangled Representation Learning for Scene Text Recognition, Removal and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text images contain not only style information (font, background) but also content information (character, texture). Different scene text tasks need different information, but previous representation learning methods use tightly coupled features for all tasks, resulting in sub-optimal performance. We propose a Disentangled Representation Learning framework (DARLING) aimed at disentangling these two types of features for improved adaptability in better addressing various downstream tasks (choose what you really need). Specifically, we synthesize a dataset of image pairs with identical style but different content. Based on the dataset, we decouple the two types of features by the supervision design. Clearly, we directly split the visual representation into style and content features, the content features are supervised by a text recognition loss, while an alignment loss aligns the style features in the image pairs. Then, style features are employed in reconstructing the counterpart image via an image decoder with a prompt that indicates the counterpart's content. Such an operation effectively decouples the features based on their distinctive properties. To the best of our knowledge, this is the first time in the field of scene text that disentangles the inherent properties of the text images. Our method achieves state-of-the-art performance in Scene Text Recognition, Removal, and Editing.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2402.15806.pdf' target='_blank'>https://arxiv.org/pdf/2402.15806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingkun Yang, Biao Yang, Minghui Liao, Yingying Zhu, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15806">Sequential Visual and Semantic Consistency for Semi-supervised Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition (STR) is a challenging task that requires large-scale annotated data for training. However, collecting and labeling real text images is expensive and time-consuming, which limits the availability of real data. Therefore, most existing STR methods resort to synthetic data, which may introduce domain discrepancy and degrade the performance of STR models. To alleviate this problem, recent semi-supervised STR methods exploit unlabeled real data by enforcing character-level consistency regularization between weakly and strongly augmented views of the same image. However, these methods neglect word-level consistency, which is crucial for sequence recognition tasks. This paper proposes a novel semi-supervised learning method for STR that incorporates word-level consistency regularization from both visual and semantic aspects. Specifically, we devise a shortest path alignment module to align the sequential visual features of different views and minimize their distance. Moreover, we adopt a reinforcement learning framework to optimize the semantic similarity of the predicted strings in the embedding space. We conduct extensive experiments on several standard and challenging STR benchmarks and demonstrate the superiority of our proposed method over existing semi-supervised STR methods.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2506.21002.pdf' target='_blank'>https://arxiv.org/pdf/2506.21002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takumi Yoshimatsu, Shumpei Takezaki, Seiichi Uchida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21002">Inverse Scene Text Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text removal (STR) aims to erase textual elements from images. It was originally intended for removing privacy-sensitiveor undesired texts from natural scene images, but is now also appliedto typographic images. STR typically detects text regions and theninpaints them. Although STR has advanced through neural networksand synthetic data, misuse risks have increased. This paper investi-gates Inverse STR (ISTR), which analyzes STR-processed images andfocuses on binary classification (detecting whether an image has un-dergone STR) and localizing removed text regions. We demonstrate inexperiments that these tasks are achievable with high accuracies, en-abling detection of potential misuse and improving STR. We also at-tempt to recover the removed text content by training a text recognizerto understand its difficulty.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2506.04999.pdf' target='_blank'>https://arxiv.org/pdf/2506.04999.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gengluo Li, Huawen Shen, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04999">Beyond Cropped Regions: New Benchmark and Corresponding Baseline for Chinese Scene Text Retrieval in Diverse Layouts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Chinese scene text retrieval is a practical task that aims to search for images containing visual instances of a Chinese query text. This task is extremely challenging because Chinese text often features complex and diverse layouts in real-world scenes. Current efforts tend to inherit the solution for English scene text retrieval, failing to achieve satisfactory performance. In this paper, we establish a Diversified Layout benchmark for Chinese Street View Text Retrieval (DL-CSVTR), which is specifically designed to evaluate retrieval performance across various text layouts, including vertical, cross-line, and partial alignments. To address the limitations in existing methods, we propose Chinese Scene Text Retrieval CLIP (CSTR-CLIP), a novel model that integrates global visual information with multi-granularity alignment training. CSTR-CLIP applies a two-stage training process to overcome previous limitations, such as the exclusion of visual features outside the text region and reliance on single-granularity alignment, thereby enabling the model to effectively handle diverse text layouts. Experiments on existing benchmark show that CSTR-CLIP outperforms the previous state-of-the-art model by 18.82% accuracy and also provides faster inference speed. Further analysis on DL-CSVTR confirms the superior performance of CSTR-CLIP in handling various text layouts. The dataset and code will be publicly available to facilitate research in Chinese scene text retrieval.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2401.10041.pdf' target='_blank'>https://arxiv.org/pdf/2401.10041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinzhi Zheng, Ruyi Ji, Libo Zhang, Yanjun Wu, Chen Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10041">CMFN: Cross-Modal Fusion Network for Irregular Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition, as a cross-modal task involving vision and text, is an important research topic in computer vision. Most existing methods use language models to extract semantic information for optimizing visual recognition. However, the guidance of visual cues is ignored in the process of semantic mining, which limits the performance of the algorithm in recognizing irregular scene text. To tackle this issue, we propose a novel cross-modal fusion network (CMFN) for irregular scene text recognition, which incorporates visual cues into the semantic mining process. Specifically, CMFN consists of a position self-enhanced encoder, a visual recognition branch and an iterative semantic recognition branch. The position self-enhanced encoder provides character sequence position encoding for both the visual recognition branch and the iterative semantic recognition branch. The visual recognition branch carries out visual recognition based on the visual features extracted by CNN and the position encoding information provided by the position self-enhanced encoder. The iterative semantic recognition branch, which consists of a language recognition module and a cross-modal fusion gate, simulates the way that human recognizes scene text and integrates cross-modal visual cues for text recognition. The experiments demonstrate that the proposed CMFN algorithm achieves comparable performance to state-of-the-art algorithms, indicating its effectiveness.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2401.10017.pdf' target='_blank'>https://arxiv.org/pdf/2401.10017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinzhi Zheng, Libo Zhang, Yanjun Wu, Chen Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10017">Text Region Multiple Information Perception Network for Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmentation-based scene text detection algorithms can handle arbitrary shape scene texts and have strong robustness and adaptability, so it has attracted wide attention. Existing segmentation-based scene text detection algorithms usually only segment the pixels in the center region of the text, while ignoring other information of the text region, such as edge information, distance information, etc., thus limiting the detection accuracy of the algorithm for scene text. This paper proposes a plug-and-play module called the Region Multiple Information Perception Module (RMIPM) to enhance the detection performance of segmentation-based algorithms. Specifically, we design an improved module that can perceive various types of information about scene text regions, such as text foreground classification maps, distance maps, direction maps, etc. Experiments on MSRA-TD500 and TotalText datasets show that our method achieves comparable performance with current state-of-the-art algorithms.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2401.09997.pdf' target='_blank'>https://arxiv.org/pdf/2401.09997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinzhi Zheng, Libo Zhang, Yanjun Wu, Chen Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09997">BPDO:Boundary Points Dynamic Optimization for Arbitrary Shape Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Arbitrary shape scene text detection is of great importance in scene understanding tasks. Due to the complexity and diversity of text in natural scenes, existing scene text algorithms have limited accuracy for detecting arbitrary shape text. In this paper, we propose a novel arbitrary shape scene text detector through boundary points dynamic optimization(BPDO). The proposed model is designed with a text aware module (TAM) and a boundary point dynamic optimization module (DOM). Specifically, the model designs a text aware module based on segmentation to obtain boundary points describing the central region of the text by extracting a priori information about the text region. Then, based on the idea of deformable attention, it proposes a dynamic optimization model for boundary points, which gradually optimizes the exact position of the boundary points based on the information of the adjacent region of each boundary point. Experiments on CTW-1500, Total-Text, and MSRA-TD500 datasets show that the model proposed in this paper achieves a performance that is better than or comparable to the state-of-the-art algorithm, proving the effectiveness of the model.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2409.11656.pdf' target='_blank'>https://arxiv.org/pdf/2409.11656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Humen Zhong, Zhibo Yang, Zhaohai Li, Peng Wang, Jun Tang, Wenqing Cheng, Cong Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11656">VL-Reader: Vision and Language Reconstructor is an Effective Scene Text Recognizer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text recognition is an inherent integration of vision and language, encompassing the visual texture in stroke patterns and the semantic context among the character sequences. Towards advanced text recognition, there are three key challenges: (1) an encoder capable of representing the visual and semantic distributions; (2) a decoder that ensures the alignment between vision and semantics; and (3) consistency in the framework during pre-training, if it exists, and fine-tuning. Inspired by masked autoencoding, a successful pre-training strategy in both vision and language, we propose an innovative scene text recognition approach, named VL-Reader. The novelty of the VL-Reader lies in the pervasive interplay between vision and language throughout the entire process. Concretely, we first introduce a Masked Visual-Linguistic Reconstruction (MVLR) objective, which aims at simultaneously modeling visual and linguistic information. Then, we design a Masked Visual-Linguistic Decoder (MVLD) to further leverage masked vision-language context and achieve bi-modal feature interaction. The architecture of VL-Reader maintains consistency from pre-training to fine-tuning. In the pre-training stage, VL-Reader reconstructs both masked visual and text tokens, while in the fine-tuning stage, the network degrades to reconstruct all characters from an image without any masked regions. VL-reader achieves an average accuracy of 97.1% on six typical datasets, surpassing the SOTA by 1.1%. The improvement was even more significant on challenging datasets. The results demonstrate that vision and language reconstructor can serve as an effective scene text recognizer.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2407.16204.pdf' target='_blank'>https://arxiv.org/pdf/2407.16204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Zhao, Qing Guo, Xiaoguang Li, Song Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16204">CLII: Visual-Text Inpainting via Cross-Modal Predictive Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image inpainting aims to fill missing pixels in damaged images and has achieved significant progress with cut-edging learning techniques. Nevertheless, state-of-the-art inpainting methods are mainly designed for nature images and cannot correctly recover text within scene text images, and training existing models on the scene text images cannot fix the issues. In this work, we identify the visual-text inpainting task to achieve high-quality scene text image restoration and text completion: Given a scene text image with unknown missing regions and the corresponding text with unknown missing characters, we aim to complete the missing information in both images and text by leveraging their complementary information. Intuitively, the input text, even if damaged, contains language priors of the contents within the images and can guide the image inpainting. Meanwhile, the scene text image includes the appearance cues of the characters that could benefit text recovery. To this end, we design the cross-modal predictive interaction (CLII) model containing two branches, i.e., ImgBranch and TxtBranch, for scene text inpainting and text completion, respectively while leveraging their complementary effectively. Moreover, we propose to embed our model into the SOTA scene text spotting method and significantly enhance its robustness against missing pixels, which demonstrates the practicality of the newly developed task. To validate the effectiveness of our method, we construct three real datasets based on existing text-related datasets, containing 1838 images and covering three scenarios with curved, incidental, and styled texts, and conduct extensive experiments to show that our method outperforms baselines significantly.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2511.23071.pdf' target='_blank'>https://arxiv.org/pdf/2511.23071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anik De, Abhirama Subramanyam Penamakuri, Rajeev Yadav, Aditya Rathore, Harshiv Shah, Devesh Sharma, Sagar Agarwal, Pravin Kumar, Anand Mishra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.23071">Bharat Scene Text: A Novel Comprehensive Dataset and Benchmark for Indian Language Scene Text Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reading scene text, that is, text appearing in images, has numerous application areas, including assistive technology, search, and e-commerce. Although scene text recognition in English has advanced significantly and is often considered nearly a solved problem, Indian language scene text recognition remains an open challenge. This is due to script diversity, non-standard fonts, and varying writing styles, and, more importantly, the lack of high-quality datasets and open-source models. To address these gaps, we introduce the Bharat Scene Text Dataset (BSTD) - a large-scale and comprehensive benchmark for studying Indian Language Scene Text Recognition. It comprises more than 100K words that span 11 Indian languages and English, sourced from over 6,500 scene images captured across various linguistic regions of India. The dataset is meticulously annotated and supports multiple scene text tasks, including: (i) Scene Text Detection, (ii) Script Identification, (iii) Cropped Word Recognition, and (iv) End-to-End Scene Text Recognition. We evaluated state-of-the-art models originally developed for English by adapting (fine-tuning) them for Indian languages. Our results highlight the challenges and opportunities in Indian language scene text recognition. We believe that this dataset represents a significant step toward advancing research in this domain. All our models and data are open source.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2511.19806.pdf' target='_blank'>https://arxiv.org/pdf/2511.19806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jihan Yao, Achin Kulshrestha, Nathalie Rauschmayr, Reed Roberts, Banghua Zhu, Yulia Tsvetkov, Federico Tombari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19806">Reading Between the Lines: Abstaining from VLM-Generated OCR Errors via Latent Representation Probes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As VLMs are deployed in safety-critical applications, their ability to abstain from answering when uncertain becomes crucial for reliability, especially in Scene Text Visual Question Answering (STVQA) tasks. For example, OCR errors like misreading "50 mph" as "60 mph" could cause severe traffic accidents. This leads us to ask: Can VLMs know when they can't see? Existing abstention methods suggest pessimistic answers: they either rely on miscalibrated output probabilities or require semantic agreement unsuitable for OCR tasks. However, this failure may indicate we are looking in the wrong place: uncertainty signals could be hidden in VLMs' internal representations. Building on this insight, we propose Latent Representation Probing (LRP): training lightweight probes on hidden states or attention patterns. We explore three probe designs: concatenating representations across all layers, aggregating attention over visual tokens, and ensembling single layer probes by majority vote. Experiments on four benchmarks across image and video modalities show LRP improves abstention accuracy by 7.6\% over best baselines. Our analysis reveals: probes generalize across various uncertainty sources and datasets, and optimal signals emerge from intermediate rather than final layers. This establishes a principled framework for building deployment-ready AI systems by detecting confidence signals from internal states rather than unreliable outputs.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2510.26339.pdf' target='_blank'>https://arxiv.org/pdf/2510.26339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyu Sung, Seungjae Ham, Kangwoo Kim, Yeokyoung Yoon, Sangseok Yun, Il-Min Kim, Jae-Mo Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.26339">GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image super-resolution(SR) is fundamental to many vision system-from surveillance and autonomy to document analysis and retail analytics-because recovering high-frequency details, especially scene-text, enables reliable downstream perception. Scene-text, i.e., text embedded in natural images such as signs, product labels, and storefronts, often carries the most actionable information; when characters are blurred or hallucinated, optical character recognition(OCR) and subsequent decisions fail even if the rest of the image appears sharp. Yet previous SR research has often been tuned to distortion (PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that are largely insensitive to character-level errors. Furthermore, studies that do address text SR often focus on simplified benchmarks with isolated characters, overlooking the challenges of text within complex natural scenes. As a result, scene-text is effectively treated as generic texture. For SR to be effective in practical deployments, it is therefore essential to explicitly optimize for both text legibility and perceptual quality. We present GLYPH-SR, a vision-language-guided diffusion framework that aims to achieve both objectives jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by OCR data, and a ping-pong scheduler that alternates between text- and scene-centric guidance. To enable targeted text restoration, we train these components on a synthetic corpus while keeping the main SR branch frozen. Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR) while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed to satisfy both objectives simultaneously-high readability and high visual realism-delivering SR that looks right and reds right.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2505.17778.pdf' target='_blank'>https://arxiv.org/pdf/2505.17778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Xie, Jielei Zhang, Pengyu Chen, Ziyue Wang, Weihang Wang, Longwen Gao, Peiyi Li, Huyang Sun, Qiang Zhang, Qian Qiao, Jiaqing Fan, Zhouhui Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17778">TextFlux: An OCR-Free DiT Model for High-Fidelity Multilingual Scene Text Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based scene text synthesis has progressed rapidly, yet existing methods commonly rely on additional visual conditioning modules and require large-scale annotated data to support multilingual generation. In this work, we revisit the necessity of complex auxiliary modules and further explore an approach that simultaneously ensures glyph accuracy and achieves high-fidelity scene integration, by leveraging diffusion models' inherent capabilities for contextual reasoning. To this end, we introduce TextFlux, a DiT-based framework that enables multilingual scene text synthesis. The advantages of TextFlux can be summarized as follows: (1) OCR-free model architecture. TextFlux eliminates the need for OCR encoders (additional visual conditioning modules) that are specifically used to extract visual text-related features. (2) Strong multilingual scalability. TextFlux is effective in low-resource multilingual settings, and achieves strong performance in newly added languages with fewer than 1,000 samples. (3) Streamlined training setup. TextFlux is trained with only 1% of the training data required by competing methods. (4) Controllable multi-line text generation. TextFlux offers flexible multi-line synthesis with precise line-level control, outperforming methods restricted to single-line or rigid layouts. Extensive experiments and visualizations demonstrate that TextFlux outperforms previous methods in both qualitative and quantitative evaluations.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2412.19504.pdf' target='_blank'>https://arxiv.org/pdf/2412.19504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Li, Bo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19504">Hear the Scene: Audio-Enhanced Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in scene text spotting have focused on end-to-end methodologies that heavily rely on precise location annotations, which are often costly and labor-intensive to procure. In this study, we introduce an innovative approach that leverages only transcription annotations for training text spotting models, substantially reducing the dependency on elaborate annotation processes. Our methodology employs a query-based paradigm that facilitates the learning of implicit location features through the interaction between text queries and image embeddings. These features are later refined during the text recognition phase using an attention activation map. Addressing the challenges associated with training a weakly-supervised model from scratch, we implement a circular curriculum learning strategy to enhance model convergence. Additionally, we introduce a coarse-to-fine cross-attention localization mechanism for more accurate text instance localization. Notably, our framework supports audio-based annotation, which significantly diminishes annotation time and provides an inclusive alternative for individuals with disabilities. Our approach achieves competitive performance against existing benchmarks, demonstrating that high accuracy in text spotting can be attained without extensive location annotations.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2408.00106.pdf' target='_blank'>https://arxiv.org/pdf/2408.00106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Xie, Yuzhe Li, Yang Liu, Zhifei Zhang, Zhaowen Wang, Wei Xiong, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00106">WAS: Dataset and Methods for Artistic Text Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate text segmentation results are crucial for text-related generative tasks, such as text image generation, text editing, text removal, and text style transfer. Recently, some scene text segmentation methods have made significant progress in segmenting regular text. However, these methods perform poorly in scenarios containing artistic text. Therefore, this paper focuses on the more challenging task of artistic text segmentation and constructs a real artistic text segmentation dataset. One challenge of the task is that the local stroke shapes of artistic text are changeable with diversity and complexity. We propose a decoder with the layer-wise momentum query to prevent the model from ignoring stroke regions of special shapes. Another challenge is the complexity of the global topological structure. We further design a skeleton-assisted head to guide the model to focus on the global structure. Additionally, to enhance the generalization performance of the text segmentation model, we propose a strategy for training data synthesis, based on the large multi-modal model and the diffusion model. Experimental results show that our proposed method and synthetic dataset can significantly enhance the performance of artistic text segmentation and achieve state-of-the-art results on other public datasets.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2404.12734.pdf' target='_blank'>https://arxiv.org/pdf/2404.12734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Da Chang, Yu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12734">Mixed Text Recognition with Efficient Parameter Fine-Tuning and Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of OCR technology, mixed-scene text recognition has become a key technical challenge. Although deep learning models have achieved significant results in specific scenarios, their generality and stability still need improvement, and the high demand for computing resources affects flexibility. To address these issues, this paper proposes DLoRA-TrOCR, a parameter-efficient hybrid text spotting method based on a pre-trained OCR Transformer. By embedding a weight-decomposed DoRA module in the image encoder and a LoRA module in the text decoder, this method can be efficiently fine-tuned on various downstream tasks. Our method requires no more than 0.7\% trainable parameters, not only accelerating the training efficiency but also significantly improving the recognition accuracy and cross-dataset generalization performance of the OCR system in mixed text scenes. Experiments show that our proposed DLoRA-TrOCR outperforms other parameter-efficient fine-tuning methods in recognizing complex scenes with mixed handwritten, printed, and street text, achieving a CER of 4.02 on the IAM dataset, a F1 score of 94.29 on the SROIE dataset, and a WAR of 86.70 on the STR Benchmark, reaching state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2509.17707.pdf' target='_blank'>https://arxiv.org/pdf/2509.17707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emre GÃ¼lsoylu, Alhassan Abdelhalim, Derya Kara Boztas, Ole Grasse, Carlos Jahn, Simone Frintrop, Janick Edinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17707">Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The standardisation of Intermodal Loading Units (ILUs), such as containers, semi-trailers and swap bodies, has revolutionised global trade yet their efficient and robust identification remains a critical bottleneck in high-throughput ports and terminals. This paper reviews 63 empirical studies that propose computer vision (CV) based solutions. It covers the last 35 years (1990-2025), tracing the field's evolution from early digital image processing (DIP) and traditional machine learning (ML) to the current dominance of deep learning (DL) techniques. While CV offers cost-effective alternatives for other types of identification techniques, its development is hindered by the lack of publicly available benchmarking datasets. This results in high variance for the reported results such as end-to-end accuracy ranging from 5 % to 96 %. Beyond dataset limitations, this review highlights the emerging challenges especially introduced by the shift from character-based text recognition to scene-text spotting and the integration of mobile cameras (e.g. drones, sensor equipped ground vehicles) for dynamic terminal monitoring. To advance the field, the paper calls for standardised terminology, open-access datasets, shared source code, while outlining future research directions such as contextless text recognition optimised for ISO6346 codes.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2504.04001.pdf' target='_blank'>https://arxiv.org/pdf/2504.04001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuang Yang, Xu Han, Tao Han, Han Han, Bingxuan Zhao, Qi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04001">Edge Approximation Text Detector</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pursuing efficient text shape representations helps scene text detection models focus on compact foreground regions and optimize the contour reconstruction steps to simplify the whole detection pipeline. Current approaches either represent irregular shapes via box-to-polygon strategy or decomposing a contour into pieces for fitting gradually, the deficiency of coarse contours or complex pipelines always exists in these models. Considering the above issues, we introduce EdgeText to fit text contours compactly while alleviating excessive contour rebuilding processes. Concretely, it is observed that the two long edges of texts can be regarded as smooth curves. It allows us to build contours via continuous and smooth edges that cover text regions tightly instead of fitting piecewise, which helps avoid the two limitations in current models. Inspired by this observation, EdgeText formulates the text representation as the edge approximation problem via parameterized curve fitting functions. In the inference stage, our model starts with locating text centers, and then creating curve functions for approximating text edges relying on the points. Meanwhile, truncation points are determined based on the location features. In the end, extracting curve segments from curve functions by using the pixel coordinate information brought by truncation points to reconstruct text contours. Furthermore, considering the deep dependency of EdgeText on text edges, a bilateral enhanced perception (BEP) module is designed. It encourages our model to pay attention to the recognition of edge features. Additionally, to accelerate the learning of the curve function parameters, we introduce a proportional integral loss (PI-loss) to force the proposed model to focus on the curve distribution and avoid being disturbed by text scales.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2411.04642.pdf' target='_blank'>https://arxiv.org/pdf/2411.04642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Fhima, Elad Ben Avraham, Oren Nuriel, Yair Kittenplon, Roy Ganz, Aviad Aberdam, Ron Litman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04642">TAP-VL: Text Layout-Aware Pre-training for Enriched Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language (VL) models have garnered considerable research interest; however, they still face challenges in effectively handling text within images. To address this limitation, researchers have developed two approaches. The first method involves utilizing external Optical Character Recognition (OCR) tools to extract textual information from images, which is then prepended to other textual inputs. The second strategy focuses on employing extremely high-resolution images to improve text recognition capabilities. In this paper, we focus on enhancing the first strategy by introducing a novel method, named TAP-VL, which treats OCR information as a distinct modality and seamlessly integrates it into any VL model. TAP-VL employs a lightweight transformer-based OCR module to receive OCR with layout information, compressing it into a short fixed-length sequence for input into the LLM. Initially, we conduct model-agnostic pretraining of the OCR module on unlabeled documents, followed by its integration into any VL architecture through brief fine-tuning. Extensive experiments demonstrate consistent performance improvements when applying TAP-VL to top-performing VL models, across scene-text and document-based VL benchmarks.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2404.17151.pdf' target='_blank'>https://arxiv.org/pdf/2404.17151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengpei Xu, Wenjing Jia, Ruomei Wang, Xiaonan Luo, Xiangjian He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17151">MorphText: Deep Morphology Regularized Arbitrary-shape Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bottom-up text detection methods play an important role in arbitrary-shape scene text detection but there are two restrictions preventing them from achieving their great potential, i.e., 1) the accumulation of false text segment detections, which affects subsequent processing, and 2) the difficulty of building reliable connections between text segments. Targeting these two problems, we propose a novel approach, named ``MorphText", to capture the regularity of texts by embedding deep morphology for arbitrary-shape text detection. Towards this end, two deep morphological modules are designed to regularize text segments and determine the linkage between them. First, a Deep Morphological Opening (DMOP) module is constructed to remove false text segment detections generated in the feature extraction process. Then, a Deep Morphological Closing (DMCL) module is proposed to allow text instances of various shapes to stretch their morphology along their most significant orientation while deriving their connections. Extensive experiments conducted on four challenging benchmark datasets (CTW1500, Total-Text, MSRA-TD500 and ICDAR2017) demonstrate that our proposed MorphText outperforms both top-down and bottom-up state-of-the-art arbitrary-shape scene text detection approaches.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2402.05472.pdf' target='_blank'>https://arxiv.org/pdf/2402.05472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roy Ganz, Yair Kittenplon, Aviad Aberdam, Elad Ben Avraham, Oren Nuriel, Shai Mazor, Ron Litman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05472">Question Aware Vision Transformer for Multimodal Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language (VL) models have gained significant research focus, enabling remarkable advances in multimodal reasoning. These architectures typically comprise a vision encoder, a Large Language Model (LLM), and a projection module that aligns visual features with the LLM's representation space. Despite their success, a critical limitation persists: the vision encoding process remains decoupled from user queries, often in the form of image-related questions. Consequently, the resulting visual features may not be optimally attuned to the query-specific elements of the image. To address this, we introduce QA-ViT, a Question Aware Vision Transformer approach for multimodal reasoning, which embeds question awareness directly within the vision encoder. This integration results in dynamic visual features focusing on relevant image aspects to the posed question. QA-ViT is model-agnostic and can be incorporated efficiently into any VL architecture. Extensive experiments demonstrate the effectiveness of applying our method to various multimodal architectures, leading to consistent improvement across diverse tasks and showcasing its potential for enhancing visual and scene-text understanding.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2510.10910.pdf' target='_blank'>https://arxiv.org/pdf/2510.10910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Honghui Yuan, Keiji Yanai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10910">SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of diffusion models, style transfer has made remarkable progress. However, flexible and localized style editing for scene text remains an unsolved challenge. Although existing scene text editing methods have achieved text region editing, they are typically limited to content replacement and simple styles, which lack the ability of free-style transfer. In this paper, we introduce SceneTextStylizer, a novel training-free diffusion-based framework for flexible and high-fidelity style transfer of text in scene images. Unlike prior approaches that either perform global style transfer or focus solely on textual content modification, our method enables prompt-guided style transformation specifically for text regions, while preserving both text readability and stylistic consistency. To achieve this, we design a feature injection module that leverages diffusion model inversion and self-attention to transfer style features effectively. Additionally, a region control mechanism is introduced by applying a distance-based changing mask at each denoising step, enabling precise spatial control. To further enhance visual quality, we incorporate a style enhancement module based on the Fourier transform to reinforce stylistic richness. Extensive experiments demonstrate that our method achieves superior performance in scene text style transformation, outperforming existing state-of-the-art methods in both visual fidelity and text preservation.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2412.15523.pdf' target='_blank'>https://arxiv.org/pdf/2412.15523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Duan, Qianyi Jiang, Pei Fu, Jiamin Chen, Shengxi Li, Zining Wang, Shan Guo, Junfeng Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15523">InstructOCR: Instruction Boosting Scene Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of scene text spotting, previous OCR methods primarily relied on image encoders and pre-trained text information, but they often overlooked the advantages of incorporating human language instructions. To address this gap, we propose InstructOCR, an innovative instruction-based scene text spotting model that leverages human language instructions to enhance the understanding of text within images. Our framework employs both text and image encoders during training and inference, along with instructions meticulously designed based on text attributes. This approach enables the model to interpret text more accurately and flexibly. Extensive experiments demonstrate the effectiveness of our model and we achieve state-of-the-art results on widely used benchmarks. Furthermore, the proposed framework can be seamlessly applied to scene text VQA tasks. By leveraging instruction strategies during pre-training, the performance on downstream VQA tasks can be significantly improved, with a 2.6% increase on the TextVQA dataset and a 2.1% increase on the ST-VQA dataset. These experimental results provide insights into the benefits of incorporating human language instructions for OCR-related tasks.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2401.11704.pdf' target='_blank'>https://arxiv.org/pdf/2401.11704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyuan Zhu, Fagui Liu, Xi Chen, Quan Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11704">EK-Net:Real-time Scene Text Detection with Expand Kernel Distance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, scene text detection has received significant attention due to its wide application. However, accurate detection in complex scenes of multiple scales, orientations, and curvature remains a challenge. Numerous detection methods adopt the Vatti clipping (VC) algorithm for multiple-instance training to address the issue of arbitrary-shaped text. Yet we identify several bias results from these approaches called the "shrinked kernel". Specifically, it refers to a decrease in accuracy resulting from an output that overly favors the text kernel. In this paper, we propose a new approach named Expand Kernel Network (EK-Net) with expand kernel distance to compensate for the previous deficiency, which includes three-stages regression to complete instance detection. Moreover, EK-Net not only realize the precise positioning of arbitrary-shaped text, but also achieve a trade-off between performance and speed. Evaluation results demonstrate that EK-Net achieves state-of-the-art or competitive performance compared to other advanced methods, e.g., F-measure of 85.72% at 35.42 FPS on ICDAR 2015, F-measure of 85.75% at 40.13 FPS on CTW1500.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2401.05338.pdf' target='_blank'>https://arxiv.org/pdf/2401.05338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daqian Shao, Lukas Fesser, Marta Kwiatkowska
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05338">STR-Cert: Robustness Certification for Deep Text Recognition on Deep Learning Pipelines and Vision Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robustness certification, which aims to formally certify the predictions of neural networks against adversarial inputs, has become an integral part of important tool for safety-critical applications. Despite considerable progress, existing certification methods are limited to elementary architectures, such as convolutional networks, recurrent networks and recently Transformers, on benchmark datasets such as MNIST. In this paper, we focus on the robustness certification of scene text recognition (STR), which is a complex and extensively deployed image-based sequence prediction problem. We tackle three types of STR model architectures, including the standard STR pipelines and the Vision Transformer. We propose STR-Cert, the first certification method for STR models, by significantly extending the DeepPoly polyhedral verification framework via deriving novel polyhedral bounds and algorithms for key STR model components. Finally, we certify and compare STR models on six datasets, demonstrating the efficiency and scalability of robustness certification, particularly for the Vision Transformer.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2510.07951.pdf' target='_blank'>https://arxiv.org/pdf/2510.07951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Dong, Yurui Zhang, Changmao Li, Naomi Rue Golding, Qing Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07951">A Large-scale Dataset for Robust Complex Anime Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current text detection datasets primarily target natural or document scenes, where text typically appear in regular font and shapes, monotonous colors, and orderly layouts. The text usually arranged along straight or curved lines. However, these characteristics differ significantly from anime scenes, where text is often diverse in style, irregularly arranged, and easily confused with complex visual elements such as symbols and decorative patterns. Text in anime scene also includes a large number of handwritten and stylized fonts. Motivated by this gap, we introduce AnimeText, a large-scale dataset containing 735K images and 4.2M annotated text blocks. It features hierarchical annotations and hard negative samples tailored for anime scenarios. %Cross-dataset evaluations using state-of-the-art methods demonstrate that models trained on AnimeText achieve superior performance in anime text detection tasks compared to existing datasets. To evaluate the robustness of AnimeText in complex anime scenes, we conducted cross-dataset benchmarking using state-of-the-art text detection methods. Experimental results demonstrate that models trained on AnimeText outperform those trained on existing datasets in anime scene text detection tasks. AnimeText on HuggingFace: https://huggingface.co/datasets/deepghs/AnimeText
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2410.18277.pdf' target='_blank'>https://arxiv.org/pdf/2410.18277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vannkinh Nom, Souhail Bakkali, Muhammad Muzzamil Luqman, MickaÃ«l Coustaty, Jean-Marc Ogier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18277">KhmerST: A Low-Resource Khmer Scene Text Detection and Recognition Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing effective scene text detection and recognition models hinges on extensive training data, which can be both laborious and costly to obtain, especially for low-resourced languages. Conventional methods tailored for Latin characters often falter with non-Latin scripts due to challenges like character stacking, diacritics, and variable character widths without clear word boundaries. In this paper, we introduce the first Khmer scene-text dataset, featuring 1,544 expert-annotated images, including 997 indoor and 547 outdoor scenes. This diverse dataset includes flat text, raised text, poorly illuminated text, distant and partially obscured text. Annotations provide line-level text and polygonal bounding box coordinates for each scene. The benchmark includes baseline models for scene-text detection and recognition tasks, providing a robust starting point for future research endeavors. The KhmerST dataset is publicly accessible at https://gitlab.com/vannkinhnom123/khmerst.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2408.00355.pdf' target='_blank'>https://arxiv.org/pdf/2408.00355.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Xie, Qian Qiao, Jun Gao, Tianxiang Wu, Jiaqing Fan, Yue Zhang, Jielei Zhang, Huyang Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00355">DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved Denoising Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>More and more end-to-end text spotting methods based on Transformer architecture have demonstrated superior performance. These methods utilize a bipartite graph matching algorithm to perform one-to-one optimal matching between predicted objects and actual objects. However, the instability of bipartite graph matching can lead to inconsistent optimization targets, thereby affecting the training performance of the model. Existing literature applies denoising training to solve the problem of bipartite graph matching instability in object detection tasks. Unfortunately, this denoising training method cannot be directly applied to text spotting tasks, as these tasks need to perform irregular shape detection tasks and more complex text recognition tasks than classification. To address this issue, we propose a novel denoising training method (DNTextSpotter) for arbitrary-shaped text spotting. Specifically, we decompose the queries of the denoising part into noised positional queries and noised content queries. We use the four Bezier control points of the Bezier center curve to generate the noised positional queries. For the noised content queries, considering that the output of the text in a fixed positional order is not conducive to aligning position with content, we employ a masked character sliding method to initialize noised content queries, thereby assisting in the alignment of text content and position. To improve the model's perception of the background, we further utilize an additional loss function for background characters classification in the denoising training part.Although DNTextSpotter is conceptually simple, it outperforms the state-of-the-art methods on four benchmarks (Total-Text, SCUT-CTW1500, ICDAR15, and Inverse-Text), especially yielding an improvement of 11.3% against the best approach in Inverse-Text dataset.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2512.14050.pdf' target='_blank'>https://arxiv.org/pdf/2512.14050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjun Liu, Qian Wu, Yifeng Hu, Yuke Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14050">SELECT: Detecting Label Errors in Real-world Scene Text Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce SELECT (Scene tExt Label Errors deteCTion), a novel approach that leverages multi-modal training to detect label errors in real-world scene text datasets. Utilizing an image-text encoder and a character-level tokenizer, SELECT addresses the issues of variable-length sequence labels, label sequence misalignment, and character-level errors, outperforming existing methods in accuracy and practical utility. In addition, we introduce Similarity-based Sequence Label Corruption (SSLC), a process that intentionally introduces errors into the training labels to mimic real-world error scenarios during training. SSLC not only can cause a change in the sequence length but also takes into account the visual similarity between characters during corruption. Our method is the first to detect label errors in real-world scene text datasets successfully accounting for variable-length labels. Experimental results demonstrate the effectiveness of SELECT in detecting label errors and improving STR accuracy on real-world text datasets, showcasing its practical utility.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2511.19820.pdf' target='_blank'>https://arxiv.org/pdf/2511.19820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miguel Carvalho, Helder Dias, Bruno Martins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19820">CropVLM: Learning to Zoom for Fine-Grained Vision-Language Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) often struggle with tasks that require fine-grained image understanding, such as scene-text recognition or document analysis, due to perception limitations and visual fragmentation. To address these challenges, we introduce CropVLM as an external low-cost method for boosting performance, enabling VLMs to dynamically ''zoom in'' on relevant image regions, enhancing their ability to capture fine details. CropVLM is trained using reinforcement learning, without using human-labeled bounding boxes as a supervision signal, and without expensive synthetic evaluations. The model is trained once and can be paired with both open-source and proprietary VLMs to improve their performance. Our approach delivers significant improvements on tasks that require high-resolution image understanding, notably for benchmarks that are out-of-domain for the target VLM, without modifying or fine-tuning the VLM, thus avoiding catastrophic forgetting.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2511.09977.pdf' target='_blank'>https://arxiv.org/pdf/2511.09977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongdeuk Seo, Hyun-seok Min, Sungchul Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09977">STELLAR: Scene Text Editor for Low-Resource Languages and Real-World Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Text Editing (STE) is the task of modifying text content in an image while preserving its visual style, such as font, color, and background. While recent diffusion-based approaches have shown improvements in visual quality, key limitations remain: lack of support for low-resource languages, domain gap between synthetic and real data, and the absence of appropriate metrics for evaluating text style preservation. To address these challenges, we propose STELLAR (Scene Text Editor for Low-resource LAnguages and Real-world data). STELLAR enables reliable multilingual editing through a language-adaptive glyph encoder and a multi-stage training strategy that first pre-trains on synthetic data and then fine-tunes on real images. We also construct a new dataset, STIPLAR(Scene Text Image Pairs of Low-resource lAnguages and Real-world data), for training and evaluation. Furthermore, we propose Text Appearance Similarity (TAS), a novel metric that assesses style preservation by independently measuring font, color, and background similarity, enabling robust evaluation even without ground truth. Experimental results demonstrate that STELLAR outperforms state-of-the-art models in visual consistency and recognition accuracy, achieving an average TAS improvement of 2.2% across languages over the baselines.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2501.02584.pdf' target='_blank'>https://arxiv.org/pdf/2501.02584.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miguel Carvalho, Bruno Martins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02584">Efficient Architectures for High Resolution Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This work introduces Pheye, a novel architecture that efficiently processes high-resolution images while training fewer parameters than similarly sized VLMs. Notably, Pheye achieves a high efficiency while maintaining strong performance, particularly in tasks that demand fine-grained image understanding and/or the handling of scene-text.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2410.21721.pdf' target='_blank'>https://arxiv.org/pdf/2410.21721.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanhita Pathak, Vinay Kaushik, Brejesh Lall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21721">DiffSTR: Controlled Diffusion Models for Scene Text Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To prevent unauthorized use of text in images, Scene Text Removal (STR) has become a crucial task. It focuses on automatically removing text and replacing it with a natural, text-less background while preserving significant details such as texture, color, and contrast. Despite its importance in privacy protection, STR faces several challenges, including boundary artifacts, inconsistent texture and color, and preserving correct shadows. Most STR approaches estimate a text region mask to train a model, solving for image translation or inpainting to generate a text-free image. Thus, the quality of the generated image depends on the accuracy of the inpainting mask and the generator's capability. In this work, we leverage the superior capabilities of diffusion models in generating high-quality, consistent images to address the STR problem. We introduce a ControlNet diffusion model, treating STR as an inpainting task. To enhance the model's robustness, we develop a mask pretraining pipeline to condition our diffusion model. This involves training a masked autoencoder (MAE) using a combination of box masks and coarse stroke masks, and fine-tuning it using masks derived from our novel segmentation-based mask refinement framework. This framework iteratively refines an initial mask and segments it using the SLIC and Hierarchical Feature Selection (HFS) algorithms to produce an accurate final text mask. This improves mask prediction and utilizes rich textural information in natural scene images to provide accurate inpainting masks. Experiments on the SCUT-EnsText and SCUT-Syn datasets demonstrate that our method significantly outperforms existing state-of-the-art techniques.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2409.17747.pdf' target='_blank'>https://arxiv.org/pdf/2409.17747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chihiro Noguchi, Shun Fukuda, Shoichiro Mihara, Masao Yamanaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17747">Text Image Generation for Low-Resource Languages with Dual Translation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition in low-resource languages frequently faces challenges due to the limited availability of training datasets derived from real-world scenes. This study proposes a novel approach that generates text images in low-resource languages by emulating the style of real text images from high-resource languages. Our approach utilizes a diffusion model that is conditioned on binary states: ``synthetic'' and ``real.'' The training of this model involves dual translation tasks, where it transforms plain text images into either synthetic or real text images, based on the binary states. This approach not only effectively differentiates between the two domains but also facilitates the model's explicit recognition of characters in the target language. Furthermore, to enhance the accuracy and variety of generated text images, we introduce two guidance techniques: Fidelity-Diversity Balancing Guidance and Fidelity Enhancement Guidance. Our experimental results demonstrate that the text images generated by our proposed framework can significantly improve the performance of scene text recognition models for low-resource languages.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2405.09125.pdf' target='_blank'>https://arxiv.org/pdf/2405.09125.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Honghui Chen, Yuhang Qiu, Jiabao Wang, Pingping Chen, Nam Ling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09125">HAAP: Vision-context Hierarchical Attention Autoregressive with Adaptive Permutation for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Internal Language Model (LM)-based methods use permutation language modeling (PLM) to solve the error correction caused by conditional independence in external LM-based methods. However, random permutations of human interference cause fit oscillations in the model training, and Iterative Refinement (IR) operation to improve multimodal information decoupling also introduces additional overhead. To address these issues, this paper proposes the Hierarchical Attention autoregressive Model with Adaptive Permutation (HAAP) to enhance the location-context-image interaction capability, improving autoregressive generalization with internal LM. First, we propose Implicit Permutation Neurons (IPN) to generate adaptive attention masks to dynamically exploit token dependencies. The adaptive masks increase the diversity of training data and prevent model dependency on a specific order. It reduces the training overhead of PLM while avoiding training fit oscillations. Second, we develop Cross-modal Hierarchical Attention mechanism (CHA) to couple context and image features. This processing establishes rich positional semantic dependencies between context and image while avoiding IR. Extensive experimental results show the proposed HAAP achieves state-of-the-art (SOTA) performance in terms of accuracy, complexity, and latency on several datasets.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2403.09288.pdf' target='_blank'>https://arxiv.org/pdf/2403.09288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixuan Shen, Haonan Luo, Sijia Li, Tianrui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09288">Adversarial Training with OCR Modality Perturbation for Scene-Text Visual Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene-Text Visual Question Answering (ST-VQA) aims to understand scene text in images and answer questions related to the text content. Most existing methods heavily rely on the accuracy of Optical Character Recognition (OCR) systems, and aggressive fine-tuning based on limited spatial location information and erroneous OCR text information often leads to inevitable overfitting. In this paper, we propose a multimodal adversarial training architecture with spatial awareness capabilities. Specifically, we introduce an Adversarial OCR Enhancement (AOE) module, which leverages adversarial training in the embedding space of OCR modality to enhance fault-tolerant representation of OCR texts, thereby reducing noise caused by OCR errors. Simultaneously, We add a Spatial-Aware Self-Attention (SASA) mechanism to help the model better capture the spatial relationships among OCR tokens. Various experiments demonstrate that our method achieves significant performance improvements on both the ST-VQA and TextVQA datasets and provides a novel paradigm for multimodal adversarial training.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2402.11540.pdf' target='_blank'>https://arxiv.org/pdf/2402.11540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longhuang Wu, Shangxuan Tian, Youxin Wang, Pengfei Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11540">CPN: Complementary Proposal Network for Unconstrained Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing methods for scene text detection can be divided into two paradigms: segmentation-based and anchor-based. While Segmentation-based methods are well-suited for irregular shapes, they struggle with compact or overlapping layouts. Conversely, anchor-based approaches excel for complex layouts but suffer from irregular shapes. To strengthen their merits and overcome their respective demerits, we propose a Complementary Proposal Network (CPN) that seamlessly and parallelly integrates semantic and geometric information for superior performance. The CPN comprises two efficient networks for proposal generation: the Deformable Morphology Semantic Network, which generates semantic proposals employing an innovative deformable morphological operator, and the Balanced Region Proposal Network, which produces geometric proposals with pre-defined anchors. To further enhance the complementarity, we introduce an Interleaved Feature Attention module that enables semantic and geometric features to interact deeply before proposal generation. By leveraging both complementary proposals and features, CPN outperforms state-of-the-art approaches with significant margins under comparable computation cost. Specifically, our approach achieves improvements of 3.6%, 1.3% and 1.0% on challenging benchmarks ICDAR19-ArT, IC15, and MSRA-TD500, respectively. Code for our method will be released.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2503.15639.pdf' target='_blank'>https://arxiv.org/pdf/2503.15639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ritabrata Chakraborty, Shivakumara Palaiahnakote, Umapada Pal, Cheng-Lin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15639">A Context-Driven Training-Free Network for Lightweight Scene Text Segmentation and Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern scene text recognition systems often depend on large end-to-end architectures that require extensive training and are prohibitively expensive for real-time scenarios. In such cases, the deployment of heavy models becomes impractical due to constraints on memory, computational resources, and latency. To address these challenges, we propose a novel, training-free plug-and-play framework that leverages the strengths of pre-trained text recognizers while minimizing redundant computations. Our approach uses context-based understanding and introduces an attention-based segmentation stage, which refines candidate text regions at the pixel level, improving downstream recognition. Instead of performing traditional text detection that follows a block-level comparison between feature map and source image and harnesses contextual information using pretrained captioners, allowing the framework to generate word predictions directly from scene context.Candidate texts are semantically and lexically evaluated to get a final score. Predictions that meet or exceed a pre-defined confidence threshold bypass the heavier process of end-to-end text STR profiling, ensuring faster inference and cutting down on unnecessary computations. Experiments on public benchmarks demonstrate that our paradigm achieves performance on par with state-of-the-art systems, yet requires substantially fewer resources.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2405.09942.pdf' target='_blank'>https://arxiv.org/pdf/2405.09942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siliang Ma, Yong Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09942">FPDIoU Loss: A Loss Function for Efficient Bounding Box Regression of Rotated Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bounding box regression is one of the important steps of object detection. However, rotation detectors often involve a more complicated loss based on SkewIoU which is unfriendly to gradient-based training. Most of the existing loss functions for rotated object detection calculate the difference between two bounding boxes only focus on the deviation of area or each points distance (e.g., $\mathcal{L}_{Smooth-\ell 1}$, $\mathcal{L}_{RotatedIoU}$ and $\mathcal{L}_{PIoU}$). The calculation process of some loss functions is extremely complex (e.g. $\mathcal{L}_{KFIoU}$). In order to improve the efficiency and accuracy of bounding box regression for rotated object detection, we proposed a novel metric for arbitrary shapes comparison based on minimum points distance, which takes most of the factors from existing loss functions for rotated object detection into account, i.e., the overlap or nonoverlapping area, the central points distance and the rotation angle. We also proposed a loss function called $\mathcal{L}_{FPDIoU}$ based on four points distance for accurate bounding box regression focusing on faster and high quality anchor boxes. In the experiments, $FPDIoU$ loss has been applied to state-of-the-art rotated object detection (e.g., RTMDET, H2RBox) models training with three popular benchmarks of rotated object detection including DOTA, DIOR, HRSC2016 and two benchmarks of arbitrary orientation scene text detection including ICDAR 2017 RRC-MLT and ICDAR 2019 RRC-MLT, which achieves better performance than existing loss functions.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2512.22218.pdf' target='_blank'>https://arxiv.org/pdf/2512.22218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hieu Minh Nguyen, Tam Le-Thanh Dang, Kiet Van Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22218">Towards Signboard-Oriented Visual Question Answering: ViSignVQA Dataset, Method and Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding signboard text in natural scenes is essential for real-world applications of Visual Question Answering (VQA), yet remains underexplored, particularly in low-resource languages. We introduce ViSignVQA, the first large-scale Vietnamese dataset designed for signboard-oriented VQA, which comprises 10,762 images and 25,573 question-answer pairs. The dataset captures the diverse linguistic, cultural, and visual characteristics of Vietnamese signboards, including bilingual text, informal phrasing, and visual elements such as color and layout. To benchmark this task, we adapted state-of-the-art VQA models (e.g., BLIP-2, LaTr, PreSTU, and SaL) by integrating a Vietnamese OCR model (SwinTextSpotter) and a Vietnamese pretrained language model (ViT5). The experimental results highlight the significant role of the OCR-enhanced context, with F1-score improvements of up to 209% when the OCR text is appended to questions. Additionally, we propose a multi-agent VQA framework combining perception and reasoning agents with GPT-4, achieving 75.98% accuracy via majority voting. Our study presents the first large-scale multimodal dataset for Vietnamese signboard understanding. This underscores the importance of domain-specific resources in enhancing text-based VQA for low-resource languages. ViSignVQA serves as a benchmark capturing real-world scene text characteristics and supporting the development and evaluation of OCR-integrated VQA models in Vietnamese.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2512.12424.pdf' target='_blank'>https://arxiv.org/pdf/2512.12424.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tue-Thu Van-Dinh, Hoang-Duy Tran, Truong-Binh Duong, Mai-Hanh Pham, Binh-Nam Le-Nguyen, Quoc-Thai Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.12424">ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2512.06657.pdf' target='_blank'>https://arxiv.org/pdf/2512.06657.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiyan Zhao, Yue Yan, Da-Han Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06657">TextMamba: Scene Text Detector with Mamba</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\%, 89.2\%, and 78.5\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2511.22499.pdf' target='_blank'>https://arxiv.org/pdf/2511.22499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyakka Nakada, Marika Kubota
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22499">What Shape Is Optimal for Masks in Text Removal?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advent of generative models has dramatically improved the accuracy of image inpainting. In particular, by removing specific text from document images, reconstructing original images is extremely important for industrial applications. However, most existing methods of text removal focus on deleting simple scene text which appears in images captured by a camera in an outdoor environment. There is little research dedicated to complex and practical images with dense text. Therefore, we created benchmark data for text removal from images including a large amount of text. From the data, we found that text-removal performance becomes vulnerable against mask profile perturbation. Thus, for practical text-removal tasks, precise tuning of the mask shape is essential. This study developed a method to model highly flexible mask profiles and learn their parameters using Bayesian optimization. The resulting profiles were found to be character-wise masks. It was also found that the minimum cover of a text region is not optimal. Our research is expected to pave the way for a user-friendly guideline for manual masking.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2511.08133.pdf' target='_blank'>https://arxiv.org/pdf/2511.08133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lixu Sun, Nurmemet Yolwas, Wushour Silamu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08133">OTSNet: A Neurocognitive-Inspired Observation-Thinking-Spelling Pipeline for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Text Recognition (STR) remains challenging due to real-world complexities, where decoupled visual-linguistic optimization in existing frameworks amplifies error propagation through cross-modal misalignment. Visual encoders exhibit attention bias toward background distractors, while decoders suffer from spatial misalignment when parsing geometrically deformed text-collectively degrading recognition accuracy for irregular patterns. Inspired by the hierarchical cognitive processes in human visual perception, we propose OTSNet, a novel three-stage network embodying a neurocognitive-inspired Observation-Thinking-Spelling pipeline for unified STR modeling. The architecture comprises three core components: (1) a Dual Attention Macaron Encoder (DAME) that refines visual features through differential attention maps to suppress irrelevant regions and enhance discriminative focus; (2) a Position-Aware Module (PAM) and Semantic Quantizer (SQ) that jointly integrate spatial context with glyph-level semantic abstraction via adaptive sampling; and (3) a Multi-Modal Collaborative Verifier (MMCV) that enforces self-correction through cross-modal fusion of visual, semantic, and character-level features. Extensive experiments demonstrate that OTSNet achieves state-of-the-art performance, attaining 83.5% average accuracy on the challenging Union14M-L benchmark and 79.1% on the heavily occluded OST dataset-establishing new records across 9 out of 14 evaluation scenarios.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2511.06087.pdf' target='_blank'>https://arxiv.org/pdf/2511.06087.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Umar Rashid, Muhammad Arslan Arshad, Ghulam Ahmad, Muhammad Zeeshan Anjum, Rizwan Khan, Muhammad Akmal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06087">Hybrid CNN-ViT Framework for Motion-Blurred Scene Text Restoration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion blur in scene text images severely impairs readability and hinders the reliability of computer vision tasks, including autonomous driving, document digitization, and visual information retrieval. Conventional deblurring approaches are often inadequate in handling spatially varying blur and typically fall short in modeling the long-range dependencies necessary for restoring textual clarity. To overcome these limitations, we introduce a hybrid deep learning framework that combines convolutional neural networks (CNNs) with vision transformers (ViTs), thereby leveraging both local feature extraction and global contextual reasoning. The architecture employs a CNN-based encoder-decoder to preserve structural details, while a transformer module enhances global awareness through self-attention. Training is conducted on a curated dataset derived from TextOCR, where sharp scene-text samples are paired with synthetically blurred versions generated using realistic motion-blur kernels of multiple sizes and orientations. Model optimization is guided by a composite loss that incorporates mean absolute error (MAE), squared error (MSE), perceptual similarity, and structural similarity (SSIM). Quantitative evaluations show that the proposed method attains 32.20 dB in PSNR and 0.934 in SSIM, while remaining lightweight with 2.83 million parameters and an average inference time of 61 ms. These results highlight the effectiveness and computational efficiency of the CNN-ViT hybrid design, establishing its practicality for real-world motion-blurred scene-text restoration.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2511.02046.pdf' target='_blank'>https://arxiv.org/pdf/2511.02046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soham Joshi, Shwet Kamal Mishra, Viswanath Gopalakrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02046">Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creation of large-scale databases for Visual Question Answering tasks pertaining to the text data in a scene (text-VQA) involves skilful human annotation, which is tedious and challenging. With the advent of foundation models that handle vision and language modalities, and with the maturity of OCR systems, it is the need of the hour to establish an end-to-end pipeline that can synthesize Question-Answer (QA) pairs based on scene-text from a given image. We propose a pipeline for automated synthesis for text-VQA dataset that can produce faithful QA pairs, and which scales up with the availability of scene text data. Our proposed method harnesses the capabilities of multiple models and algorithms involving OCR detection and recognition (text spotting), region of interest (ROI) detection, caption generation, and question generation. These components are streamlined into a cohesive pipeline to automate the synthesis and validation of QA pairs. To the best of our knowledge, this is the first pipeline proposed to automatically synthesize and validate a large-scale text-VQA dataset comprising around 72K QA pairs based on around 44K images.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2508.01153.pdf' target='_blank'>https://arxiv.org/pdf/2508.01153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiahan Yang, Hui Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01153">TEACH: Text Encoding as Curriculum Hints for Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Text Recognition (STR) remains a challenging task due to complex visual appearances and limited semantic priors. We propose TEACH, a novel training paradigm that injects ground-truth text into the model as auxiliary input and progressively reduces its influence during training. By encoding target labels into the embedding space and applying loss-aware masking, TEACH simulates a curriculum learning process that guides the model from label-dependent learning to fully visual recognition. Unlike language model-based approaches, TEACH requires no external pretraining and introduces no inference overhead. It is model-agnostic and can be seamlessly integrated into existing encoder-decoder frameworks. Extensive experiments across multiple public benchmarks show that models trained with TEACH achieve consistently improved accuracy, especially under challenging conditions, validating its robustness and general applicability.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2505.18479.pdf' target='_blank'>https://arxiv.org/pdf/2505.18479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li-Syun Hsiung, Jun-Kai Tu, Kuan-Wu Chu, Yu-Hsuan Chiu, Yan-Tsung Peng, Sheng-Luen Chung, Gee-Sern Jison Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18479">Syn3DTxt: Embedding 3D Cues for Scene Text Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study aims to investigate the challenge of insufficient three-dimensional context in synthetic datasets for scene text rendering. Although recent advances in diffusion models and related techniques have improved certain aspects of scene text generation, most existing approaches continue to rely on 2D data, sourcing authentic training examples from movie posters and book covers, which limits their ability to capture the complex interactions among spatial layout and visual effects in real-world scenes. In particular, traditional 2D datasets do not provide the necessary geometric cues for accurately embedding text into diverse backgrounds. To address this limitation, we propose a novel standard for constructing synthetic datasets that incorporates surface normals to enrich three-dimensional scene characteristic. By adding surface normals to conventional 2D data, our approach aims to enhance the representation of spatial relationships and provide a more robust foundation for future scene text rendering methods. Extensive experiments demonstrate that datasets built under this new standard offer improved geometric context, facilitating further advancements in text rendering under complex 3D-spatial conditions.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2504.13690.pdf' target='_blank'>https://arxiv.org/pdf/2504.13690.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Usama, Syeda Aishah Asim, Syed Bilal Ali, Syed Talal Wasim, Umair Bin Mansoor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13690">Analysing the Robustness of Vision-Language-Models to Common Corruptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have demonstrated impressive capabilities in understanding and reasoning about visual and textual content. However, their robustness to common image corruptions remains under-explored. In this work, we present the first comprehensive analysis of VLM robustness across 19 corruption types from the ImageNet-C benchmark, spanning four categories: noise, blur, weather, and digital distortions. We introduce two new benchmarks, TextVQA-C and GQA-C, to systematically evaluate how corruptions affect scene text understanding and object-based reasoning, respectively. Our analysis reveals that transformer-based VLMs exhibit distinct vulnerability patterns across tasks: text recognition deteriorates most severely under blur and snow corruptions, while object reasoning shows higher sensitivity to corruptions such as frost and impulse noise. We connect these observations to the frequency-domain characteristics of different corruptions, revealing how transformers' inherent bias toward low-frequency processing explains their differential robustness patterns. Our findings provide valuable insights for developing more corruption-robust vision-language models for real-world applications.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2504.00410.pdf' target='_blank'>https://arxiv.org/pdf/2504.00410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongwoo Park, Suk Pil Ko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00410">NCAP: Scene Text Image Super-Resolution with Non-CAtegorical Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text image super-resolution (STISR) enhances the resolution and quality of low-resolution images. Unlike previous studies that treated scene text images as natural images, recent methods using a text prior (TP), extracted from a pre-trained text recognizer, have shown strong performance. However, two major issues emerge: (1) Explicit categorical priors, like TP, can negatively impact STISR if incorrect. We reveal that these explicit priors are unstable and propose replacing them with Non-CAtegorical Prior (NCAP) using penultimate layer representations. (2) Pre-trained recognizers used to generate TP struggle with low-resolution images. To address this, most studies jointly train the recognizer with the STISR network to bridge the domain gap between low- and high-resolution images, but this can cause an overconfidence phenomenon in the prior modality. We highlight this issue and propose a method to mitigate it by mixing hard and soft labels. Experiments on the TextZoom dataset demonstrate an improvement by 3.5%, while our method significantly enhances generalization performance by 14.8\% across four text recognition datasets. Our method generalizes to all TP-guided STISR networks.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2502.09026.pdf' target='_blank'>https://arxiv.org/pdf/2502.09026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Wei, Xiuzhuang Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09026">Billet Number Recognition Based on Test-Time Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>During the steel billet production process, it is essential to recognize machine-printed or manually written billet numbers on moving billets in real-time. To address the issue of low recognition accuracy for existing scene text recognition methods, caused by factors such as image distortions and distribution differences between training and test data, we propose a billet number recognition method that integrates test-time adaptation with prior knowledge. First, we introduce a test-time adaptation method into a model that uses the DB network for text detection and the SVTR network for text recognition. By minimizing the model's entropy during the testing phase, the model can adapt to the distribution of test data without the need for supervised fine-tuning. Second, we leverage the billet number encoding rules as prior knowledge to assess the validity of each recognition result. Invalid results, which do not comply with the encoding rules, are replaced. Finally, we introduce a validation mechanism into the CTC algorithm using prior knowledge to address its limitations in recognizing damaged characters. Experimental results on real datasets, including both machine-printed billet numbers and handwritten billet numbers, show significant improvements in evaluation metrics, validating the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2502.02951.pdf' target='_blank'>https://arxiv.org/pdf/2502.02951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Madhuri Latha Madaka, Chakravarthy Bhagvati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02951">VQA-Levels: A Hierarchical Approach for Classifying Questions in VQA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing datasets for Visual Question Answering (VQA) is a difficult and complex task that requires NLP for parsing and computer vision for analysing the relevant aspects of the image for answering the question asked. Several benchmark datasets have been developed by researchers but there are many issues with using them for methodical performance tests. This paper proposes a new benchmark dataset -- a pilot version called VQA-Levels is ready now -- for testing VQA systems systematically and assisting researchers in advancing the field. The questions are classified into seven levels ranging from direct answers based on low-level image features (without needing even a classifier) to those requiring high-level abstraction of the entire image content. The questions in the dataset exhibit one or many of ten properties. Each is categorised into a specific level from 1 to 7. Levels 1 - 3 are directly on the visual content while the remaining levels require extra knowledge about the objects in the image. Each question generally has a unique one or two-word answer. The questions are 'natural' in the sense that a human is likely to ask such a question when seeing the images. An example question at Level 1 is, ``What is the shape of the red colored region in the image?" while at Level 7, it is, ``Why is the man cutting the paper?". Initial testing of the proposed dataset on some of the existing VQA systems reveals that their success is high on Level 1 (low level features) and Level 2 (object classification) questions, least on Level 3 (scene text) followed by Level 6 (extrapolation) and Level 7 (whole scene analysis) questions. The work in this paper will go a long way to systematically analyze VQA systems.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2411.11150.pdf' target='_blank'>https://arxiv.org/pdf/2411.11150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raihan Kabir, Naznin Haque, Md Saiful Islam, Marium-E-Jannat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11150">A Comprehensive Survey on Visual Question Answering Datasets and Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual question answering (VQA) refers to the problem where, given an image and a natural language question about the image, a correct natural language answer has to be generated. A VQA model has to demonstrate both the visual understanding of the image and the semantic understanding of the question, demonstrating reasoning capability. Since the inception of this field, a plethora of VQA datasets and models have been published. In this article, we meticulously analyze the current state of VQA datasets and models, while cleanly dividing them into distinct categories and then summarizing the methodologies and characteristics of each category. We divide VQA datasets into four categories: (1) available datasets that contain a rich collection of authentic images, (2) synthetic datasets that contain only synthetic images produced through artificial means, (3) diagnostic datasets that are specially designed to test model performance in a particular area, e.g., understanding the scene text, and (4) KB (Knowledge-Based) datasets that are designed to measure a model's ability to utilize outside knowledge. Concurrently, we explore six main paradigms of VQA models: fusion, where we discuss different methods of fusing information between visual and textual modalities; attention, the technique of using information from one modality to filter information from another; external knowledge base, where we discuss different models utilizing outside information; composition or reasoning, where we analyze techniques to answer advanced questions that require complex reasoning steps; explanation, which is the process of generating visual and textual descriptions to verify sound reasoning; and graph models, which encode and manipulate relationships through nodes in a graph. We also discuss some miscellaneous topics, such as scene text understanding, counting, and bias reduction.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2411.00355.pdf' target='_blank'>https://arxiv.org/pdf/2411.00355.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengcheng Li, Fei Chao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00355">TextDestroyer: A Training- and Annotation-Free Diffusion Method for Destroying Anomal Text from Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose TextDestroyer, the first training- and annotation-free method for scene text destruction using a pre-trained diffusion model. Existing scene text removal models require complex annotation and retraining, and may leave faint yet recognizable text information, compromising privacy protection and content concealment. TextDestroyer addresses these issues by employing a three-stage hierarchical process to obtain accurate text masks. Our method scrambles text areas in the latent start code using a Gaussian distribution before reconstruction. During the diffusion denoising process, self-attention key and value are referenced from the original latent to restore the compromised background. Latent codes saved at each inversion step are used for replacement during reconstruction, ensuring perfect background restoration. The advantages of TextDestroyer include: (1) it eliminates labor-intensive data annotation and resource-intensive training; (2) it achieves more thorough text destruction, preventing recognizable traces; and (3) it demonstrates better generalization capabilities, performing well on both real-world scenes and generated images.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2403.08007.pdf' target='_blank'>https://arxiv.org/pdf/2403.08007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harsh Lunia, Ajoy Mondal, C V Jawahar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08007">IndicSTR12: A Dataset for Indic Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The importance of Scene Text Recognition (STR) in today's increasingly digital world cannot be overstated. Given the significance of STR, data intensive deep learning approaches that auto-learn feature mappings have primarily driven the development of STR solutions. Several benchmark datasets and substantial work on deep learning models are available for Latin languages to meet this need. On more complex, syntactically and semantically, Indian languages spoken and read by 1.3 billion people, there is less work and datasets available. This paper aims to address the Indian space's lack of a comprehensive dataset by proposing the largest and most comprehensive real dataset - IndicSTR12 - and benchmarking STR performance on 12 major Indian languages. A few works have addressed the same issue, but to the best of our knowledge, they focused on a small number of Indian languages. The size and complexity of the proposed dataset are comparable to those of existing Latin contemporaries, while its multilingualism will catalyse the development of robust text detection and recognition models. It was created specifically for a group of related languages with different scripts. The dataset contains over 27000 word-images gathered from various natural scenes, with over 1000 word-images for each language. Unlike previous datasets, the images cover a broader range of realistic conditions, including blur, illumination changes, occlusion, non-iconic texts, low resolution, perspective text etc. Along with the new dataset, we provide a high-performing baseline on three models - PARSeq, CRNN, and STARNet.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2403.07518.pdf' target='_blank'>https://arxiv.org/pdf/2403.07518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuhua Ren, Hengcan Shi, Jin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07518">Open-Vocabulary Scene Text Recognition via Pseudo-Image Labeling and Margin Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene text recognition is an important and challenging task in computer vision. However, most prior works focus on recognizing pre-defined words, while there are various out-of-vocabulary (OOV) words in real-world applications.
  In this paper, we propose a novel open-vocabulary text recognition framework, Pseudo-OCR, to recognize OOV words. The key challenge in this task is the lack of OOV training data. To solve this problem, we first propose a pseudo label generation module that leverages character detection and image inpainting to produce substantial pseudo OOV training data from real-world images. Unlike previous synthetic data, our pseudo OOV data contains real characters and backgrounds to simulate real-world applications. Secondly, to reduce noises in pseudo data, we present a semantic checking mechanism to filter semantically meaningful data. Thirdly, we introduce a quality-aware margin loss to boost the training with pseudo data. Our loss includes a margin-based part to enhance the classification ability, and a quality-aware part to penalize low-quality samples in both real and pseudo data.
  Extensive experiments demonstrate that our approach outperforms the state-of-the-art on eight datasets and achieves the first rank in the ICDAR2022 challenge.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2507.13374.pdf' target='_blank'>https://arxiv.org/pdf/2507.13374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Dela Rosa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13374">Smart Routing for Multimodal Video Retrieval: When to Search What</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce ModaRoute, an LLM-based intelligent routing system that dynamically selects optimal modalities for multimodal video retrieval. While dense text captions can achieve 75.9% Recall@5, they require expensive offline processing and miss critical visual information present in 34% of clips with scene text not captured by ASR. By analyzing query intent and predicting information needs, ModaRoute reduces computational overhead by 41% while achieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR (speech), OCR (text), and visual indices, averaging 1.78 modalities per query versus exhaustive 3.0 modality search. Evaluation on 1.8M video clips demonstrates that intelligent routing provides a practical solution for scaling multimodal retrieval systems, reducing infrastructure costs while maintaining competitive effectiveness for real-world deployment.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2404.05967.pdf' target='_blank'>https://arxiv.org/pdf/2404.05967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masato Fujitake
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05967">JSTR: Judgment Improves Scene Text Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a method for enhancing the accuracy of scene text recognition tasks by judging whether the image and text match each other. While previous studies focused on generating the recognition results from input images, our approach also considers the model's misrecognition results to understand its error tendencies, thus improving the text recognition pipeline. This method boosts text recognition accuracy by providing explicit feedback on the data that the model is likely to misrecognize by predicting correct or incorrect between the image and text. The experimental results on publicly available datasets demonstrate that our proposed method outperforms the baseline and state-of-the-art methods in scene text recognition.
